{"meta":{"title":"å¥½ä¹æ— è’","subtitle":"å¥½ä¹æ— è’ï¼Œè‰¯å£«ä¼‘ä¼‘","description":null,"author":"Yuqiu Ji","url":"http://shamy1997.github.io","root":"/"},"pages":[{"title":"tags","date":"2019-04-05T02:48:02.000Z","updated":"2019-04-05T02:48:27.444Z","comments":true,"path":"tags/index.html","permalink":"http://shamy1997.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-04-05T02:40:09.000Z","updated":"2019-04-05T02:44:03.107Z","comments":true,"path":"categories/index.html","permalink":"http://shamy1997.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-04-05T01:54:38.000Z","updated":"2019-04-05T02:43:12.587Z","comments":true,"path":"about/index.html","permalink":"http://shamy1997.github.io/about/index.html","excerpt":"","text":"å…³äºæˆ‘å°šä¸çŸ¥å¦‚ä½•å®šä¹‰â€¦â€¦"}],"posts":[{"title":"word-embedding.md","slug":"word-embedding-md","date":"2019-08-18T02:27:04.000Z","updated":"2019-09-18T03:01:42.016Z","comments":true,"path":"passages/word-embedding-md/","link":"","permalink":"http://shamy1997.github.io/passages/word-embedding-md/","excerpt":"","text":"ç†è§£Word Embeddingï¼ˆ1ï¼‰ï¼šä»Count Vectoråˆ°word2vecå‚è€ƒåšå®¢é“¾æ¥: ğŸ”— è¿™ä¸ª 1. ä»€ä¹ˆæ˜¯Word Embeddingåœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éƒ½æ— æ³•ç›´æ¥å¤„ç†å­—ç¬¦ä¸²æˆ–å¹³æ–‡æœ¬ï¼Œæ‰€ä»¥éœ€è¦é€šè¿‡ä¸€ç§ç¼–ç æ–¹å¼å°†å…¶å¤„ç†ä¸ºæ•°å€¼ï¼ŒWord Embedding å°±æ˜¯è¿™æ ·å°†æ–‡æœ¬å¤„ç†æˆæ•°å€¼çš„ä¸€ç±»æ–¹æ³•ã€‚ 2. ä¸åŒçš„Word Embedding ç±»å‹2.1. åŸºäºé¢‘ç‡çš„Word Embedding2.1.1 Count Vectorså‡è®¾ä¸€ä¸ªä¸€ä¸ªè¯­æ–™åº“Cæœ‰Dä¸ªæ–‡æœ¬ç‰‡æ®µ{d1,d2,d3,â€¦dD} ä»¥åŠNä¸ªä»è¯­æ–™åº“Cä¸­æå–çš„tokenã€‚è¿™Nä¸ªtokenå°†ä¼šå½¢æˆæˆ‘ä»¬çš„è¯å…¸ï¼Œè¿™æ ·æˆ‘ä»¬è®¾å®šçš„Count Vector å¤§å°ä¾¿æ˜¯ DxNã€‚åœ¨çŸ©é˜µMä¸­ï¼Œæ¯è¡Œéƒ½åŒ…å«ç€æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µçš„tokenå‡ºç°çš„é¢‘ç‡ã€‚ è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ç†è§£ã€‚ D1: He is lazy boy. She is also lazy. D2: Neeraj is a lazy person. å‡è®¾æˆ‘ä»¬çš„è¯­æ–™åº“å°±ä»…æœ‰è¿™ä¸¤å¥è¯ç»„æˆï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„è¯å…¸å³ä¸º[â€˜Heâ€™,â€™Sheâ€™,â€™lazyâ€™,â€™boyâ€™,â€™Neerajâ€™,â€™personâ€™]ã€‚è¿™é‡Œï¼ŒD=2ï¼ŒN=6ã€‚ é‚£ä¹ˆæˆ‘ä»¬2x6çš„çŸ©é˜µå°†è¢«è¡¨ç¤ºä¸ºï¼š He She lazy boy Neeraj person D1 1 1 2 1 0 0 D2 0 0 1 0 1 1 è¿™æ ·ï¼Œæ¯ä¸€çºµåˆ—ä¾¿å¯è¢«è®¤ä¸ºæ˜¯æ¯ä¸ªå•è¯çš„è¯å‘é‡ã€‚æ¯”å¦‚ï¼Œlazyçš„è¯å‘é‡å°±æ˜¯[2,1]ï¼Œå…¶ä»–å•è¯çš„è¯å‘é‡ä»¥æ­¤ç±»æ¨ã€‚åœ¨ä¸Šå›¾è¿™ä¸ªçŸ©é˜µä¸­ï¼Œè¡Œå¯¹åº”ç€è¯­æ–™åº“ä¸­çš„ä¸€ä¸ªä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œåˆ—å¯¹åº”ç€è¯å…¸ä¸­çš„ä¸€ä¸ªä¸ªtokenã€‚æˆ‘ä»¬è¦åƒè¿™æ ·é˜…è¯»è¿™ä¸ªçŸ©é˜µã€‚D2 åŒ…å«äº†â€™lazyâ€™ï¼šä¸€æ¬¡ï¼Œâ€™Neerajâ€™ï¼šä¸€æ¬¡ä»¥åŠâ€™personâ€™ï¼šä¸€æ¬¡ã€‚ ç„¶è€Œï¼Œåœ¨å‡†å¤‡ä¸Šé¢è¿™ä¸ªçŸ©é˜µMæ—¶ï¼Œå¯èƒ½æœ‰ä¸€äº›å˜ä½“ã€‚è¿™äº›å˜ä½“çš„å˜åŒ–ä¹‹å¤„åœ¨äºï¼š å‡†å¤‡è¯å…¸çš„æ–¹å¼ ä½ å¯ä»¥ä¼šç–‘æƒ‘ä¸ºä½•å‡†å¤‡è¯å…¸æ—¶æˆ‘ä»¬ä¹Ÿè¦åŠ ä»¥å˜åŠ¨ï¼Ÿäº‹å®ä¸Šï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬çš„è¯­æ–™åº“å¯èƒ½åŒ…å«ç€æˆç™¾ä¸Šåƒä¸ªæ–‡æœ¬ç‰‡æ®µã€‚é‚£ä¹ˆæˆ‘ä»¬å°±éœ€è¦ä»è¿™æˆç™¾ä¸Šåƒçš„æ–‡æœ¬ç‰‡æ®µä¸­æå–å‡ºç‹¬ç‰¹çš„tokenï¼Œé‚£ä¹ˆè¿™åŠ¿å¿…ä¼šå¯¼è‡´æˆ‘ä»¬æ‰€å¾—å‡ºçš„ä¾‹å¦‚ä¸Šå›¾çš„çŸ©é˜µéå¸¸ç¨€ç–ï¼Œä¸”è®¡ç®—æ—¶éå¸¸ä½æ•ˆã€‚å› æ­¤ï¼Œä¸€ä¸ªå¯é€‰çš„è§£å†³æ–¹æ³•æ˜¯ï¼Œæˆ‘ä»¬å°†åŸºäºé¢‘ç‡é€‰å–æ¯”æ–¹å‰10000ä¸ªå•è¯æ¥ä½œä¸ºæˆ‘ä»¬çš„è¯å…¸ï¼Œç„¶åå†åŸºäºè¿™ä¸ªè¯å…¸æ¥æ„å»ºæˆ‘ä»¬çš„çŸ©é˜µã€‚ è®¡ç®—å•è¯é¢‘æ¬¡çš„æ–¹å¼ åœ¨è®¡æ•°æ—¶ï¼Œå…¶å®æˆ‘ä»¬æœ‰ä¸¤ç§é€‰æ‹©ï¼Œä¸€ç§æ˜¯è®¡ç®—é¢‘ç‡ï¼Œå³ä¸€ä¸ªå•è¯åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­çš„æ¬¡æ•°ï¼Œä¸€ç§æ˜¯è®¡ç®—æ˜¯å¦å‡ºç°ï¼Œå³ä¸€ä¸ªå•è¯å¦‚æœåœ¨è¿™ä¸ªæ–‡ä¸­å‡ºç°åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿˜æ˜¯å€¾å‘äºä½¿ç”¨å‰è€…ã€‚ ä¸‹å›¾æ˜¯çŸ©é˜µMçš„ç¤ºæ„å›¾ï¼Œæ–¹ä¾¿ä½ ç†è§£ï¼š 2.1.2 TF-IDF vectorizationTF-IDF vectorization æ˜¯å¦ä¸€ç§åŸºäºé¢‘ç‡çš„è¡¨ç¤ºæ–¹å¼ï¼Œä½†æ˜¯å®ƒä¸ Count Vectorä¸åŒåœ¨äºå®ƒæ‰€è€ƒè™‘çš„ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå•è¯åœ¨å•ä¸ªæ–‡æœ¬ç‰‡æ®µä¸­çš„å‡ºç°é¢‘æ¬¡ï¼Œè€Œæ˜¯è€ƒè™‘å®ƒåœ¨æ•´ä¸ªè¯­æ–™ä¸­çš„å‡ºç°é¢‘ç‡ã€‚æ‰€ä»¥ï¼Œè¿™èƒŒåæœ‰ä½•åˆç†æ€§å‘¢ï¼Ÿè®©æˆ‘ä»¬è¯•ç€ç†è§£è¿™ä¸€ç‚¹ã€‚ æ¯”è¾ƒå¸¸è§çš„å•è¯ï¼Œä¾‹å¦‚â€isâ€,â€theâ€,â€aâ€ç­‰å’Œé‚£äº›å¯¹äºæ–‡æœ¬ç‰‡æ®µæ›´ä¸ºé‡è¦çš„ç‰‡æ®µç›¸æ¯”å¾€å¾€å‡ºç°å¾—æ›´ä¸ºé¢‘ç¹ã€‚æ¯”å¦‚â€theâ€è¿™ç§å•è¯åœ¨å„ä¸ªæ–‡æœ¬ç‰‡æ®µä¸­éƒ½æœ‰å‡ºç°ï¼Œè€Œâ€Harry Potterâ€å¯èƒ½åªå‡ºç°åœ¨ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹è¿™éƒ¨å°è¯´æœ‰å…³çš„æ–‡æœ¬ç‰‡æ®µé‡Œï¼Œä½†æ˜¯å¯¹äºè¿™äº›ç‰‡æ®µæ¥è¯´ï¼Œâ€Harry Potterâ€æ˜¾ç„¶æ¯”â€theâ€æ›´é‡è¦ï¼Œå› ä¸ºå®ƒæŠŠè¿™äº›æ–‡æœ¬å’Œå…¶ä»–æ–‡æœ¬åŒºåˆ«å¼€ã€‚äºæ˜¯æˆ‘ä»¬å¸Œæœ›é™ä½è¿™äº›è¾ƒä¸ºå¸¸è§çš„å•è¯çš„æƒé‡å¹¶ä¸”æ›´åŠ é‡è§†é‚£äº›æ–‡æœ¬ç‰‡æ®µä¸­ç‹¬ç‰¹çš„å•è¯ã€‚ TF-IDFå°±å¯ä»¥åšåˆ°ä¸Šé¢è¿™ä¸€ç‚¹ã€‚é‚£ä¹ˆTD-IDFæ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿ å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‡è®¾æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªè¡¨æ ¼ï¼Œç¬¬ä¸€åˆ—æ˜¯æ–‡æœ¬ä¸­çš„tokenï¼Œç¬¬äºŒåˆ—æ˜¯å‡ºç°çš„é¢‘æ¬¡ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæ¥å®šä¹‰ä¸€ä¸‹TF-IDFç›¸å…³çš„ä¸€äº›æœ¯è¯­ï¼š TFï¼šæ–‡æœ¬ä¸­term Tå‡ºç°çš„æ¬¡æ•°/æ–‡æœ¬çš„termæ€»æ•° å› æ­¤ï¼ŒTF(This,Doucument1)=1/8,TF(This,Document2)=1/5ã€‚ TFè¡¨ç¤ºäº†è¿™ä¸ªå•è¯å¯¹è¿™ä¸ªæ–‡æœ¬çš„è´¡çŒ®ç¨‹åº¦ï¼Œæ¯”å¦‚è¯´å’Œæ–‡æœ¬æ›´ä¸ºç›¸å…³çš„å•è¯ï¼Œå®ƒçš„TFå€¼ä¼šæ¯”è¾ƒå¤§ï¼Œå› ä¸ºå®ƒä¼šæ›´é«˜é¢‘åœ°å‡ºç°åœ¨æ–‡æœ¬ä¸­ã€‚ DFï¼šlog(è¯­æ–™åº“ä¸­çš„æ–‡æœ¬æ€»æ•°/è¯­æ–™åº“ä¸­å«æœ‰term Tçš„æ–‡æœ¬æ•°) å› æ­¤ï¼ŒIDF(this)=log(2/2)=0ã€‚ ç†è®ºä¸Šæ¥è¯´ï¼Œå¦‚æœä¸€ä¸ªå•è¯åœ¨è¯­æ–™åº“æ‰€æœ‰çš„å•è¯ä¸­éƒ½å‡ºç°äº†ï¼Œé‚£ä¹ˆå¯èƒ½è¿™ä¸ªå•è¯å¯¹äºæŸä¸ªæˆ–æŸäº›ç‰¹å®šçš„æ–‡æœ¬å¹¶ä¸é‡è¦ï¼Œå³æ˜¯æˆ‘ä»¬æ‰€è¯´çš„é‚£ç±»æ¯”è¾ƒå¸¸è§çš„å•è¯ã€‚ä½†æ˜¯å¦‚æœä¸€ä¸ªå•è¯åªå‡ºç°è¯­æ–™åº“çš„ä¸€ä¸ªå­é›†çš„æ–‡æœ¬ä¸­å‡ºç°ï¼Œé‚£ä¹ˆè¿™ä¸ªå•è¯å¯¹äºé‚£äº›æ–‡æœ¬å…·æœ‰ä¸€å®šçš„ç›¸å…³æ€§ã€‚æ¯”å¦‚IDF(Messi)=log(2/1)=0.301ã€‚ ç”±æ­¤å¯è§ï¼Œå¯¹äºæ–‡æœ¬ç‰‡æ®µ1è€Œè¨€ï¼ŒTF-IDFæ–¹æ³•ç‹ ç‹ åœ°å¤„ç½šäº†â€thisâ€ä½†æ˜¯å´ç»™äºˆâ€Messiâ€æ›´é«˜çš„æƒé‡ã€‚å› æ­¤è¿™ä¸ªæ–¹æ³•èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬æ›´å¥½çš„ç†è§£â€Messiâ€æ˜¯æ–‡æœ¬ç‰‡æ®µ1çš„ä¸€ä¸ªé‡è¦å•è¯ã€‚ 2.1.3 å…·æœ‰å›ºå®šä¸Šä¸‹æ–‡çª—å£çš„å…±ç°çŸ©é˜µæŒ‡å¯¼æ€æƒ³ï¼šç›¸ä¼¼çš„å•è¯å¾€å¾€ä¸€èµ·å‡ºç°å¹¶ä¸”å…·æœ‰ç›¸ä¼¼çš„æ–‡æœ¬ç¯å¢ƒã€‚æ¯”å¦‚â€Apple is a fruitâ€,â€Mango is a fruitâ€ï¼Œè‹¹æœå’ŒèŠ’æœå€¾å‘äºæœ‰ä¸€ä¸ªç›¸ä¼¼çš„ä¸Šä¸‹åˆï¼Œå¦‚â€fruitâ€ã€‚ åœ¨æˆ‘æ·±å…¥ä¸€ä¸ªå…±ç°çŸ©é˜µæ˜¯å¦‚ä½•æ„å»ºçš„ç»†èŠ‚ä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰å¿…è¦å…ˆç†æ¸…ä¸¤ä¸ªæ¦‚å¿µã€‚ Co-occurenceï¼šå¯¹äºä¸€ä¸ªç»™å®šçš„è¯­æ–™åº“ï¼Œä¸€å¯¹å•è¯çš„å…±ç°ï¼Œæ¯”æ–¹è¯´w1å’Œw2çš„å…±ç°ï¼Œå°±æ˜¯åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ƒä»¬å…±åŒå‡ºç°çš„æ¬¡æ•°ã€‚ Context Windowï¼šä¸Šä¸‹æ–‡çª—å£çš„å‚æ•°ç”±æ•°å­—å’Œæ–¹å‘è®¾å®šã€‚æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­æ¥å¸®åŠ©ç†è§£ã€‚ Quick brown fox jump over the lazy dog Quick brown fox jump Over The lazy dog è¿™ä¸ªè¡¨æ ¼ç¬¬ä¸€è¡Œçš„æ–œä½“æ˜¯foxçš„é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œç¬¬äºŒè¡Œçš„æ–œä½“æ˜¯overçš„é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ã€‚ ç°åœ¨ï¼Œæˆ‘ä»¬ç¼–ä¸€ä¸ªç”¨æ¥è®¡ç®—å…±ç°çŸ©é˜µçš„è¯­æ–™åº“ã€‚ è¯­æ–™åº“ï¼šHe is not lazy. He is intelligent. He is smart. è®©æˆ‘ä»¬é€šè¿‡çœ‹ä¸Šé¢ä¸¤ä¸ªçº¢ã€è“ç€è‰²çš„ä¾‹å­æ¥ç†è§£å…±ç°çŸ©é˜µã€‚ çº¢è‰²æ ¼å­æ‰€è¡¨ç¤ºçš„ï¼Œæ˜¯â€Heâ€å’Œâ€isâ€åœ¨é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œçº¢è‰²æ ¼å­ä¸­çš„æ•°å­—ä¸º4ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„è¡¨æ ¼å¯è§†åŒ–è¿™ä¸ªè®¡æ•°è¿‡ç¨‹ã€‚ï¼ˆå³å‡ºç°æ— éœ€ç´§æŒ¨ç€ï¼Œåªè¦éƒ½åœ¨çª—å£ä¸­ï¼Œå³ä½¿é¡ºåºé¢ å€’ï¼Œéƒ½æ˜¯å¯ä»¥çš„ï¼‰ã€‚ è“è‰²æ ¼å­æ‰€è¡¨ç¤ºçš„ï¼Œæ˜¯â€lazyâ€å’Œâ€intelligentâ€åœ¨é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œå…¶ä¸­æ•°å­—ä¸º0ï¼Œè¡¨ç¤ºå®ƒä»¬ä»ä¸æ›¾åŒæ—¶å‡ºç°åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­ã€‚ å…±ç°çŸ©é˜µçš„å˜ä½“å‡è®¾è¯­æ–™åº“ä¸­æœ‰Vä¸ªç‹¬ç‰¹çš„å•è¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„è¯æ±‡é•¿åº¦å³ä¸ºVã€‚ä¸‹é¢ç»™å‡ºäº†å…±ç°çŸ©é˜µçš„ä¸¤ç§ä¸åŒå˜ä½“ï¼š å¤§å°ä¸ºVxVçš„å…±ç°çŸ©é˜µã€‚ä½†æ˜¯ç”±äºè¿™æ ·çš„çŸ©é˜µå¤ªå¤§è€Œéš¾äºè®¡ç®—ï¼Œæ‰€ä»¥å®é™…ä¸­è¿™ç±»å»ºæ¨¡å¹¶ä¸è¢«çœ‹å¥½ã€‚ å¤§å°ä¸ºVxNçš„å…±ç°çŸ©é˜µã€‚Nè¡¨ç¤ºå»é™¤æ‰åœç”¨è¯ç­‰ä¸ç›¸å…³è¯æ±‡åçš„Vçš„ä¸€ä¸ªè‡ªå·±ã€‚ä½†æ˜¯è¿™ç±»å»ºæ¨¡ä¹Ÿä¾ç„¶å¾ˆå¤§ä¸”éš¾äºè®¡ç®—ã€‚ è¿™é‡Œè¦å¼ºè°ƒä¸€ç‚¹ï¼Œå…±ç°çŸ©é˜µå¹¶éæ˜¯æˆ‘ä»¬å¹¿æ³›ä½¿ç”¨çš„è¯å‘é‡ã€‚ç›¸åï¼Œè¿™ç±»å…±ç°çŸ©é˜µå¸¸å¸¸ä¸è¯¸å¦‚PCAï¼ŒSVDè¿™æ ·çš„æŠ€æœ¯ç»„åˆä½¿ç”¨åœ¨å› ç´ åˆ†è§£ä¸Šã€‚è€Œè¿™äº›å› ç´ åˆ†è§£çš„ç»„åˆæ„æˆäº†è¯å‘é‡è¡¨ç¤ºã€‚ è¯´å¾—æ›´æ˜ç™½äº›ï¼Œä½ åœ¨ä¸‹é¢VxVçš„å…±ç°çŸ©é˜µä¸Šä½¿ç”¨äº†PCAï¼Œä½ ä¼šå¾—åˆ°Vä¸ªä¸»å…ƒç´ ã€‚å¦‚æ­¤ï¼Œä½ å¯ä»¥ä»è¿™Vä¸ªå…ƒç´ ä¸­é€‰å‡ºkä¸ªã€‚å› æ­¤ï¼Œä½ è®²å¾—åˆ°ä¸€ä¸ªVxkçš„æ–°å‰§è¯ã€‚è¿™æ ·ï¼Œè™½ç„¶ä¸€ä¸ªå•è¯æ˜¯ç”±kç»´è¡¨ç¤ºçš„è€Œä¸æ˜¯Vç»´è¡¨ç¤ºçš„ï¼Œä½†æ˜¯å®ƒä¾ç„¶èƒ½æ•æ‰åˆ°å‡ ä¹åŒæ ·çš„æ„ä¹‰ã€‚ å› æ­¤ï¼ŒPCAèƒŒåçš„æ“ä½œå®é™…ä¸Šå°±æ˜¯è®²å…±ç°çŸ©é˜µæ‹†è§£ä¸ºä¸‰ä¸ªçŸ©é˜µï¼ŒUï¼ŒSå’ŒVã€‚ å…±ç°çŸ©é˜µçš„ä¼˜ç‚¹ å®ƒè•´å«äº†å•è¯ä¹‹é—´çš„è¯­ä¹‰è”ç³»ã€‚æ¯”å¦‚â€ç”·äººâ€å’Œâ€å¥³äººâ€ä¼šæ¯”â€ç”·äººâ€å’Œâ€è‹¹æœâ€æ›´è¿‘ã€‚ å®ƒçš„æ ¸å¿ƒåœ¨äºä½¿ç”¨SVDæ¥åˆ›é€ å‡ºæ¯”ç°å­˜æ–¹æ³•æ›´å‡†ç¡®çš„è¯å‘é‡è¡¨ç¤ºã€‚ å®ƒä½¿ç”¨å› å­åˆ†è§£ï¼Œå› å­åˆ†è§£æ˜¯ä¸€ä¸ªè‰¯å®šä¹‰é—®é¢˜å¹¶ä¸”å¯ä»¥è¢«æœ‰æ•ˆè§£å†³ã€‚ å®ƒåªè¦è¢«è®¡ç®—ä¸€æ¬¡ï¼Œä¹‹åä»»ä½•æ—¶å€™éƒ½å¯ä»¥è¢«ä½¿ç”¨ã€‚åœ¨è¿™ä¸ªæ„ä¹‰ä¸Šï¼Œå®ƒæ¯”å…¶ä»–æ–¹æ³•æ›´å¿«ã€‚ å…±ç°çŸ©é˜µçš„ç¼ºç‚¹ å®ƒéœ€è¦å·¨å¤§çš„å†…å­˜å»å­˜å‚¨ã€‚ 2.2 åŸºäºé¢„æµ‹çš„word embeddingå‰é¢æ‰€è¯´çš„åŸºäºé¢‘ç‡çš„word embeddingæœ‰å„ç§å„æ ·çš„å±€é™ã€‚è€ŒåMitolovç­‰äººå°†word2vecä»‹ç»åˆ°nlpçš„å„ä¸ªé¢†åŸŸä¸­ï¼Œä½¿å¾—åŸºäºé¢„æµ‹çš„word embeddingèµ°ä¸Šå†å²èˆå°ã€‚è¿™äº›æ–¹æ³•æ˜¯åŸºäºé¢„æµ‹çš„ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒä»¬ä¼šç»™å‡ºå„ä¸ªå•è¯çš„æ¦‚ç‡ã€‚å®ƒä»¬åœ¨è¯æ±‡ç›¸ä¼¼åº¦çš„ä»»åŠ¡ä¸Šè¡¨ç°å¾—éå¸¸å¥½ï¼Œç”šè‡³èƒ½è¾¾åˆ°King -man +woman = Queenè¿™æ ·çš„ç¥å¥‡æ•ˆæœã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°±æ¥çœ‹çœ‹word2vecå…·ä½“æ˜¯å¦‚ä½•å¾—å‡ºè¯å‘é‡çš„ã€‚ word2vecå¹¶ä¸æ˜¯ä¸€ä¸ªå•ç‹¬çš„ç®—æ³•ï¼Œå®ƒæ˜¯ç”±CBOWå’ŒSkip-gramæ¨¡å‹ç»„åˆè€Œæˆçš„ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹éƒ½æ˜¯ç”±è¯åˆ°è¯çš„æµ…å±‚ç¥ç»ç½‘ç»œç»„æˆçš„ã€‚å®ƒä»¬æ‰€å­¦ä¹ çš„æƒé‡å°†æˆä¸ºå•è¯çš„å‘é‡è¡¨ç¤ºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±åˆ†åˆ«ä»‹ç»è¿™ä¸¤ç§æ¨¡å‹ã€‚ 2.2.1 CBOW è¿ç»­è¯è¢‹æ¨¡å‹CBOWæ˜¯åŸºäºè¯­å¢ƒï¼ˆæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼‰æ¥é¢„æµ‹å‡ºä¸€ä¸ªå•è¯çš„æ¦‚ç‡ã€‚è¿™ä¸ªè¯­å¢ƒå¯èƒ½æ˜¯ä¸€ä¸ªå•è¯æˆ–è€…ä¸€ç»„è¯ã€‚ä½†ä¸ºäº†ä»‹ç»çš„ç®€å•ï¼Œæˆ‘ä»¬è¿™è¾¹ä»¥ä¸€ä¸ªå•è¯ä½œä¸ºè¯­å¢ƒå¹¶ä¸€æ¬¡æ¥é¢„æµ‹ä¸€ä¸ªç›®æ ‡è¯æ±‡ã€‚ å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¯­æ–™åº“C=â€Hey, this is sample corpus using only one context word.â€ã€‚å¹¶ä¸”æˆ‘ä»¬å·²ç»å®šä¹‰äº†ä¸Šä¸‹æ–‡çª—å£å¤§å°ä¸º1ã€‚è¿™ä¸ªè¯­æ–™åº“å°†ä¼šè¢«è½¬æ¢æˆä¸‹å›¾æ‰€ç¤ºçš„è®­ç»ƒé›†åˆã€‚è¾“å…¥å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­å³è¾¹çš„çŸ©é˜µåŒ…å«äº†å·¦è¾¹çš„è¾“å…¥çš„ç‹¬çƒ­ç¼–ç ã€‚ æ¯”å¦‚è¯´isçš„ç›®æ ‡çš„è¾“å…¥ä¸º[0001000000]ã€‚ ä¸Šå›¾æ‰€æ˜¾ç¤ºçš„çŸ©é˜µå°†ä¼šè¢«é€å…¥ä¸€ä¸ªç”±ä¸‰å±‚ç»„æˆçš„æµ…å±‚ç¥ç»ç½‘ç»œï¼šä¸€ä¸ªè¾“å…¥å±‚ï¼Œä¸€ä¸ªéšè—å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ã€‚è¾“å‡ºå±‚ä¼šä½¿ç”¨softmaxå‡½æ•°ï¼Œsoftmaxå‡½æ•°æ˜¯ä¸€ä¸ªç”¨äºåˆ†ç±»é¢„æµ‹çš„å¸¸ç”¨å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å¾—å‡ºçš„å„ä¸ªç±»åˆ«çš„æ•°å€¼æ€»å’Œä¸º1ã€‚ æµç¨‹æ˜¯è¿™æ ·çš„ï¼š è¾“å…¥å±‚å’Œç›®æ ‡å±‚éƒ½æ˜¯ç”±1xVçš„ç‹¬çƒ­ç¼–ç ç»„æˆï¼Œè¿™é‡ŒV=10ã€‚ è¿™é‡Œæœ‰ä¸¤å¥—æƒé‡ï¼Œä¸€å¥—æ˜¯è¾“å…¥å±‚å’Œéšè—å±‚ä¹‹é—´çš„æƒé‡ï¼Œä¸€å¥—æ˜¯éšè—å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„æƒé‡ã€‚input-hidden å±‚çŸ©é˜µå¤§å°ä¸ºVxN, hidden-output é‚£å±‚çš„çŸ©é˜µå¤§å°ä¸ºNXV ï¼šè¿™é‡Œï¼ŒNæŒ‡çš„æ˜¯æˆ‘ä»¬é€‰æ‹©ç”¨æ¥è¡¨è¾¾å•è¯çš„ç»´åº¦ã€‚å®ƒæ˜¯ä»»æ„çš„ï¼Œå¹¶ä¸”æ˜¯ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªè¶…å‚æ•°ã€‚å¹¶ä¸”ï¼ŒNæ˜¯ä¹Ÿæ˜¯éšè—å±‚çš„èŠ‚ç‚¹æ•°ï¼Œè¿™é‡Œï¼Œæˆ‘ä»¬è®¾N=4ã€‚ ä»»æ„ä¸€å±‚ä¹‹é—´éƒ½ä¸å­˜åœ¨æ¿€æ´»å‡½æ•°ã€‚ è¾“å…¥ä¸ input-hiddenå±‚æƒé‡çš„ä¹˜ç§¯è¢«ç§°ä¸º hidden activationã€‚ Hidden input ä¸hidden-outputå±‚æƒé‡ç›¸ä¹˜ï¼Œå¾—åˆ°è¾“å‡ºã€‚ è¾“å‡ºå±‚å’Œç›®æ ‡ä¹‹é—´çš„è¯¯å·®å°†ä¼šè¢«ç”¨æ¥è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»¥è°ƒæ•´weightsã€‚ åœ¨éšè—å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´æƒé‡å°†ä¼šæˆä¸ºè¯å‘é‡ã€‚ ä¸Šé¢çš„æµç¨‹æ˜¯é’ˆå¯¹äºä¸Šä¸‹æ–‡çª—å£ä¸º1çš„ï¼Œä¸‹å›¾æ˜¾ç¤ºäº†ä¸Šä¸‹æ–‡çª—å£å¤§äº1çš„æƒ…å†µã€‚ ä¸‹å›¾æ˜¯ä¸ºäº†æ›´å¥½ç†è§£è¿™ä¸ªç»“æ„çš„çŸ©é˜µå›¾ä¾‹ã€‚ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬è¦ä½¿ç”¨3ä¸ª context wordå»é¢„æµ‹ç›®æ ‡å•è¯çš„æ¦‚ç‡ã€‚è¾“å…¥å±‚æ˜¯3ä¸ª[1xV]çš„å‘é‡ï¼Œè€Œè¾“å‡ºæ˜¯ä¸€ä¸ª[1xV]ã€‚å‰©ä¸‹çš„æ„é€ å’Œ1-contextçš„CBOWæ˜¯ä¸€æ ·çš„ã€‚ ä½†æ˜¯åœ¨éšè—å±‚ä¸­ï¼Œ3-context wordçš„æ¨¡å‹ä¸å†æ˜¯ç®€å•å¤åˆ¶è¾“å…¥ï¼Œè€Œæ˜¯è¦è¿›è¡Œä¸€ä¸ªå¹³å‡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸Šé¢çš„å›¾æ¥ç†è§£è¿™ä¸€ç‚¹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰3ä¸ªcontext word, é‚£ä¹ˆæˆ‘ä»¬å°†ä¼šæœ‰3ä¸ª åˆæ­¥çš„hidden activation ç„¶åæœ€åå¹³å‡å¾—åˆ°æœ€ç»ˆçš„ hidden activiationã€‚ é‚£ä¹ˆCBOWå’Œä¸€èˆ¬çš„MLPä¹‹é—´æœ‰ä½•ä¸åŒå‘¢ï¼Ÿ CBOWçš„ç›®æ ‡å‡½æ•°å’ŒMLPä¸åŒï¼ŒCBOWæ˜¯ç›®æ ‡å‡½æ•°è´Ÿçš„æœ€å¤§ä¼¼ç„¶ã€‚ è¯¯å·®æ¢¯åº¦ä¸ä¸€æ ·ï¼Œå› ä¸ºMLPä»¥sigmoidä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œè€ŒCBOWä¸€èˆ¬æ˜¯çº¿æ€§æ¿€æ´»ã€‚ CBOWçš„ä¼˜ç‚¹ï¼š åŸºäºæ¦‚ç‡çš„ï¼Œæ›´ç¬¦åˆå®é™…ï¼› å ç”¨å†…å­˜å°ã€‚ CBOWçš„ç¼ºç‚¹ï¼š CBOWæ˜¯åˆ©ç”¨å•è¯çš„è¯­å¢ƒæ¥è¡¨ç¤ºå•è¯çš„ï¼Œä½†æ˜¯å¯¹äºå¤šä¹‰è¯è€Œè¨€ï¼Œæ¯”å¦‚è‹¹æœæ—¢æ˜¯æ°´æœä¹Ÿæ˜¯ä¸€å®¶å…¬å¸ï¼Œä½†æ˜¯ç”±äºCBOWå°†è¿™ä¸ªæ–‡æœ¬éƒ½è€ƒè™‘è¿›å»ï¼Œæ‰€ä»¥è‹¹æœè¢«è¡¨ç¤ºåœ¨æ°´æœå’Œå…¬å¸ä¹‹é—´äº†ã€‚ ä»å¤´è®­ç»ƒä¸€ä¸ªCBOWè‹¥æ²¡æœ‰å¾ˆå¥½ä¼˜åŒ–çš„è¯å°†è®­ç»ƒå¾ˆä¹…ã€‚ 2.2.2 Skip-Gram modelSkip-gram çš„ç±»å‹ä¸CBOWä¸€æ ·ï¼Œä½†æ˜¯å®ƒçš„ç»“æ„æ˜¯åè¿‡æ¥çš„ï¼Œå®ƒæ˜¯åŸºäºç»™å®šå•è¯å»é¢„æµ‹å•è¯çš„ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬ä¾ç„¶ç”¨è®²è§£CBOWæ—¶ä½¿ç”¨çš„è¯­æ–™ã€‚ Skip-gramçš„è¾“å…¥å’Œ1-contextçš„CBOW æ¨¡å‹å¾ˆç±»ä¼¼ã€‚å¹¶ä¸”éšè—å±‚çš„activitiaonä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚ä¸åŒçš„ä»…ä»…æ˜¯ç›®æ ‡å˜é‡ã€‚å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å•è¯ä¸¤è¾¹éƒ½å®šä¹‰äº†ä¸€ä¸ªé•¿åº¦ä¸º1çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šæœ‰ä¸¤ä¸ªç‹¬çƒ­ç¼–ç çš„ç›®æ ‡å˜é‡å’Œä¸¤ä¸ªç›¸åº”çš„è¾“å‡ºã€‚ è€Œè¿™ä¸¤ä¸ªç›®æ ‡å˜é‡çš„è¯¯å·®å°†ä¼šè¢«åˆ†åˆ«è®¡ç®—ï¼Œç„¶åå°†ä¸¤ä¸ªè¯¯å·®åŠ èµ·æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚ Skip-Gram æ¨¡å‹çš„ä¼˜ç‚¹ å¯ä»¥æŠ“ä½ä¸€ä¸ªå•è¯çš„å¤šä¸ªä¹‰é¡¹ã€‚ ä½¿ç”¨è´Ÿé‡‡æ ·çš„Skip-Gramæ¨¡å‹ä¼šæ¯”å…¶ä»–æ–¹æ³•æ›´é«˜æ•ˆã€‚ Skip-Gramæ¨¡å‹çš„ç¼ºç‚¹ ä¾èµ–è¯­æ–™åº“çš„å¤§å° é‡‡æ ·æ˜¯å¯¹ç»Ÿè®¡æ•°æ®çš„ä½æ•ˆåˆ©ç”¨","categories":[{"name":"NLP","slug":"NLP","permalink":"http://shamy1997.github.io/categories/NLP/"}],"tags":[{"name":"Word Embedding","slug":"Word-Embedding","permalink":"http://shamy1997.github.io/tags/Word-Embedding/"}]},{"title":"ç†è§£Word Embeddingï¼ˆ1ï¼‰ï¼šä»Count Vectoråˆ°word2vec","slug":"Word Embeddingæ¢³ç†","date":"2019-07-04T07:10:50.000Z","updated":"2019-09-18T02:29:12.160Z","comments":true,"path":"passages/Word Embeddingæ¢³ç†/","link":"","permalink":"http://shamy1997.github.io/passages/Word Embeddingæ¢³ç†/","excerpt":"","text":"ç†è§£Word Embeddingï¼ˆ1ï¼‰ï¼šä»Count Vectoråˆ°word2vec å‚è€ƒåšå®¢é“¾æ¥ 1. ä»€ä¹ˆæ˜¯Word Embeddingåœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬éƒ½æ— æ³•ç›´æ¥å¤„ç†å­—ç¬¦ä¸²æˆ–å¹³æ–‡æœ¬ï¼Œæ‰€ä»¥éœ€è¦é€šè¿‡ä¸€ç§ç¼–ç æ–¹å¼å°†å…¶å¤„ç†ä¸ºæ•°å€¼ï¼ŒWord Embedding å°±æ˜¯è¿™æ ·å°†æ–‡æœ¬å¤„ç†æˆæ•°å€¼çš„ä¸€ç±»æ–¹æ³•ã€‚ 2. ä¸åŒçš„Word Embedding ç±»å‹2.1. åŸºäºé¢‘ç‡çš„Word Embedding2.1.1 Count Vectorså‡è®¾ä¸€ä¸ªä¸€ä¸ªè¯­æ–™åº“Cæœ‰Dä¸ªæ–‡æœ¬ç‰‡æ®µ{d1,d2,d3,â€¦dD} ä»¥åŠNä¸ªä»è¯­æ–™åº“Cä¸­æå–çš„tokenã€‚è¿™Nä¸ªtokenå°†ä¼šå½¢æˆæˆ‘ä»¬çš„è¯å…¸ï¼Œè¿™æ ·æˆ‘ä»¬è®¾å®šçš„Count Vector å¤§å°ä¾¿æ˜¯ DxNã€‚åœ¨çŸ©é˜µMä¸­ï¼Œæ¯è¡Œéƒ½åŒ…å«ç€æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µçš„tokenå‡ºç°çš„é¢‘ç‡ã€‚ è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ç†è§£ã€‚ D1: He is lazy boy. She is also lazy. D2: Neeraj is a lazy person. å‡è®¾æˆ‘ä»¬çš„è¯­æ–™åº“å°±ä»…æœ‰è¿™ä¸¤å¥è¯ç»„æˆï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„è¯å…¸å³ä¸º[â€˜Heâ€™,â€™Sheâ€™,â€™lazyâ€™,â€™boyâ€™,â€™Neerajâ€™,â€™personâ€™]ã€‚è¿™é‡Œï¼ŒD=2ï¼ŒN=6ã€‚ é‚£ä¹ˆæˆ‘ä»¬2x6çš„çŸ©é˜µå°†è¢«è¡¨ç¤ºä¸ºï¼š He She lazy boy Neeraj person D1 1 1 2 1 0 0 D2 0 0 1 0 1 1 è¿™æ ·ï¼Œæ¯ä¸€çºµåˆ—ä¾¿å¯è¢«è®¤ä¸ºæ˜¯æ¯ä¸ªå•è¯çš„è¯å‘é‡ã€‚æ¯”å¦‚ï¼Œlazyçš„è¯å‘é‡å°±æ˜¯[2,1]ï¼Œå…¶ä»–å•è¯çš„è¯å‘é‡ä»¥æ­¤ç±»æ¨ã€‚åœ¨ä¸Šå›¾è¿™ä¸ªçŸ©é˜µä¸­ï¼Œè¡Œå¯¹åº”ç€è¯­æ–™åº“ä¸­çš„ä¸€ä¸ªä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œåˆ—å¯¹åº”ç€è¯å…¸ä¸­çš„ä¸€ä¸ªä¸ªtokenã€‚æˆ‘ä»¬è¦åƒè¿™æ ·é˜…è¯»è¿™ä¸ªçŸ©é˜µã€‚D2 åŒ…å«äº†â€™lazyâ€™ï¼šä¸€æ¬¡ï¼Œâ€™Neerajâ€™ï¼šä¸€æ¬¡ä»¥åŠâ€™personâ€™ï¼šä¸€æ¬¡ã€‚ ç„¶è€Œï¼Œåœ¨å‡†å¤‡ä¸Šé¢è¿™ä¸ªçŸ©é˜µMæ—¶ï¼Œå¯èƒ½æœ‰ä¸€äº›å˜ä½“ã€‚è¿™äº›å˜ä½“çš„å˜åŒ–ä¹‹å¤„åœ¨äºï¼š å‡†å¤‡è¯å…¸çš„æ–¹å¼ ä½ å¯ä»¥ä¼šç–‘æƒ‘ä¸ºä½•å‡†å¤‡è¯å…¸æ—¶æˆ‘ä»¬ä¹Ÿè¦åŠ ä»¥å˜åŠ¨ï¼Ÿäº‹å®ä¸Šï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬çš„è¯­æ–™åº“å¯èƒ½åŒ…å«ç€æˆç™¾ä¸Šåƒä¸ªæ–‡æœ¬ç‰‡æ®µã€‚é‚£ä¹ˆæˆ‘ä»¬å°±éœ€è¦ä»è¿™æˆç™¾ä¸Šåƒçš„æ–‡æœ¬ç‰‡æ®µä¸­æå–å‡ºç‹¬ç‰¹çš„tokenï¼Œé‚£ä¹ˆè¿™åŠ¿å¿…ä¼šå¯¼è‡´æˆ‘ä»¬æ‰€å¾—å‡ºçš„ä¾‹å¦‚ä¸Šå›¾çš„çŸ©é˜µéå¸¸ç¨€ç–ï¼Œä¸”è®¡ç®—æ—¶éå¸¸ä½æ•ˆã€‚å› æ­¤ï¼Œä¸€ä¸ªå¯é€‰çš„è§£å†³æ–¹æ³•æ˜¯ï¼Œæˆ‘ä»¬å°†åŸºäºé¢‘ç‡é€‰å–æ¯”æ–¹å‰10000ä¸ªå•è¯æ¥ä½œä¸ºæˆ‘ä»¬çš„è¯å…¸ï¼Œç„¶åå†åŸºäºè¿™ä¸ªè¯å…¸æ¥æ„å»ºæˆ‘ä»¬çš„çŸ©é˜µã€‚ è®¡ç®—å•è¯é¢‘æ¬¡çš„æ–¹å¼ åœ¨è®¡æ•°æ—¶ï¼Œå…¶å®æˆ‘ä»¬æœ‰ä¸¤ç§é€‰æ‹©ï¼Œä¸€ç§æ˜¯è®¡ç®—é¢‘ç‡ï¼Œå³ä¸€ä¸ªå•è¯åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­çš„æ¬¡æ•°ï¼Œä¸€ç§æ˜¯è®¡ç®—æ˜¯å¦å‡ºç°ï¼Œå³ä¸€ä¸ªå•è¯å¦‚æœåœ¨è¿™ä¸ªæ–‡ä¸­å‡ºç°åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿˜æ˜¯å€¾å‘äºä½¿ç”¨å‰è€…ã€‚ ä¸‹å›¾æ˜¯çŸ©é˜µMçš„ç¤ºæ„å›¾ï¼Œæ–¹ä¾¿ä½ ç†è§£ï¼š 2.1.2 TF-IDF vectorizationTF-IDF vectorization æ˜¯å¦ä¸€ç§åŸºäºé¢‘ç‡çš„è¡¨ç¤ºæ–¹å¼ï¼Œä½†æ˜¯å®ƒä¸ Count Vectorä¸åŒåœ¨äºå®ƒæ‰€è€ƒè™‘çš„ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå•è¯åœ¨å•ä¸ªæ–‡æœ¬ç‰‡æ®µä¸­çš„å‡ºç°é¢‘æ¬¡ï¼Œè€Œæ˜¯è€ƒè™‘å®ƒåœ¨æ•´ä¸ªè¯­æ–™ä¸­çš„å‡ºç°é¢‘ç‡ã€‚æ‰€ä»¥ï¼Œè¿™èƒŒåæœ‰ä½•åˆç†æ€§å‘¢ï¼Ÿè®©æˆ‘ä»¬è¯•ç€ç†è§£è¿™ä¸€ç‚¹ã€‚ æ¯”è¾ƒå¸¸è§çš„å•è¯ï¼Œä¾‹å¦‚â€isâ€,â€theâ€,â€aâ€ç­‰å’Œé‚£äº›å¯¹äºæ–‡æœ¬ç‰‡æ®µæ›´ä¸ºé‡è¦çš„ç‰‡æ®µç›¸æ¯”å¾€å¾€å‡ºç°å¾—æ›´ä¸ºé¢‘ç¹ã€‚æ¯”å¦‚â€theâ€è¿™ç§å•è¯åœ¨å„ä¸ªæ–‡æœ¬ç‰‡æ®µä¸­éƒ½æœ‰å‡ºç°ï¼Œè€Œâ€Harry Potterâ€å¯èƒ½åªå‡ºç°åœ¨ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹è¿™éƒ¨å°è¯´æœ‰å…³çš„æ–‡æœ¬ç‰‡æ®µé‡Œï¼Œä½†æ˜¯å¯¹äºè¿™äº›ç‰‡æ®µæ¥è¯´ï¼Œâ€Harry Potterâ€æ˜¾ç„¶æ¯”â€theâ€æ›´é‡è¦ï¼Œå› ä¸ºå®ƒæŠŠè¿™äº›æ–‡æœ¬å’Œå…¶ä»–æ–‡æœ¬åŒºåˆ«å¼€ã€‚äºæ˜¯æˆ‘ä»¬å¸Œæœ›é™ä½è¿™äº›è¾ƒä¸ºå¸¸è§çš„å•è¯çš„æƒé‡å¹¶ä¸”æ›´åŠ é‡è§†é‚£äº›æ–‡æœ¬ç‰‡æ®µä¸­ç‹¬ç‰¹çš„å•è¯ã€‚ TF-IDFå°±å¯ä»¥åšåˆ°ä¸Šé¢è¿™ä¸€ç‚¹ã€‚é‚£ä¹ˆTD-IDFæ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿ å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‡è®¾æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªè¡¨æ ¼ï¼Œç¬¬ä¸€åˆ—æ˜¯æ–‡æœ¬ä¸­çš„tokenï¼Œç¬¬äºŒåˆ—æ˜¯å‡ºç°çš„é¢‘æ¬¡ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæ¥å®šä¹‰ä¸€ä¸‹TF-IDFç›¸å…³çš„ä¸€äº›æœ¯è¯­ï¼š TFï¼šæ–‡æœ¬ä¸­term Tå‡ºç°çš„æ¬¡æ•°/æ–‡æœ¬çš„termæ€»æ•° å› æ­¤ï¼ŒTF(This,Doucument1)=1/8,TF(This,Document2)=1/5ã€‚ TFè¡¨ç¤ºäº†è¿™ä¸ªå•è¯å¯¹è¿™ä¸ªæ–‡æœ¬çš„è´¡çŒ®ç¨‹åº¦ï¼Œæ¯”å¦‚è¯´å’Œæ–‡æœ¬æ›´ä¸ºç›¸å…³çš„å•è¯ï¼Œå®ƒçš„TFå€¼ä¼šæ¯”è¾ƒå¤§ï¼Œå› ä¸ºå®ƒä¼šæ›´é«˜é¢‘åœ°å‡ºç°åœ¨æ–‡æœ¬ä¸­ã€‚ DFï¼šlog(è¯­æ–™åº“ä¸­çš„æ–‡æœ¬æ€»æ•°/è¯­æ–™åº“ä¸­å«æœ‰term Tçš„æ–‡æœ¬æ•°) å› æ­¤ï¼ŒIDF(this)=log(2/2)=0ã€‚ ç†è®ºä¸Šæ¥è¯´ï¼Œå¦‚æœä¸€ä¸ªå•è¯åœ¨è¯­æ–™åº“æ‰€æœ‰çš„å•è¯ä¸­éƒ½å‡ºç°äº†ï¼Œé‚£ä¹ˆå¯èƒ½è¿™ä¸ªå•è¯å¯¹äºæŸä¸ªæˆ–æŸäº›ç‰¹å®šçš„æ–‡æœ¬å¹¶ä¸é‡è¦ï¼Œå³æ˜¯æˆ‘ä»¬æ‰€è¯´çš„é‚£ç±»æ¯”è¾ƒå¸¸è§çš„å•è¯ã€‚ä½†æ˜¯å¦‚æœä¸€ä¸ªå•è¯åªå‡ºç°è¯­æ–™åº“çš„ä¸€ä¸ªå­é›†çš„æ–‡æœ¬ä¸­å‡ºç°ï¼Œé‚£ä¹ˆè¿™ä¸ªå•è¯å¯¹äºé‚£äº›æ–‡æœ¬å…·æœ‰ä¸€å®šçš„ç›¸å…³æ€§ã€‚æ¯”å¦‚IDF(Messi)=log(2/1)=0.301ã€‚ ç”±æ­¤å¯è§ï¼Œå¯¹äºæ–‡æœ¬ç‰‡æ®µ1è€Œè¨€ï¼ŒTF-IDFæ–¹æ³•ç‹ ç‹ åœ°å¤„ç½šäº†â€thisâ€ä½†æ˜¯å´ç»™äºˆâ€Messiâ€æ›´é«˜çš„æƒé‡ã€‚å› æ­¤è¿™ä¸ªæ–¹æ³•èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬æ›´å¥½çš„ç†è§£â€Messiâ€æ˜¯æ–‡æœ¬ç‰‡æ®µ1çš„ä¸€ä¸ªé‡è¦å•è¯ã€‚ 2.1.3 å…·æœ‰å›ºå®šä¸Šä¸‹æ–‡çª—å£çš„å…±ç°çŸ©é˜µæŒ‡å¯¼æ€æƒ³ï¼šç›¸ä¼¼çš„å•è¯å¾€å¾€ä¸€èµ·å‡ºç°å¹¶ä¸”å…·æœ‰ç›¸ä¼¼çš„æ–‡æœ¬ç¯å¢ƒã€‚æ¯”å¦‚â€Apple is a fruitâ€,â€Mango is a fruitâ€ï¼Œè‹¹æœå’ŒèŠ’æœå€¾å‘äºæœ‰ä¸€ä¸ªç›¸ä¼¼çš„ä¸Šä¸‹åˆï¼Œå¦‚â€fruitâ€ã€‚ åœ¨æˆ‘æ·±å…¥ä¸€ä¸ªå…±ç°çŸ©é˜µæ˜¯å¦‚ä½•æ„å»ºçš„ç»†èŠ‚ä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰å¿…è¦å…ˆç†æ¸…ä¸¤ä¸ªæ¦‚å¿µã€‚ Co-occurenceï¼šå¯¹äºä¸€ä¸ªç»™å®šçš„è¯­æ–™åº“ï¼Œä¸€å¯¹å•è¯çš„å…±ç°ï¼Œæ¯”æ–¹è¯´w1å’Œw2çš„å…±ç°ï¼Œå°±æ˜¯åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ƒä»¬å…±åŒå‡ºç°çš„æ¬¡æ•°ã€‚ Context Windowï¼šä¸Šä¸‹æ–‡çª—å£çš„å‚æ•°ç”±æ•°å­—å’Œæ–¹å‘è®¾å®šã€‚æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­æ¥å¸®åŠ©ç†è§£ã€‚ Quick brown fox jump over the lazy dog Quick brown fox jump Over The lazy dog è¿™ä¸ªè¡¨æ ¼ç¬¬ä¸€è¡Œçš„æ–œä½“æ˜¯foxçš„é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œç¬¬äºŒè¡Œçš„æ–œä½“æ˜¯overçš„é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ã€‚ ç°åœ¨ï¼Œæˆ‘ä»¬ç¼–ä¸€ä¸ªç”¨æ¥è®¡ç®—å…±ç°çŸ©é˜µçš„è¯­æ–™åº“ã€‚ è¯­æ–™åº“ï¼šHe is not lazy. He is intelligent. He is smart. è®©æˆ‘ä»¬é€šè¿‡çœ‹ä¸Šé¢ä¸¤ä¸ªçº¢ã€è“ç€è‰²çš„ä¾‹å­æ¥ç†è§£å…±ç°çŸ©é˜µã€‚ çº¢è‰²æ ¼å­æ‰€è¡¨ç¤ºçš„ï¼Œæ˜¯â€Heâ€å’Œâ€isâ€åœ¨é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œçº¢è‰²æ ¼å­ä¸­çš„æ•°å­—ä¸º4ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„è¡¨æ ¼å¯è§†åŒ–è¿™ä¸ªè®¡æ•°è¿‡ç¨‹ã€‚ï¼ˆå³å‡ºç°æ— éœ€ç´§æŒ¨ç€ï¼Œåªè¦éƒ½åœ¨çª—å£ä¸­ï¼Œå³ä½¿é¡ºåºé¢ å€’ï¼Œéƒ½æ˜¯å¯ä»¥çš„ï¼‰ã€‚ è“è‰²æ ¼å­æ‰€è¡¨ç¤ºçš„ï¼Œæ˜¯â€lazyâ€å’Œâ€intelligentâ€åœ¨é•¿åº¦ä¸º2çš„ä¸Šä¸‹æ–‡çª—å£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œå…¶ä¸­æ•°å­—ä¸º0ï¼Œè¡¨ç¤ºå®ƒä»¬ä»ä¸æ›¾åŒæ—¶å‡ºç°åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­ã€‚ å…±ç°çŸ©é˜µçš„å˜ä½“å‡è®¾è¯­æ–™åº“ä¸­æœ‰Vä¸ªç‹¬ç‰¹çš„å•è¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„è¯æ±‡é•¿åº¦å³ä¸ºVã€‚ä¸‹é¢ç»™å‡ºäº†å…±ç°çŸ©é˜µçš„ä¸¤ç§ä¸åŒå˜ä½“ï¼š å¤§å°ä¸ºVxVçš„å…±ç°çŸ©é˜µã€‚ä½†æ˜¯ç”±äºè¿™æ ·çš„çŸ©é˜µå¤ªå¤§è€Œéš¾äºè®¡ç®—ï¼Œæ‰€ä»¥å®é™…ä¸­è¿™ç±»å»ºæ¨¡å¹¶ä¸è¢«çœ‹å¥½ã€‚ å¤§å°ä¸ºVxNçš„å…±ç°çŸ©é˜µã€‚Nè¡¨ç¤ºå»é™¤æ‰åœç”¨è¯ç­‰ä¸ç›¸å…³è¯æ±‡åçš„Vçš„ä¸€ä¸ªè‡ªå·±ã€‚ä½†æ˜¯è¿™ç±»å»ºæ¨¡ä¹Ÿä¾ç„¶å¾ˆå¤§ä¸”éš¾äºè®¡ç®—ã€‚ è¿™é‡Œè¦å¼ºè°ƒä¸€ç‚¹ï¼Œå…±ç°çŸ©é˜µå¹¶éæ˜¯æˆ‘ä»¬å¹¿æ³›ä½¿ç”¨çš„è¯å‘é‡ã€‚ç›¸åï¼Œè¿™ç±»å…±ç°çŸ©é˜µå¸¸å¸¸ä¸è¯¸å¦‚PCAï¼ŒSVDè¿™æ ·çš„æŠ€æœ¯ç»„åˆä½¿ç”¨åœ¨å› ç´ åˆ†è§£ä¸Šã€‚è€Œè¿™äº›å› ç´ åˆ†è§£çš„ç»„åˆæ„æˆäº†è¯å‘é‡è¡¨ç¤ºã€‚ è¯´å¾—æ›´æ˜ç™½äº›ï¼Œä½ åœ¨ä¸‹é¢VxVçš„å…±ç°çŸ©é˜µä¸Šä½¿ç”¨äº†PCAï¼Œä½ ä¼šå¾—åˆ°Vä¸ªä¸»å…ƒç´ ã€‚å¦‚æ­¤ï¼Œä½ å¯ä»¥ä»è¿™Vä¸ªå…ƒç´ ä¸­é€‰å‡ºkä¸ªã€‚å› æ­¤ï¼Œä½ è®²å¾—åˆ°ä¸€ä¸ªVxkçš„æ–°å‰§è¯ã€‚è¿™æ ·ï¼Œè™½ç„¶ä¸€ä¸ªå•è¯æ˜¯ç”±kç»´è¡¨ç¤ºçš„è€Œä¸æ˜¯Vç»´è¡¨ç¤ºçš„ï¼Œä½†æ˜¯å®ƒä¾ç„¶èƒ½æ•æ‰åˆ°å‡ ä¹åŒæ ·çš„æ„ä¹‰ã€‚ å› æ­¤ï¼ŒPCAèƒŒåçš„æ“ä½œå®é™…ä¸Šå°±æ˜¯è®²å…±ç°çŸ©é˜µæ‹†è§£ä¸ºä¸‰ä¸ªçŸ©é˜µï¼ŒUï¼ŒSå’ŒVã€‚ å…±ç°çŸ©é˜µçš„ä¼˜ç‚¹ å®ƒè•´å«äº†å•è¯ä¹‹é—´çš„è¯­ä¹‰è”ç³»ã€‚æ¯”å¦‚â€ç”·äººâ€å’Œâ€å¥³äººâ€ä¼šæ¯”â€ç”·äººâ€å’Œâ€è‹¹æœâ€æ›´è¿‘ã€‚ å®ƒçš„æ ¸å¿ƒåœ¨äºä½¿ç”¨SVDæ¥åˆ›é€ å‡ºæ¯”ç°å­˜æ–¹æ³•æ›´å‡†ç¡®çš„è¯å‘é‡è¡¨ç¤ºã€‚ å®ƒä½¿ç”¨å› å­åˆ†è§£ï¼Œå› å­åˆ†è§£æ˜¯ä¸€ä¸ªè‰¯å®šä¹‰é—®é¢˜å¹¶ä¸”å¯ä»¥è¢«æœ‰æ•ˆè§£å†³ã€‚ å®ƒåªè¦è¢«è®¡ç®—ä¸€æ¬¡ï¼Œä¹‹åä»»ä½•æ—¶å€™éƒ½å¯ä»¥è¢«ä½¿ç”¨ã€‚åœ¨è¿™ä¸ªæ„ä¹‰ä¸Šï¼Œå®ƒæ¯”å…¶ä»–æ–¹æ³•æ›´å¿«ã€‚ å…±ç°çŸ©é˜µçš„ç¼ºç‚¹ å®ƒéœ€è¦å·¨å¤§çš„å†…å­˜å»å­˜å‚¨ã€‚ 2.2 åŸºäºé¢„æµ‹çš„word embeddingå‰é¢æ‰€è¯´çš„åŸºäºé¢‘ç‡çš„word embeddingæœ‰å„ç§å„æ ·çš„å±€é™ã€‚è€ŒåMitolovç­‰äººå°†word2vecä»‹ç»åˆ°nlpçš„å„ä¸ªé¢†åŸŸä¸­ï¼Œä½¿å¾—åŸºäºé¢„æµ‹çš„word embeddingèµ°ä¸Šå†å²èˆå°ã€‚è¿™äº›æ–¹æ³•æ˜¯åŸºäºé¢„æµ‹çš„ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒä»¬ä¼šç»™å‡ºå„ä¸ªå•è¯çš„æ¦‚ç‡ã€‚å®ƒä»¬åœ¨è¯æ±‡ç›¸ä¼¼åº¦çš„ä»»åŠ¡ä¸Šè¡¨ç°å¾—éå¸¸å¥½ï¼Œç”šè‡³èƒ½è¾¾åˆ°King -man +woman = Queenè¿™æ ·çš„ç¥å¥‡æ•ˆæœã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°±æ¥çœ‹çœ‹word2vecå…·ä½“æ˜¯å¦‚ä½•å¾—å‡ºè¯å‘é‡çš„ã€‚ word2vecå¹¶ä¸æ˜¯ä¸€ä¸ªå•ç‹¬çš„ç®—æ³•ï¼Œå®ƒæ˜¯ç”±CBOWå’ŒSkip-gramæ¨¡å‹ç»„åˆè€Œæˆçš„ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹éƒ½æ˜¯ç”±è¯åˆ°è¯çš„æµ…å±‚ç¥ç»ç½‘ç»œç»„æˆçš„ã€‚å®ƒä»¬æ‰€å­¦ä¹ çš„æƒé‡å°†æˆä¸ºå•è¯çš„å‘é‡è¡¨ç¤ºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±åˆ†åˆ«ä»‹ç»è¿™ä¸¤ç§æ¨¡å‹ã€‚ 2.2.1 CBOW è¿ç»­è¯è¢‹æ¨¡å‹CBOWæ˜¯åŸºäºè¯­å¢ƒï¼ˆæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼‰æ¥é¢„æµ‹å‡ºä¸€ä¸ªå•è¯çš„æ¦‚ç‡ã€‚è¿™ä¸ªè¯­å¢ƒå¯èƒ½æ˜¯ä¸€ä¸ªå•è¯æˆ–è€…ä¸€ç»„è¯ã€‚ä½†ä¸ºäº†ä»‹ç»çš„ç®€å•ï¼Œæˆ‘ä»¬è¿™è¾¹ä»¥ä¸€ä¸ªå•è¯ä½œä¸ºè¯­å¢ƒå¹¶ä¸€æ¬¡æ¥é¢„æµ‹ä¸€ä¸ªç›®æ ‡è¯æ±‡ã€‚ å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¯­æ–™åº“C=â€Hey, this is sample corpus using only one context word.â€ã€‚å¹¶ä¸”æˆ‘ä»¬å·²ç»å®šä¹‰äº†ä¸Šä¸‹æ–‡çª—å£å¤§å°ä¸º1ã€‚è¿™ä¸ªè¯­æ–™åº“å°†ä¼šè¢«è½¬æ¢æˆä¸‹å›¾æ‰€ç¤ºçš„è®­ç»ƒé›†åˆã€‚è¾“å…¥å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä¸‹å›¾ä¸­å³è¾¹çš„çŸ©é˜µåŒ…å«äº†å·¦è¾¹çš„è¾“å…¥çš„ç‹¬çƒ­ç¼–ç ã€‚ æ¯”å¦‚è¯´isçš„ç›®æ ‡çš„è¾“å…¥ä¸º[0001000000]ã€‚ ä¸Šå›¾æ‰€æ˜¾ç¤ºçš„çŸ©é˜µå°†ä¼šè¢«é€å…¥ä¸€ä¸ªç”±ä¸‰å±‚ç»„æˆçš„æµ…å±‚ç¥ç»ç½‘ç»œï¼šä¸€ä¸ªè¾“å…¥å±‚ï¼Œä¸€ä¸ªéšè—å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ã€‚è¾“å‡ºå±‚ä¼šä½¿ç”¨softmaxå‡½æ•°ï¼Œsoftmaxå‡½æ•°æ˜¯ä¸€ä¸ªç”¨äºåˆ†ç±»é¢„æµ‹çš„å¸¸ç”¨å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å¾—å‡ºçš„å„ä¸ªç±»åˆ«çš„æ•°å€¼æ€»å’Œä¸º1ã€‚ æµç¨‹æ˜¯è¿™æ ·çš„ï¼š è¾“å…¥å±‚å’Œç›®æ ‡å±‚éƒ½æ˜¯ç”±1xVçš„ç‹¬çƒ­ç¼–ç ç»„æˆï¼Œè¿™é‡ŒV=10ã€‚ è¿™é‡Œæœ‰ä¸¤å¥—æƒé‡ï¼Œä¸€å¥—æ˜¯è¾“å…¥å±‚å’Œéšè—å±‚ä¹‹é—´çš„æƒé‡ï¼Œä¸€å¥—æ˜¯éšè—å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„æƒé‡ã€‚input-hidden å±‚çŸ©é˜µå¤§å°ä¸ºVxN, hidden-output é‚£å±‚çš„çŸ©é˜µå¤§å°ä¸ºNXV ï¼šè¿™é‡Œï¼ŒNæŒ‡çš„æ˜¯æˆ‘ä»¬é€‰æ‹©ç”¨æ¥è¡¨è¾¾å•è¯çš„ç»´åº¦ã€‚å®ƒæ˜¯ä»»æ„çš„ï¼Œå¹¶ä¸”æ˜¯ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªè¶…å‚æ•°ã€‚å¹¶ä¸”ï¼ŒNæ˜¯ä¹Ÿæ˜¯éšè—å±‚çš„èŠ‚ç‚¹æ•°ï¼Œè¿™é‡Œï¼Œæˆ‘ä»¬è®¾N=4ã€‚ ä»»æ„ä¸€å±‚ä¹‹é—´éƒ½ä¸å­˜åœ¨æ¿€æ´»å‡½æ•°ã€‚ è¾“å…¥ä¸ input-hiddenå±‚æƒé‡çš„ä¹˜ç§¯è¢«ç§°ä¸º hidden activationã€‚ Hidden input ä¸hidden-outputå±‚æƒé‡ç›¸ä¹˜ï¼Œå¾—åˆ°è¾“å‡ºã€‚ è¾“å‡ºå±‚å’Œç›®æ ‡ä¹‹é—´çš„è¯¯å·®å°†ä¼šè¢«ç”¨æ¥è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»¥è°ƒæ•´weightsã€‚ åœ¨éšè—å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´æƒé‡å°†ä¼šæˆä¸ºè¯å‘é‡ã€‚ ä¸Šé¢çš„æµç¨‹æ˜¯é’ˆå¯¹äºä¸Šä¸‹æ–‡çª—å£ä¸º1çš„ï¼Œä¸‹å›¾æ˜¾ç¤ºäº†ä¸Šä¸‹æ–‡çª—å£å¤§äº1çš„æƒ…å†µã€‚ ä¸‹å›¾æ˜¯ä¸ºäº†æ›´å¥½ç†è§£è¿™ä¸ªç»“æ„çš„çŸ©é˜µå›¾ä¾‹ã€‚ å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬è¦ä½¿ç”¨3ä¸ª context wordå»é¢„æµ‹ç›®æ ‡å•è¯çš„æ¦‚ç‡ã€‚è¾“å…¥å±‚æ˜¯3ä¸ª[1xV]çš„å‘é‡ï¼Œè€Œè¾“å‡ºæ˜¯ä¸€ä¸ª[1xV]ã€‚å‰©ä¸‹çš„æ„é€ å’Œ1-contextçš„CBOWæ˜¯ä¸€æ ·çš„ã€‚ ä½†æ˜¯åœ¨éšè—å±‚ä¸­ï¼Œ3-context wordçš„æ¨¡å‹ä¸å†æ˜¯ç®€å•å¤åˆ¶è¾“å…¥ï¼Œè€Œæ˜¯è¦è¿›è¡Œä¸€ä¸ªå¹³å‡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸Šé¢çš„å›¾æ¥ç†è§£è¿™ä¸€ç‚¹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰3ä¸ªcontext word, é‚£ä¹ˆæˆ‘ä»¬å°†ä¼šæœ‰3ä¸ª åˆæ­¥çš„hidden activation ç„¶åæœ€åå¹³å‡å¾—åˆ°æœ€ç»ˆçš„ hidden activiationã€‚ é‚£ä¹ˆCBOWå’Œä¸€èˆ¬çš„MLPä¹‹é—´æœ‰ä½•ä¸åŒå‘¢ï¼Ÿ CBOWçš„ç›®æ ‡å‡½æ•°å’ŒMLPä¸åŒï¼ŒCBOWæ˜¯ç›®æ ‡å‡½æ•°è´Ÿçš„æœ€å¤§ä¼¼ç„¶ã€‚ è¯¯å·®æ¢¯åº¦ä¸ä¸€æ ·ï¼Œå› ä¸ºMLPä»¥sigmoidä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œè€ŒCBOWä¸€èˆ¬æ˜¯çº¿æ€§æ¿€æ´»ã€‚ CBOWçš„ä¼˜ç‚¹ï¼š åŸºäºæ¦‚ç‡çš„ï¼Œæ›´ç¬¦åˆå®é™…ï¼› å ç”¨å†…å­˜å°ã€‚ CBOWçš„ç¼ºç‚¹ï¼š CBOWæ˜¯åˆ©ç”¨å•è¯çš„è¯­å¢ƒæ¥è¡¨ç¤ºå•è¯çš„ï¼Œä½†æ˜¯å¯¹äºå¤šä¹‰è¯è€Œè¨€ï¼Œæ¯”å¦‚è‹¹æœæ—¢æ˜¯æ°´æœä¹Ÿæ˜¯ä¸€å®¶å…¬å¸ï¼Œä½†æ˜¯ç”±äºCBOWå°†è¿™ä¸ªæ–‡æœ¬éƒ½è€ƒè™‘è¿›å»ï¼Œæ‰€ä»¥è‹¹æœè¢«è¡¨ç¤ºåœ¨æ°´æœå’Œå…¬å¸ä¹‹é—´äº†ã€‚ ä»å¤´è®­ç»ƒä¸€ä¸ªCBOWè‹¥æ²¡æœ‰å¾ˆå¥½ä¼˜åŒ–çš„è¯å°†è®­ç»ƒå¾ˆä¹…ã€‚ 2.2.2 Skip-Gram modelSkip-gram çš„ç±»å‹ä¸CBOWä¸€æ ·ï¼Œä½†æ˜¯å®ƒçš„ç»“æ„æ˜¯åè¿‡æ¥çš„ï¼Œå®ƒæ˜¯åŸºäºç»™å®šå•è¯å»é¢„æµ‹å•è¯çš„ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬ä¾ç„¶ç”¨è®²è§£CBOWæ—¶ä½¿ç”¨çš„è¯­æ–™ã€‚ Skip-gramçš„è¾“å…¥å’Œ1-contextçš„CBOW æ¨¡å‹å¾ˆç±»ä¼¼ã€‚å¹¶ä¸”éšè—å±‚çš„activitiaonä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚ä¸åŒçš„ä»…ä»…æ˜¯ç›®æ ‡å˜é‡ã€‚å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å•è¯ä¸¤è¾¹éƒ½å®šä¹‰äº†ä¸€ä¸ªé•¿åº¦ä¸º1çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šæœ‰ä¸¤ä¸ªç‹¬çƒ­ç¼–ç çš„ç›®æ ‡å˜é‡å’Œä¸¤ä¸ªç›¸åº”çš„è¾“å‡ºã€‚ è€Œè¿™ä¸¤ä¸ªç›®æ ‡å˜é‡çš„è¯¯å·®å°†ä¼šè¢«åˆ†åˆ«è®¡ç®—ï¼Œç„¶åå°†ä¸¤ä¸ªè¯¯å·®åŠ èµ·æ¥è¿›è¡Œåå‘ä¼ æ’­ã€‚ Skip-Gram æ¨¡å‹çš„ä¼˜ç‚¹ å¯ä»¥æŠ“ä½ä¸€ä¸ªå•è¯çš„å¤šä¸ªä¹‰é¡¹ã€‚ ä½¿ç”¨è´Ÿé‡‡æ ·çš„Skip-Gramæ¨¡å‹ä¼šæ¯”å…¶ä»–æ–¹æ³•æ›´é«˜æ•ˆã€‚ Skip-Gramæ¨¡å‹çš„ç¼ºç‚¹ ä¾èµ–è¯­æ–™åº“çš„å¤§å° é‡‡æ ·æ˜¯å¯¹ç»Ÿè®¡æ•°æ®çš„ä½æ•ˆåˆ©ç”¨","categories":[{"name":"NLP","slug":"NLP","permalink":"http://shamy1997.github.io/categories/NLP/"}],"tags":[{"name":"Word Embedding","slug":"Word-Embedding","permalink":"http://shamy1997.github.io/tags/Word-Embedding/"}]},{"title":"è®ºæ–‡é˜…è¯» | Deep Semantic Role Labeling:What Works and Whatâ€™s Next","slug":"srl-paper-reading-md","date":"2019-04-10T07:10:50.000Z","updated":"2019-04-10T08:08:27.967Z","comments":true,"path":"passages/srl-paper-reading-md/","link":"","permalink":"http://shamy1997.github.io/passages/srl-paper-reading-md/","excerpt":"","text":"ä»€ä¹ˆæ˜¯SRLSemantic Role Labeling ä»»åŠ¡æŒ‡çš„æ˜¯å›´ç»•ç€è°“è¯æ ‡è®°ä¸€å¥è¯çš„è®ºå…ƒä¿¡æ¯ï¼Œè¯†åˆ«å‡ºwhatï¼Œwhoï¼Œwhomï¼Œwhenï¼Œwhereç­‰ä¿¡æ¯ã€‚è¿™æ˜¯ä¸€é¡¹æ ‡è®°å¥å­äº‹ä»¶çš„æµ…å±‚è¯­ä¹‰å¤„ç†ï¼Œä¸æ¶‰åŠå¥å­çš„å¥æ³•åˆ†æã€‚æ¯”å¦‚å¯¹äºâ€œä»–æ˜¨å¤©æŠŠä¹¦äº¤ç»™äº†å¼ ä¸‰â€å’Œâ€œæ˜¨å¤©ä¹¦è¢«ä»–äº¤ç»™äº†å¼ ä¸‰â€è¿™ä¸¤å¥è¯ï¼Œå®ƒä»¬åœ¨å¥æ³•ä¸Šä¸ä¸€æ ·ï¼Œä½†æ˜¯åœ¨è¯­ä¹‰è§’è‰²æ ‡æ³¨ä¸Šæ˜¯ä¸€æ ·çš„ã€‚ è¯­ä¹‰è§’è‰²æ ‡æ³¨æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„åº•å±‚ä»»åŠ¡ï¼Œé€šè¿‡è¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è·å–åˆ°ä¸€å¥è¯äº‹ä»¶æ€§çš„ä¿¡æ¯ï¼Œå¦‚æœèƒ½å¤Ÿå¤„ç†å¥½ï¼Œå°†å¯¹è‡ªåŠ¨é—®ç­”ã€è‡ªåŠ¨æ–‡æ‘˜ç­‰ä»»åŠ¡äº§ç”Ÿç›´æ¥è€Œæœ‰åŠ›çš„å¸®åŠ©ã€‚ æœ¬æ–‡çš„æ¨¡å‹Luçš„æ¨¡å‹ä¹‹æ‰€ä»¥èƒ½å¤Ÿæ¯”åŸæœ‰çš„ç³»ç»Ÿæœ‰é‚£ä¹ˆå¤§çš„æå‡ï¼Œå¥¹è®¤ä¸ºä¸»è¦åŸå› æ˜¯ä¸¤æ–¹é¢ï¼Œä¸€æ–¹é¢æ˜¯ä½¿ç”¨äº†ä¼˜åŒ–è¿‡çš„BiLSTMæ¨¡å‹ï¼Œå¦ä¸€æ–¹é¢æ˜¯å¯¹è¾“å‡ºè¿›è¡Œäº†ä¼˜åŒ–ç¼–ç ã€‚ ä¼˜åŒ–è¿‡çš„BiLSTMæ¨¡å‹Inputã€Output &amp; Function è®­ç»ƒè¾“å…¥$(w,v)$ã€‚ $w$ä»£è¡¨è¯å‘é‡ï¼Œæœ¬æ–‡ä½¿ç”¨çš„æ˜¯GLoVe embeddingï¼Œç„¶å$v$ä»£è¡¨æ˜¯å¦æ˜¯predicateï¼ˆè°“è¯ï¼‰ï¼Œè‹¥æ˜¯ï¼Œåˆ™ä¸º1ï¼Œå¦ä¸º0ï¼Œä¸¤ä¸ªéƒ½æ˜¯100 dimã€‚ è¾“å‡ºæ˜¯y(BIO-tagger) Scoring Functionï¼š $f(\\boldsymbol{w}, \\boldsymbol{y})=\\sum_{t=1}^{n} \\log p\\left(y_{t} | \\boldsymbol{w}\\right)-\\sum_{c \\in \\mathcal{C}} c\\left(\\boldsymbol{w}, y_{1 : t}\\right)$ å¯èƒ½æ€§å‡å»æƒ©ç½šå€¼ã€‚å› ä¸ºè¾“å‡ºçš„ç»“æœæœ‰ä¸€äº›é™åˆ¶ï¼Œè¿™äº›åé¢ä¼šè®²ã€‚ ä¸ºä½¿ç›®æ ‡å‡½æ•°æœ€å¤§è¿›è¡Œå‰å‘åé¦ˆå’Œåå‘åé¦ˆè¿›è¡Œè®­ç»ƒã€‚ BiLSTMBiLSTM çš„å†…éƒ¨æ„é€ å°±æ˜¯ç®€å•çš„LSTMåªä¸è¿‡å åŠ äº†ä¸¤å±‚ï¼Œå³ä¸€ä¸ªå•å…ƒä¼šæ”¶åˆ°å‰è¯ä¿¡æ¯ä¹Ÿä¼šæ”¶åˆ°åè¯ä¿¡æ¯ã€‚ \\begin{aligned} \\boldsymbol{i}_{l, t} &=\\sigma\\left(\\mathbf{W}_{\\mathrm{i}}^{l}\\left[\\boldsymbol{h}_{l, t+\\delta_{l}}, \\boldsymbol{x}_{l, t}\\right]+\\boldsymbol{b}_{\\mathrm{i}}^{l}\\right) \\\\ \\boldsymbol{o}_{l, t} &=\\sigma\\left(\\mathbf{W}_{\\mathrm{o}}^{l}\\left[\\boldsymbol{h}_{l, t+\\delta_{l}}, \\boldsymbol{x}_{l, t}\\right]+\\boldsymbol{b}_{\\mathrm{o}}^{l}\\right) \\\\ \\boldsymbol{f}_{l, t} &=\\sigma\\left(\\mathbf{W}_{\\mathrm{f}}^{l}\\left[\\boldsymbol{h}_{l, t+\\delta_{l}}, \\boldsymbol{x}_{l, t}\\right]+\\boldsymbol{b}_{\\mathrm{f}}^{l}+1\\right) \\\\ \\tilde{\\boldsymbol{c}}_{l, t} &=\\tanh \\left(\\mathbf{W}_{\\mathrm{c}}^{l}\\left[\\boldsymbol{h}_{l, t+\\delta_{l}}, \\boldsymbol{x}_{l, t}\\right]+\\boldsymbol{b}_{\\mathrm{c}}^{l}\\right) \\end{aligned}\\begin{aligned} \\boldsymbol{c}_{l, t} &=\\boldsymbol{i}_{l, t} \\circ \\tilde{\\boldsymbol{c}}_{l, t}+\\boldsymbol{f}_{l, t} \\circ \\boldsymbol{c}_{t+\\delta_{l}} \\\\ \\boldsymbol{h}_{l, t} &=\\boldsymbol{o}_{l, t} \\circ \\tanh \\left(\\boldsymbol{c}_{l, t}\\right) \\end{aligned}\\boldsymbol{x}_{l, t}=\\left\\{\\begin{array}{ll}{\\left[\\mathbf{W}_{\\mathrm{emb}}\\left(w_{t}\\right), \\mathbf{W}_{\\mathrm{mask}}(t=v)\\right]} & {l=1} \\\\ {\\boldsymbol{h}_{l-1, t}} & {l>1}\\end{array}\\right.\\delta_{l}=\\left\\{\\begin{array}{ll}{1} & {\\text { if } l \\text { is even }} \\\\ {-1} & {\\text { otherwise }}\\end{array}\\right. Recurrent dropoutä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨dropoutçš„æ–¹æ³•ã€‚è¿‡å»çš„dropoutæˆ‘ä»¬å¤§å¤šä½¿ç”¨éšæœºç”Ÿæˆï¼Œä½†æ˜¯åœ¨è¿™æ ·å¤æ‚çš„ç½‘ç»œä¸­ï¼Œå¦‚æœé‡‡å–ä¹‹å‰çš„åšæ³•ä¼šè®©æ¨¡å‹è®­ç»ƒçš„å™ªå£°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Recurrent dropoutï¼Œè¿™ç§dropoutæ¯å±‚éƒ½æ˜¯ä¸€æ ·çš„ï¼ˆsharedï¼‰ï¼Œå› æ­¤å¯ä»¥å‡å°‘å™ªå£°ï¼Œè¾¾åˆ°é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ•ˆæœã€‚ \\begin{aligned} \\widetilde{\\boldsymbol{h}}_{l, t} &=\\boldsymbol{r}_{l, t} \\circ \\boldsymbol{h}_{l, t}^{\\prime}+\\left(1-\\boldsymbol{r}_{l, t}\\right) \\circ \\mathbf{W}_{\\mathrm{h}}^{l} \\boldsymbol{x}_{l, t} \\\\ \\boldsymbol{h}_{l, t} &=\\boldsymbol{z}_{l} \\circ \\widetilde{\\boldsymbol{h}}_{l, t} \\end{aligned} Constrained A* decodingç»è¿‡softmaxå±‚ä¹‹åï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œä½†æ˜¯å¹¶éé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„é‚£ä¸ªtagå°±æ˜¯æˆ‘ä»¬æ‰€è¦çš„tagï¼Œå› ä¸ºå‰è¯åè¯çš„tagé€‰æ‹©å¹¶éç‹¬ç«‹ï¼Œè€Œæ˜¯ä¼šç›¸äº’å½±å“çš„ï¼Œæ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æœ€åé€‰æ‹©tagæ—¶ä¼šæ”¶åˆ°ä¸€äº›é™åˆ¶ã€‚ ä½œè€…ä¸»è¦è®²äº†æœ‰ä¸‰ç§é™åˆ¶ï¼š ç¬¬ä¸€æ˜¯BIOæ ‡ç­¾ä½“ç³»çš„é™åˆ¶ï¼Œæ¯”å¦‚I-tagä¸èƒ½åœ¨B-å‰é¢ï¼› ç¬¬äºŒæ˜¯è¯­ä¹‰è§’è‰²ä¸Šçš„é™åˆ¶ï¼Œæ¯”å¦‚æ ¸å¿ƒçš„è¯­ä¹‰è§’è‰²AG0-AG5ï¼Œåœ¨åªæœ‰ä¸€ä¸ªè°“è¯çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæœ€å¤šå‡ºç°1æ¬¡ï¼› ç¬¬ä¸‰æ˜¯å¥æ³•ä¸Šçš„é™åˆ¶ï¼Œæ¯”å¦‚å¥æ³•ä¸Šä¸åŒåœ¨ä¸€ä¸ªçˆ¶èŠ‚ç‚¹ä¸­çš„ä¸¤ä¸ªè®ºå…ƒä¸èƒ½è¢«æ ‡è®°ä¸ºB-Xï¼ŒI-Xï¼ˆXæŒ‡æœ‰åŒæ ·çš„è¯­ä¹‰è§’è‰²ï¼‰ã€‚ é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…ç»™å‡ºäº†ä¸€ä¸ªæƒ©ç½šå‡½æ•°æ¥æ§åˆ¶æœ€åçš„åˆ†æ•°ï¼Œå¥¹å¸Œæœ›é€‰å‡ºåœ¨è€ƒè™‘äº†è¿™äº›é™åˆ¶ä¹‹åæ¦‚ç‡æœ€å¤§çš„ç»“æœã€‚ ç»“æœæ¯”ä»¥å‰çš„æ¨¡å‹çš„F1æé«˜äº†10%ã€‚å¹¶é€šè¿‡å®éªŒè¯æ˜äº†ï¼š Deep-BiLSTM å¯ä»¥å¾ˆå¥½åœ°è§£å†³è¯­ä¹‰è§’è‰²æ ‡æ³¨ä¸­é•¿è·ç¦»ä¾å­˜çš„é—®é¢˜ï¼› è®­ç»ƒæ—¶å¯¹æƒé‡è¿›è¡Œéšæœºæ­£äº¤åˆ†è§£èƒ½å¤Ÿä½¿è®­ç»ƒæ›´å¿«å¼€å§‹ï¼› å¥æ³•ä¿¡æ¯å¯¹è¯­ä¹‰è§’è‰²æ ‡æ³¨æ˜¯æœ‰ç”¨çš„ï¼Œæœªæ¥å¯ä»¥è€ƒè™‘åœ¨æƒ©ç½šå‡½æ•°ä¸­ä¼˜åŒ–ï¼Œæˆ‘è§‰å¾—å°±æ˜¯èƒ½å°†ä¹‹å‰ç‰¹å¾å·¥ç¨‹ä¸­æ‰€æ€»ç»“çš„ä¸€äº›æ¡ä»¶è§„åˆ’åˆ°è¿™ä¸ªæ¨¡å‹é‡Œæ¥ã€‚ åç»­å­¦ä¹  è¿™ä¸ªç®—æ³•æ¨¡å‹å·²ç»è¢«æ•´åˆåˆ°AllenNLPä¸­ï¼Œå¯ä»¥å­¦ä¹ ä¸‹å¦‚ä½•åœ¨æœ¬åœ°ä½¿ç”¨ï¼› å¦‚ä½•è¿ç§»åˆ°ä¸­æ–‡ä»»åŠ¡ä¸­ï¼Ÿ å‚è€ƒèµ„æ–™ è®ºæ–‡ Luå…³äºè¿™ä¸ªæ¨¡å‹çš„talk(æ²¹ç®¡)","categories":[{"name":"SRL","slug":"SRL","permalink":"http://shamy1997.github.io/categories/SRL/"}],"tags":[{"name":"SRL","slug":"SRL","permalink":"http://shamy1997.github.io/tags/SRL/"},{"name":"paper-reading","slug":"paper-reading","permalink":"http://shamy1997.github.io/tags/paper-reading/"},{"name":"NLP","slug":"NLP","permalink":"http://shamy1997.github.io/tags/NLP/"}]},{"title":"ç»ˆç«¯å‘½ä»¤ç¬”è®°","slug":"about-terminal","date":"2018-12-22T12:37:49.000Z","updated":"2018-12-22T13:02:22.472Z","comments":true,"path":"passages/about-terminal/","link":"","permalink":"http://shamy1997.github.io/passages/about-terminal/","excerpt":"","text":"vi å¸¸ç”¨å‘½ä»¤1. \bè¿›å…¥ vi ç¼–è¾‘å™¨1sudo vi &lt;path&gt; 2. ä¿®æ”¹å†…å®¹è¾“å…¥iï¼Œè¿›å…¥insertæ¨¡å¼ã€‚æŒ‰escï¼Œé€€å‡ºæ¨¡å¼ã€‚ 3. ä¿å­˜ï¼Œé€€å‡º :w ä¿å­˜æ–‡ä»¶ä½†ä¸é€€å‡ºvi; :wq ä¿å­˜æ–‡ä»¶å¹¶é€€å‡ºvi; q: ä¸ä¿å­˜æ–‡ä»¶ï¼Œé€€å‡ºvi :e! æ”¾å¼ƒæ‰€æœ‰ä¿®æ”¹ï¼Œä»ä¸Šæ¬¡ä¿å­˜æ–‡ä»¶å¼€å§‹å†ç¼–è¾‘ viå‘½ä»¤å¤§å…¨ Jupyter or conda not foundæˆ‘å®‰è£…å¥½anacondaåï¼Œæ‰“ç®—ç”¨å‘½ä»¤è¡Œç›´æ¥æ‰“å¼€jupyter notebookï¼Œç»“æœå´æ²¡æœ‰æˆåŠŸï¼Œç½‘ä¸Šä¸€èˆ¬çš„è§£é‡Šæ˜¯è¦æŠŠanacondaé…ç½®åˆ°ç¯å¢ƒå˜é‡é‡Œï¼šåœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š 1sudo vi ~/.bash_profile æ‰“å¼€ååœ¨æœ«å°¾åŠ ä¸Šï¼š 1234567export PATH='~/anaconda/bin:$PATH'# è¿™é‡Œçš„pathè¦æ ¹æ®anacondaæ‰€åœ¨çš„ä½ç½®å®šä¹‰source ~/.bash_profile# è¡¨ç¤ºä¿®æ”¹ç«‹å³ç”Ÿæ•ˆ ä½†æ˜¯å‘¢ï¼Œæˆ‘è¯•äº†å¥½å‡ æ¬¡éƒ½æ²¡æœ‰æˆåŠŸï¼Œäº‹å®ä¸Šï¼Œæ˜¯æˆ‘é…ç½®äº†oh-my-zshçš„åŸå› ã€‚ å› æ­¤ï¼Œæ­£ç¡®çš„è§£å†³æ–¹æ³•æ˜¯ï¼Œæ‰“å¼€~/.zshrcï¼Œç„¶ååœ¨æ–‡ä»¶æœ€åä¸€è¡Œæ·»åŠ ï¼š 1export PATH=$PATH:$HOME/anaconda/bin ä¿å­˜æ–‡ä»¶åï¼Œå…³é—­çª—å£\bï¼Œé‡æ–°å¼€å¯çª—å£æ—¶ï¼Œè¾“å…¥å‘½ä»¤conda --væ¥æ£€æµ‹æ˜¯å¦æˆåŠŸã€‚ å‚è€ƒé“¾æ¥ğŸ”— zsh not found1exec /bin/zsh å‚è€ƒèµ„æ–™ğŸ”—","categories":[],"tags":[{"name":"å‘½ä»¤è¡Œ","slug":"å‘½ä»¤è¡Œ","permalink":"http://shamy1997.github.io/tags/å‘½ä»¤è¡Œ/"}]},{"title":"Slot Filling with SimpleRNN","slug":"slot-filling","date":"2018-09-10T02:20:20.000Z","updated":"2019-01-20T12:04:25.961Z","comments":true,"path":"passages/slot-filling/","link":"","permalink":"http://shamy1997.github.io/passages/slot-filling/","excerpt":"","text":"ä»€ä¹ˆæ˜¯Slot Fillingï¼ŸSlot Fillingæ˜¯è‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œæ˜¯å¯¹è¯­è¨€å«ä¹‰çš„ç®€å•åŒ–å¤„ç†ï¼Œå®ƒçš„æ€æƒ³ç±»ä¼¼äºè¯­è¨€å­¦ä¸­æ¡†æ¶ä¸»ä¹‰çš„ä¸€æ´¾ï¼Œå…ˆè®¾å®šå¥½ç‰¹å®šçš„è¯­è¨€ç±»å‹æ§½ï¼Œå†å°†è¾“å…¥çš„å•è¯ä¸€ä¸€å¡«å…¥æ§½å†…ï¼Œè€Œè·å–è¨€è¯­å«ä¹‰çš„æ—¶å€™å³æ˜¯æ ¹æ®è¯­ä¹‰æ§½çš„å«ä¹‰è¿›è¡Œæå–å’Œæ£€ç´¢ã€‚æˆ‘ä»¬è¿™é‡Œçš„ä»»åŠ¡å°±æ˜¯å°†è¡¨ç¤ºå®šè´­èˆªç­ï¼ˆATISæ•°æ®é›†ï¼‰è¿™ä¸€è¨€è¯­è¡Œä¸ºçš„ä¸€ç³»åˆ—è¯­å¥å¡«å…¥å„ç§ç±»å‹çš„è¯­ä¹‰æ§½ä¸­ã€‚ ä¸ºä»€ä¹ˆä½¿ç”¨SimpleRNN?Slot Fillingå±äºRNNåº”ç”¨ä¸­ä¸€å¯¹ä¸€çš„åº”ç”¨ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ï¼Œæ¯ä¸ªè¯éƒ½èƒ½è¢«å¡«åˆ°åˆé€‚çš„æ§½ä¸­ã€‚RNNå’Œä¸€èˆ¬çš„ç¥ç»ç½‘ç»œçš„ä¸åŒåœ¨äºï¼Œåœ¨RNNä¸­ï¼Œæˆ‘ä»¬åœ¨æ—¶é—´tçš„è¾“å‡ºä¸ä»…å–å†³äºå½“å‰çš„è¾“å…¥å’Œæƒé‡ï¼Œè¿˜å–å†³äºä¹‹å‰çš„è¾“å…¥ï¼Œè€Œå¯¹äºå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ¯ä¸ªæ—¶åˆ»çš„è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯ç‹¬ç«‹è€Œéšæœºçš„ï¼Œæ²¡æœ‰ç›¸å…³æ€§ã€‚æ”¾åˆ°æˆ‘ä»¬è¦å¤„ç†è¯­ä¹‰ç†è§£çš„é—®é¢˜ä¸Šçœ‹ï¼Œè¯­è¨€ä½œä¸ºä¸€ç§åŸºäºæ—¶é—´çš„çº¿æ€§è¾“å‡ºï¼Œæ˜¾ç„¶ä¼šå—åˆ°å‰è¯çš„å½±å“ï¼Œå› æ­¤æˆ‘ä»¬é€‰å–RNNæ¨¡å‹æ¥è¿›è¡Œè§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™é‡Œé€‰å–SimpleRNN,æ˜¯å› ä¸ºè¿™ä¸ªRNNæ¯”è¾ƒç®€å•ï¼Œèƒ½è¾¾åˆ°ç†Ÿæ‚‰æ¡†æ¶çš„ç»ƒä¹ æ•ˆæœï¼Œä¹‹åå¯ä»¥é€‰å–å…¶ä»–æœ‰æ•ˆçš„RNNæ¨¡å‹ï¼Œå¦‚LSTMSè¿›è¡Œä¼˜åŒ–ã€‚ æ„å»ºæ€è·¯ä¸€è§ˆï¼š è½½å…¥æ•°æ®ï¼Œä½¿ç”¨çš„æ˜¯chsasankä¿®æ”¹çš„mesnilgrçš„load.pyã€‚ å®šä¹‰æ¨¡å‹ã€‚é‡‡å–Kerasä¸­çš„åºåˆ—æ¨¡å‹æ­å»ºï¼Œé¦–å…ˆä½¿ç”¨ä¸€ä¸ª100ç»´çš„word embeddingå±‚å°†è¾“å…¥çš„å•è¯è½¬åŒ–ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªå‘é‡ï¼ˆåœ¨è¿™ä¸ªç©ºé—´ä¸­ï¼Œè¯­ä¹‰å’Œè¯­æ³•ä½ç½®è¶Šè¿‘çš„å•è¯çš„è·ç¦»è¶Šå°ï¼‰ï¼Œç„¶åæˆ‘ä»¬æ„å»ºä¸€ä¸ªdropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè®¾ç½®SimpleRNNå±‚ï¼Œè®¾ç½®TimeDistributedå±‚ä»¥å®ŒæˆåŸºäºæ—¶é—´çš„åå‘ä¼ æ’­ã€‚æœ€åæˆ‘ä»¬å°†è¿™äº›å±‚ç»„ç»‡åœ¨ä¸€èµ·ï¼Œå¹¶ç¡®å®šoptimizerå’Œloss functionã€‚æˆ‘ä»¬é€‰å–çš„optimizeræ˜¯rmsprop,è¿™æ ·åœ¨è®­ç»ƒåæœŸä¾ç„¶èƒ½æ‰¾åˆ°è¾ƒæœ‰é¡¹ï¼Œè€Œé€‰å–categorical_crossentropyä½œä¸ºæŸå¤±å‡½æ•°ï¼Œåˆ™æ˜¯å› ä¸ºå¤„ç†çš„é—®é¢˜æ€§è´¨é€‚åˆäºæ­¤ã€‚ è®­ç»ƒæ¨¡å‹ã€‚å‡ºäºå¯¹è®¡ç®—èµ„æºçš„è€ƒè™‘ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨minibtachçš„æ–¹æ³•æ‰¹é‡å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚ä½†æ˜¯æˆ‘ä»¬è¿™é‡Œçš„æ•°æ®æ˜¯ä¸€å¥å¥è¯ï¼Œå¦‚æœæŒ‰ç…§ä¸€ä¸ªå›ºå®šçš„batch_sizeå°†å…¶åˆ†è£‚ï¼Œå¯èƒ½å¢åŠ äº†ä¸å¿…è¦çš„è”ç³»ï¼ˆå› ä¸ºä¸Šä¸‹ä¸¤å¥è¯æ˜¯ç‹¬ç«‹çš„ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸€å¥è¯ä½œä¸ºä¸€ä¸ªbatchå»è¿›è¡Œè®­ç»ƒã€éªŒè¯ä»¥åŠé¢„æµ‹ï¼Œå¹¶æ‰‹åŠ¨ç®—å‡ºä¸€ä¸ªepochçš„å¹³å‡è¯¯å·®ã€‚ è¯„ä¼°å’Œé¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡è§‚å¯ŸéªŒè¯è¯¯å·®å’Œé¢„æµ‹F1ç²¾åº¦æ¥å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚é¢„æµ‹F1ç²¾åº¦ä½¿ç”¨çš„æ˜¯signsmileç¼–å†™çš„conlleval.pyã€‚ ä¿å­˜æ¨¡å‹ã€‚ 123456789101112import numpy as npimport picklefrom keras.models import Sequentialfrom keras.layers.embeddings import Embeddingfrom keras.layers.recurrent import SimpleRNNfrom keras.layers.core import Dense,Dropoutfrom keras.utils import to_categoricalfrom keras.layers.wrappers import TimeDistributedfrom matplotlib import pyplot as pltimport data.loadfrom metrics.accuracy import evaluate Using TensorFlow backend. Load Data123456train_set,valid_set,dicts = data.load.atisfull()# print(train_set[:1])# dicts = &#123;'label2idx':&#123;&#125;,'words2idx':&#123;&#125;,'table2idx':&#123;&#125;&#125;w2idx,labels2idx = dicts['words2idx'],dicts['labels2idx']train_x,_,train_label = train_setval_x,_,val_label = valid_set 12idx2w = &#123;w2idx[i]:i for i in w2idx&#125;idx2lab = &#123;labels2idx[i]:i for i in labels2idx&#125; 12n_classes = len(idx2lab)n_vocab = len(idx2w) 123456789101112131415words_train = [[idx2w[i] for i in w[:]] for w in train_x]labels_train = [[idx2lab[i] for i in w[:]] for w in train_label]words_val = [[idx2w[i] for i in w[:]] for w in val_x]# labels_val = [[idx2lab[i] for i in w[:]] for w in val_label]labels_val =[]for w in val_label: for i in w[:]: labels_val.append(idx2lab[i])print('Real Sentence : &#123;&#125;'.format(words_train[0]))print('Encoded Form : &#123;&#125;'.format(train_x[0]))print('='*40)print('Real Label : &#123;&#125;'.format(labels_train[0]))print('Encoded Form : &#123;&#125;'.format(train_label[0])) Real Sentence : [&#39;i&#39;, &#39;want&#39;, &#39;to&#39;, &#39;fly&#39;, &#39;from&#39;, &#39;boston&#39;, &#39;at&#39;, &#39;DIGITDIGITDIGIT&#39;, &#39;am&#39;, &#39;and&#39;, &#39;arrive&#39;, &#39;in&#39;, &#39;denver&#39;, &#39;at&#39;, &#39;DIGITDIGITDIGITDIGIT&#39;, &#39;in&#39;, &#39;the&#39;, &#39;morning&#39;] Encoded Form : [232 542 502 196 208 77 62 10 35 40 58 234 137 62 11 234 481 321] ======================================== Real Label : [&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-fromloc.city_name&#39;, &#39;O&#39;, &#39;B-depart_time.time&#39;, &#39;I-depart_time.time&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-toloc.city_name&#39;, &#39;O&#39;, &#39;B-arrive_time.time&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-arrive_time.period_of_day&#39;] Encoded Form : [126 126 126 126 126 48 126 35 99 126 126 126 78 126 14 126 126 12] Define and Compile the model1234567model = Sequential()model.add(Embedding(n_vocab,100))model.add(Dropout(0.25))model.add(SimpleRNN(100,return_sequences=True))model.add(TimeDistributed(Dense(n_classes,activation='softmax')))model.compile(optimizer = 'rmsprop',loss = 'categorical_crossentropy')model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, None, 100) 57200 _________________________________________________________________ dropout_1 (Dropout) (None, None, 100) 0 _________________________________________________________________ simple_rnn_1 (SimpleRNN) (None, None, 100) 20100 _________________________________________________________________ time_distributed_1 (TimeDist (None, None, 127) 12827 ================================================================= Total params: 90,127 Trainable params: 90,127 Non-trainable params: 0 _________________________________________________________________ Train the model1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def train_the_model(n_epochs,train_x,train_label,val_x,val_label): epoch,train_avgloss,val_avgloss,f1s = [],[],[],[] for i in range(1,n_epochs+1): epoch.append(i) ## training train_avg_loss =0 for n_batch,sent in enumerate(train_x): label = train_label[n_batch] # label to one-hot label = to_categorical(label,num_classes=n_classes)[np.newaxis,:] sent = sent[np.newaxis,:] loss = model.train_on_batch(sent,label) train_avg_loss += loss train_avg_loss = train_avg_loss/n_batch train_avgloss.append(train_avg_loss) ## evaluate&amp;predict val_pred_label,pred_label_val,val_avg_loss = [],[],0 for n_batch,sent in enumerate(val_x): label = val_label[n_batch] label = to_categorical(label,num_classes=n_classes)[np.newaxis,:] sent = sent[np.newaxis,:] loss = model.test_on_batch(sent,label) val_avg_loss += loss pred = model.predict_on_batch(sent) pred = np.argmax(pred,-1)[0] val_pred_label.append(pred) val_avg_loss = val_avg_loss/n_batch val_avgloss.append(val_avg_loss) for w in val_pred_label: for k in w[:]: pred_label_val.append(idx2lab[k]) prec, rec, f1 = evaluate(labels_val,pred_label_val, verbose=False) print('Training epoch &#123;&#125;\\t train_avg_loss = &#123;&#125; \\t val_avg_loss = &#123;&#125;'.format(i,train_avg_loss,val_avg_loss)) print('precision: &#123;:.2f&#125;% \\t recall: &#123;:.2f&#125;% \\t f1 :&#123;:.2f&#125;%'.format(prec,rec,f1)) print('-'*60) f1s.append(f1) # return epoch,pred_label_train,train_avgloss,pred_label_val,val_avgloss return epoch,f1s,val_avgloss,train_avgloss 1epoch,f1s,val_avgloss,train_avgloss = train_the_model(40,train_x,train_label,val_x,val_label) è¾“å‡ºï¼š1234567891011121314 Training epoch 1 train_avg_loss = 0.5546463992293973 val_avg_loss = 0.4345020865901363 precision: 84.79% recall: 80.79% f1 :82.74% ------------------------------------------------------------ Training epoch 2 train_avg_loss = 0.2575569036037627 val_avg_loss = 0.36228470020366654 precision: 86.64% recall: 83.86% f1 :85.22% ------------------------------------------------------------ Training epoch 3 train_avg_loss = 0.2238766908014994 val_avg_loss = 0.33974187403771694 precision: 88.03% recall: 85.55% f1 :86.77% ------------------------------------------------------------â€¦â€¦ ------------------------------------------------------------ Training epoch 40 train_avg_loss = 0.09190682124901069 val_avg_loss = 0.2697056618613356 precision: 92.51% recall: 91.47% f1 :91.99% ------------------------------------------------------------ å¯è§†åŒ–è§‚å¯ŸéªŒè¯è¯¯å·®ï¼Œé€‰å–åˆé€‚çš„epochã€‚ 123456%matplotlib inlineplt.xlabel=('epoch')plt.ylabel=('loss')plt.plot(epoch,train_avgloss,'b')plt.plot(epoch,val_avgloss,'r',label=('validation error'))plt.show() 1print('æœ€å¤§f1å€¼ä¸º &#123;:.2f&#125;%'.format(max(f1s))) æœ€å¤§f1å€¼ä¸º 92.56% ä¿å­˜æ¨¡å‹1model.save('slot_filling_with_simpleRNN.h5') ç»“æœåˆ†æä½¿ç”¨SimpleRNNæœ€ç»ˆå¾—åˆ°çš„F1å€¼ä¸º92.56%ï¼Œå’Œå¸ˆå…„çš„95.47%ç›¸æ¯”ç¡®å®è¿˜ç›¸å·®å¾ˆå¤šã€‚è¿™ä¸»è¦æ˜¯å’Œæˆ‘ä»¬æ¨¡å‹çš„é€‰å–æœ‰å…³ï¼ŒSimpleRNNåªèƒ½å°†å‰è¯çš„å½±å“å¸¦å…¥åˆ°æ¨¡å‹ä¸­ï¼Œä½†æ˜¯è¯­è¨€ä¸­åè¯å¯¹å‰è¯ä¹Ÿä¼šæœ‰ä¸€å®šçš„å½±å“ï¼Œå› æ­¤å¯ä»¥é€šè¿‡é€‰æ‹©æ›´åŠ å¤æ‚çš„æ¨¡å‹æˆ–è€…å¢åŠ èƒ½å¤Ÿæ•æ‰åˆ°åè¯ä¿¡æ¯çš„å±‚æ¥è¿›è¡Œä¼˜åŒ–ã€‚ å‚è€ƒèµ„æ–™ Keras Tutorial - Spoken Language Understanding pytorch-slot-filling liu946 AtisSlotLabeling ã€Kerasæƒ…æ„Ÿåˆ†ç±»ã€‘è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„é—®é¢˜æ±‡æ€» keras-SimpleRNN æœºå™¨å­¦ä¹ ä¸­è¿‡æ‹Ÿåˆçš„è§£å†³åŠæ³•","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://shamy1997.github.io/tags/NLP/"},{"name":"keras","slug":"keras","permalink":"http://shamy1997.github.io/tags/keras/"},{"name":"RNN","slug":"RNN","permalink":"http://shamy1997.github.io/tags/RNN/"}]},{"title":"TensorFlowå°è¯•ç‰›åˆ€","slug":"tfå°è¯•ç‰›åˆ€","date":"2018-09-02T16:02:32.000Z","updated":"2019-01-20T12:02:50.287Z","comments":true,"path":"passages/tfå°è¯•ç‰›åˆ€/","link":"","permalink":"http://shamy1997.github.io/passages/tfå°è¯•ç‰›åˆ€/","excerpt":"","text":"æ­¤æ—¥å¿—ä¸ºå‚ç…§Udacityè¯¾ç¨‹ä¸­ã€ŠIntro to tensorflowã€‹çš„jupyter notebookæ‰€åšçš„åˆ†è§£æºç ï¼Œç›®çš„åœ¨äºç†è§£ä»£ç é€»è¾‘ï¼Œç†Ÿæ‚‰åˆ›å»ºæµç¨‹å’Œå¥—è·¯ã€‚å…¶ä¸­å‚è€ƒäº†ä¸å°‘åšæ–‡é“¾æ¥ï¼Œéå¸¸æ„Ÿè°¢ï¼Œå…¨éƒ¨æ”¾åœ¨æ–‡æœ«ï¼Œåœ¨åŸæ–‡ä¸­ä¸å†æŒ‡å‡ºã€‚ æ•°æ®é“¾æ¥ï¼šç™¾åº¦äº‘ï¼šNoMNIST å¯†ç ï¼šfsks P1:é¢„å¤„ç†æ•°æ®123456789101112import hashlibimport osimport picklefrom urllib.request import urlretrieveimport numpy as npfrom PIL import Imagefrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.utils import resamplefrom tqdm import tqdmfrom zipfile import ZipFile è§£å‹å›¾ç‰‡æ–‡ä»¶1234567891011121314151617181920212223242526272829303132333435363738394041def uncompress_features_labels(file): \"\"\" Uncompress features and labels from a zip file :param file: The zip file to extract the data from \"\"\" features = [] labels = [] with ZipFile(file) as zipf: # Progress Bar filenames_pbar = tqdm(zipf.namelist(), unit='files') # Get features and labels from all files for filename in filenames_pbar: # Check if the file is a directory if not filename.endswith('/'): with zipf.open(filename) as image_file: image = Image.open(image_file) image.load() # Load image data as 1 dimensional array # We're using float32 to save on memory space feature = np.array(image, dtype=np.float32).flatten() # Get the the letter from the filename. This is the letter of the image. label = os.path.split(filename)[1][0] features.append(feature) labels.append(label) return np.array(features), np.array(labels)# Get the features and labels from the zip filestrain_features, train_labels = uncompress_features_labels('notMNIST_train.zip')test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')# Limit the amount of data to work with a docker containerdocker_size_limit = 150000train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)# Set flags for feature engineering. This will prevent you from skipping an important step.is_features_normal = Falseis_labels_encod = False 12100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210001/210001 [00:54&lt;00:00, 3832.78files/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10001/10001 [00:03&lt;00:00, 3207.15files/s] Min-Max ScalingImplement Min-Max scaling in the normalize_grayscale() function to a range of a=0.1 and b=0.9. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9. Since the raw notMNIST image data is in grayscale, the current values range from a min of 0 to a max of 255. Min-Max Scaling:$Xâ€™=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}$ 123456789101112def normalize_grayscale(image_data): \"\"\" Normalize the image data with Min-Max scaling to a range of [0.1, 0.9] :param image_data: The image data to be normalized :return: Normalized image data \"\"\" a = 0.1 b = 0.9 max_grayscale = 255 min_grayscale = 0 return a+((image_data-min_grayscale))*(b-a)/(max_grayscale-min_grayscale) 12train_features = normalize_grayscale(train_features)test_features = normalize_grayscale(test_features) æ ‡ç­¾äºŒå€¼åŒ–LabelBinarizer()æ˜¯sklearn.preprocessionä¸­ç”¨æ¥å°†éæ•°å€¼ç±»æ ‡ç­¾è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç å‘é‡çš„å‡½æ•°ã€‚ 123456789# Create the encoder åˆ›å»ºç¼–ç å™¨encoder = LabelBinarizer()# ç¼–ç å™¨æ‰¾åˆ°ç±»åˆ«å¹¶åˆ†é… one-hot å‘é‡encoder.fit(train_labels)#æœ€åæŠŠç›®æ ‡ï¼ˆlablesï¼‰è½¬æ¢æˆç‹¬çƒ­ç¼–ç çš„ï¼ˆone-hot encodedï¼‰å‘é‡train_labels = encoder.transform(train_labels)test_labels = encoder.transform(test_labels) è½¬æ¢æ•°æ®ç±»å‹ï¼Œè¿™æ ·åé¢å…¬å¼ä¸­æ‰å¯ä»¥è¿›è¡Œè¿ç®—ã€‚ 12train_labels = train_labels.astype(np.float32)test_labels = test_labels.astype(np.float32) éšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¸¸è§å½¢å¼ä¸ºï¼šX_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0) å‚æ•°è§£é‡Šï¼š train_dataï¼šæ‰€è¦åˆ’åˆ†çš„æ ·æœ¬ç‰¹å¾é›† train_targetï¼šæ‰€è¦åˆ’åˆ†çš„æ ·æœ¬ç»“æœ test_sizeï¼šæ ·æœ¬å æ¯”ï¼Œå¦‚æœæ˜¯æ•´æ•°çš„è¯å°±æ˜¯æ ·æœ¬çš„æ•°é‡ random_stateï¼šæ˜¯éšæœºæ•°çš„ç§å­ã€‚ éšæœºæ•°ç§å­ï¼šå…¶å®å°±æ˜¯è¯¥ç»„éšæœºæ•°çš„ç¼–å·ï¼Œåœ¨éœ€è¦é‡å¤è¯•éªŒçš„æ—¶å€™ï¼Œä¿è¯å¾—åˆ°ä¸€ç»„ä¸€æ ·çš„éšæœºæ•°ã€‚æ¯”å¦‚ä½ æ¯æ¬¡éƒ½å¡«1ï¼Œå…¶ä»–å‚æ•°ä¸€æ ·çš„æƒ…å†µä¸‹ä½ å¾—åˆ°çš„éšæœºæ•°ç»„æ˜¯ä¸€æ ·çš„ã€‚ä½†å¡«0æˆ–ä¸å¡«ï¼Œæ¯æ¬¡éƒ½ä¼šä¸ä¸€æ ·ã€‚ 123456# Get randomized datasets for training and validationtrain_features, valid_features, train_labels, valid_labels = train_test_split( train_features, train_labels, test_size=0.05, random_state=832289) æ‰“åŒ…æ•°æ®æ–¹ä¾¿ä¸‹æ¬¡å–ç”¨åºåˆ—åŒ–çš„æ–¹æ³•ä¸º pickle.dump()ï¼Œè¯¥æ–¹æ³•çš„ç›¸å…³å‚æ•°å¦‚ä¸‹ï¼špickle.dump(obj, file, protocol=None,*,fix_imports=True) 1234567891011121314151617181920# æ–°å»ºpickle_file# å‚æ•°fileå¿…é¡»æ˜¯ä»¥äºŒè¿›åˆ¶çš„å½¢å¼è¿›è¡Œæ“ä½œ,å³ã€Œwbã€pickle_file = 'notMNIST.pickle'if not os.path.isfile(pickle_file): print('Saving data to pickle file...') try: with open('notMNIST.pickle', 'wb') as pfile: pickle.dump( &#123; 'train_dataset': train_features, 'train_labels': train_labels, 'valid_dataset': valid_features, 'valid_labels': valid_labels, 'test_dataset': test_features, 'test_labels': test_labels, &#125;, pfile, pickle.HIGHEST_PROTOCOL) except Exception as e: print('Unable to save data to', pickle_file, ':', e) raise P2:ä»é¢„å¤„ç†å¥½çš„pickleä¸­è¯»å–æ•°æ®12345678910111213141516171819202122%matplotlib inline# Load the modulesimport pickleimport mathimport numpy as npimport tensorflow as tffrom tqdm import tqdmimport matplotlib.pyplot as plt# Reload the datapickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: pickle_data = pickle.load(f) train_features = pickle_data['train_dataset'] train_labels = pickle_data['train_labels'] valid_features = pickle_data['valid_dataset'] valid_labels = pickle_data['valid_labels'] test_features = pickle_data['test_dataset'] test_labels = pickle_data['test_labels'] del pickle_data # Free up memory C:\\Users\\10677\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters ä½¿ç”¨TFåˆ›å»ºå•å±‚ç¥ç»ç½‘ç»œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨TensorFlowåˆ›å»ºä¸€ä¸ªåªæœ‰ä¸€ä¸ªè¾“å…¥å±‚å’Œè¾“å‡ºå±‚çš„ç¥ç»ç½‘ç»œï¼Œæ¿€æ´»å‡½æ•°ä¸ºsoftmaxã€‚åœ¨TensorFlowä¸­ï¼Œæ•°æ®ä¸æ˜¯ä»¥æ•´æ•°ã€æµ®ç‚¹æ•°æˆ–å­—ç¬¦ä¸²çš„å½¢å¼å­˜å‚¨çš„ï¼Œè€Œæ˜¯ä»¥tensorå¯¹è±¡çš„å½¢å¼è¢«å­˜å‚¨çš„ã€‚ åœ¨tensorä¸­ä¼ é€’å€¼æœ‰ä¸¤ç§æ–¹æ³•ï¼š ä½¿ç”¨tf.constant()ï¼Œä¼ å…¥å˜é‡ï¼Œä½†æ˜¯ä¼ å…¥ä¹‹åå°±ä¸å¯å˜äº† å¦‚æœè¦ä½¿æ•°æ®å¯å˜ï¼Œç»“åˆtf.placeholder()å’Œtf.feed_dictæ¥è¾“å…¥ 1234567891011121314151617181920212223242526272829303132333435363738394041# All the pixels in the image (28 * 28 = 784)features_count = 784# All the labels (\"A,B...J\")labels_count = 10features = tf.placeholder(tf.float32)labels = tf.placeholder(tf.float32)# Set the weights and biases tensors# tf.truncated_normal:ç”Ÿæˆæ­£æ€åˆ†å¸ƒçš„éšæœºå€¼# weightså·²ç»éšæœºåŒ–ï¼Œbiaseså°±ä¸å¿…éšæœºï¼Œç®€åŒ–ä¸º0å³å¯weights = tf.Variable(tf.truncated_normal((features_count,labels_count)))biases = tf.Variable(tf.zeros(labels_count))# Feed dicts for training, validation, and test sessiontrain_feed_dict = &#123;features: train_features, labels: train_labels&#125;valid_feed_dict = &#123;features: valid_features, labels: valid_labels&#125;test_feed_dict = &#123;features: test_features, labels: test_labels&#125;# Linear Function WX + blogits = tf.matmul(features, weights) + biasesprediction = tf.nn.softmax(logits)# Cross entropycross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)# Training lossloss = tf.reduce_mean(cross_entropy)# Create an operation that initializes all variablesinit = tf.global_variables_initializer()# Test Caseswith tf.Session() as session: session.run(init) session.run(loss, feed_dict=train_feed_dict) session.run(loss, feed_dict=valid_feed_dict) session.run(loss, feed_dict=test_feed_dict) biases_data = session.run(biases) 12is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32)) P3:è®­ç»ƒç¥ç»ç½‘ç»œ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Change if you have memory restrictionsbatch_size = 128# Find the best parameters for each configurationepochs = 4learning_rate = 0.2# Gradient Descent# ä½¿ç”¨æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # The accuracy measured against the validation setvalidation_accuracy = 0.0# Measurements use for graphing loss and accuracylog_batch_step = 50batches = []loss_batch = []train_acc_batch = []valid_acc_batch = []with tf.Session() as session: session.run(init) batch_count = int(math.ceil(len(train_features)/batch_size)) for epoch_i in range(epochs): # Progress bar batches_pbar = tqdm(range(batch_count), desc='Epoch &#123;:&gt;2&#125;/&#123;&#125;'.format(epoch_i+1, epochs), unit='batches') # The training cycle for batch_i in batches_pbar: # Get a batch of training features and labels batch_start = batch_i*batch_size batch_features = train_features[batch_start:batch_start + batch_size] batch_labels = train_labels[batch_start:batch_start + batch_size] # Run optimizer and get loss _, l = session.run( [optimizer, loss], feed_dict=&#123;features: batch_features, labels: batch_labels&#125;) # Log every 50 batches if not batch_i % log_batch_step: # Calculate Training and Validation accuracy training_accuracy = session.run(accuracy, feed_dict=train_feed_dict) validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict) # Log batches previous_batch = batches[-1] if batches else 0 batches.append(log_batch_step + previous_batch) loss_batch.append(l) train_acc_batch.append(training_accuracy) valid_acc_batch.append(validation_accuracy) # Check accuracy against Validation data validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)loss_plot = plt.subplot(211)loss_plot.set_title('Loss')loss_plot.plot(batches, loss_batch, 'g')loss_plot.set_xlim([batches[0], batches[-1]])acc_plot = plt.subplot(212)acc_plot.set_title('Accuracy')acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')acc_plot.set_ylim([0, 1.0])acc_plot.set_xlim([batches[0], batches[-1]])acc_plot.legend(loc=4)plt.tight_layout()plt.show()print('Validation accuracy at &#123;&#125;'.format(validation_accuracy)) Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:11&lt;00:00, 101.27batches/s] Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:10&lt;00:00, 101.99batches/s] Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:10&lt;00:00, 101.38batches/s] Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:12&lt;00:00, 92.55batches/s] Validation accuracy at 0.7662666440010071 P4:æ£€æµ‹12345678910111213141516171819202122232425262728test_accuracy = 0.0with tf.Session() as session: session.run(init) batch_count = int(math.ceil(len(train_features)/batch_size)) for epoch_i in range(epochs): # Progress bar batches_pbar = tqdm(range(batch_count), desc='Epoch &#123;:&gt;2&#125;/&#123;&#125;'.format(epoch_i+1, epochs), unit='batches') # The training cycle for batch_i in batches_pbar: # Get a batch of training features and labels batch_start = batch_i*batch_size batch_features = train_features[batch_start:batch_start + batch_size] batch_labels = train_labels[batch_start:batch_start + batch_size] # Run optimizer _ = session.run(optimizer, feed_dict=&#123;features: batch_features, labels: batch_labels&#125;) # Check accuracy against Test data test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)assert test_accuracy &gt;= 0.80, 'Test accuracy at &#123;&#125;, should be equal to or greater than 0.80'.format(test_accuracy)print('Nice Job! Test Accuracy is &#123;&#125;'.format(test_accuracy)) Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 588.57batches/s] Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 634.64batches/s] Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 633.74batches/s] Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 638.60batches/s] Nice Job! Test Accuracy is 0.8468999862670898 å‚è€ƒé“¾æ¥ï¼š python tqdmæ¨¡å—åˆ†æ Sklearn-train_test_splitéšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† numpy_ndarray.flatten sklearn.LabelBinarizer","categories":[],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://shamy1997.github.io/tags/tensorflow/"},{"name":"æ·±åº¦å­¦ä¹ ","slug":"æ·±åº¦å­¦ä¹ ","permalink":"http://shamy1997.github.io/tags/æ·±åº¦å­¦ä¹ /"}]},{"title":"åŒ—å¤§åˆ†è¯æ–¹æ¡ˆè§£è¯»åŠé¢—ç²’åº¦åˆ†è¯æ–¹æ¡ˆ","slug":"é¢—ç²’åº¦åˆ†è¯è°ƒç ”","date":"2018-08-30T06:23:49.000Z","updated":"2019-01-20T11:41:16.746Z","comments":true,"path":"passages/é¢—ç²’åº¦åˆ†è¯è°ƒç ”/","link":"","permalink":"http://shamy1997.github.io/passages/é¢—ç²’åº¦åˆ†è¯è°ƒç ”/","excerpt":"","text":"ä¸€ã€è°ƒç ”èµ„æ–™ åŒ—å¤§ç°ä»£æ±‰è¯­è¯­æ–™åº“åŸºæœ¬åŠ å·¥è§„èŒƒ è®¡ç®—æ‰€æ±‰è¯­è¯æ€§æ ‡æ³¨é›† å‡ ä¸ªå¼€æºåˆ†è¯ç³»ç»Ÿæ‰€ä½¿ç”¨æ ‡æ³¨é›†çš„æ¥æº æµ·é‡ä¸­æ–‡æ™ºèƒ½åˆ†è¯æ¥å£æ‰‹å†Œ é˜¿é‡Œå¤šç²’åº¦åˆ†è¯ä¸“åˆ© è…¾è®¯å¤šç²’åº¦åˆ†è¯ä¸“åˆ© ç™¾åº¦å¤šç²’åº¦åˆ†è¯ä¸“åˆ© KTDictSeg åˆ†è¯ç»„ä»¶1.3ç‰ˆæœ¬ éƒ¨åˆ†ç®—æ³•è®¨è®º â€” åˆ†è¯ç²’åº¦ äºŒã€è°ƒç ”ç›®çš„åˆ†è¯å•ä½ä¸åŒäºè¯­è¨€å­¦ä¸­çš„â€œè¯â€ï¼Œä¸åŒçš„ç®—æ³•ä¸‹çš„åˆ†è¯ç»“æœåƒå·®ä¸‡åˆ«ï¼Œæœ‰çš„åˆ†å‡ºçš„æ˜¯è¯­è¨€å­¦æ„ä¹‰ä¸Šçš„è¯ï¼Œè€Œæœ‰çš„åˆ†å‡ºçš„æ˜¯è¯­è¨€å­¦æ„ä¹‰ä¸Šçš„â€œçŸ­è¯­â€ï¼ˆæˆ–è€…è¯´â€œè¯ç»„â€ï¼‰å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å¯»æ‰¾ä¸€ä¸ªå¯ç†è§£çš„ç»Ÿä¸€çš„ç²’åº¦æ ‡å‡†ï¼Œè€Œè¿™ä¸ªç²’åº¦æ ‡å‡†èƒ½å¤Ÿå®ç°å¯¹ä¸åŒåˆ†è¯ä»»åŠ¡çš„ä¸åŒå±‚æ¬¡çš„åˆ†è¯ã€‚ä¸ºè¯å®å¤šé¢—ç²’åº¦çš„åˆ†è¯æ ‡æ³¨ç¡®å®èƒ½æé«˜ç‰¹å®šçš„åˆ†è¯ä»»åŠ¡çš„å‡†ç¡®ç‡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¿™æ ·çš„å‰æœŸè°ƒç ”ã€‚é€šè¿‡æœé›†èµ„æ–™ï¼Œæˆ‘ä»¬ä»¥åŒ—å¤§æ–¹æ¡ˆä¸ºè“æœ¬ï¼Œä»¥ä¸€å®šçš„è¯­è¨€å­¦çŸ¥è¯†ä¸ºåŸºç¡€ï¼Œå¯¹åˆ†è¯é¢—ç²’è¿›è¡Œä¸åŒç²’åº¦çš„åˆ’åˆ†ã€‚é¦–å…ˆå¯¹åŒ—å¤§åˆ†è¯æ–¹æ¡ˆè¿›è¡Œè§£è¯»ï¼Œç„¶åå†é˜é‡Šæˆ‘å¯¹åˆ†è¯ç²’åº¦åˆæ­¥çš„æ„å»ºæƒ³æ³•ã€‚ æ³¨ï¼šé¢—ç²’åº¦æ–¹æ¡ˆåªè€ƒè™‘åˆ†è¯é—®é¢˜ï¼Œä¸è€ƒè™‘è¯æ€§æ ‡æ³¨ã€‚ ä¸‰ã€åŒ—å¤§åˆ†è¯æ–¹æ¡ˆè®²è§£1. åˆ†è¯å•ä½çš„æ¦‚å¿µç•Œå®šåˆ†è¯å•ä½ï¼Œâ€œæŒ‡ä¿¡æ¯å¤„ç†ä¸­ä½¿ç”¨çš„ã€å…·æœ‰ç¡®å®šçš„è¯­ä¹‰å’Œè¯­æ³•åŠŸèƒ½çš„åŸºæœ¬å•ä½â€ï¼Œè¯¥æ¦‚å¿µæ˜ç¡®äº†å…¶ä½¿ç”¨çš„ç‰¹å®šç¯å¢ƒâ€”â€”â€œä¿¡æ¯å¤„ç†ä»»åŠ¡â€ï¼Œä»¥åŠå…¶è¯­ä¹‰å’Œè¯­æ³•åŠŸèƒ½æ˜ç¡®çš„ç‰¹ç‚¹ã€‚ åŸºäºè¿™æ ·çš„æ¦‚å¿µåˆ’åˆ†ï¼ŒåŒ—å¤§æ–¹æ¡ˆè®¤å®šçš„åˆ†è¯å•ä½é‡Œä¸ä»…åŒ…æ‹¬äº†è¯ï¼Œè¿˜â€œåŒ…æ‹¬äº†ä¸€éƒ¨åˆ†ç»“åˆç´§å¯†ã€ä½¿ç”¨ç¨³å®šçš„è¯ç»„â€ï¼Œå¹¶ä¸”â€œåœ¨æŸäº›ç‰¹æ®Šæƒ…å†µå­¤ç«‹çš„è¯­ç´ æˆ–éè¯­ç´ å­—â€ã€‚ äº‹å®ä¸Šï¼Œæˆ‘ä»¬æ’‡å¼€åŒ—å¤§æ–¹æ¡ˆæ¥çœ‹è¯è¿™ä¸ªæ•´ä½“ï¼Œæ ¹æ®æœ±å¾·ç†™å…ˆç”Ÿçš„åˆ’åˆ†ï¼Œå¯ä»¥åˆ†ä¸ºå¯ç©·å°½çš„è™šè¯ç±»å’Œä¸å¯ç©·å°½çš„å®è¯ç±»ã€‚è™šè¯ç±»ï¼Œä¸¾ä¾‹æ¥è¯´ï¼ŒåŒ…æ‹¬è¿è¯ã€è¯­æ°”è¯ã€ä»‹è¯ç­‰ï¼Œè¿™ç±»è¯å¯ä»¥åœ¨è¯­æ³•è¯å…¸ä¸­è¢«æšä¸¾å‡ºæ¥ï¼Œå› æ­¤åœ¨è¿›è¡Œåˆ†è¯æ—¶éš¾åº¦è¾ƒå°ã€‚å› æ­¤ï¼Œåˆ†è¯çš„å›°éš¾å¸¸å¸¸å‡ºç°åœ¨å®è¯çš„åˆ‡åˆ†ä¸Šã€‚ ç»“åˆåŒ—å¤§æ–¹æ¡ˆçš„åˆ’åˆ†ï¼Œæˆ‘è®¤ä¸ºå¯¹å®è¯åºåˆ—è¿›è¡Œåˆ’åˆ†æ—¶ï¼Œä¸€èˆ¬å¯ä»¥éµç…§ä»¥ä¸‹åŸåˆ™ï¼š ï¼ˆ1ï¼‰ä¾æ®è¯­æ³•è¯å…¸æ¥åˆ’åˆ†ï¼Œå¦‚æœè¯­æ³•è¯å…¸ä¸­è¿›è¡Œè§„å®šï¼Œé‚£ä¹ˆå°±ä¸åšåˆ’åˆ†ã€‚è¯­è¨€æ˜¯çº¦å®šä¿—æˆçš„äº§ç‰©ï¼Œå½“æŸä¸ªè¯è¯­ç»„åˆè¢«å¹¿æ³›è€Œç¨³å®šåœ°ä½¿ç”¨æ—¶ï¼Œé‚£ä¹ˆç¤¾ä¼šå›¢ä½“ä¾¿ä¼šæ¥å—è¿™æ ·çš„ä¸€ä¸ªâ€œæ–°è¯â€ï¼Œå› æ­¤è¿™æ ·çš„ä¸€ä¸ªè¯è¯­ç»„åˆä¹Ÿå¯ä»¥è¢«è§†ä½œæ˜¯ä¸€ä¸ªåˆ†è¯å•ä½ã€‚è€Œåˆ¤æ–­ç¤¾ä¼šå›¢ä½“æ˜¯å¦å·²ç»æ¥å—è¿™ä¸€è¯­è¨€ç°è±¡å¾ˆæ˜¾æ€§çš„ä¸€å¤§æ ‡å¿—ä¾¿æ˜¯è¯å…¸æ”¶å½•äº†è¯¥è¯æ¡ã€‚é‚£ä¹ˆé—®é¢˜å°±è½¬å˜ä¸ºï¼Œä»€ä¹ˆæ ·çš„è¯å…¸å¯ä»¥æˆä¸ºå¯ä¾›åˆ’åˆ†çš„è¯­æ³•è¯å…¸ã€‚ ï¼ˆ2ï¼‰è€ƒè™‘åˆ‡åˆ†åºåˆ—çš„éŸ³èŠ‚ç»„åˆã€‚æ±‰è¯­åœ¨å‘å±•è¿‡ç¨‹ä¸­ç»å†äº†ä¸€ä¸ªä»å•éŸ³èŠ‚å‘åŒéŸ³èŠ‚çš„å‘å±•è¿‡ç¨‹ã€‚è™½ç„¶ç°ä»£æ±‰è¯­ä»¥åŒéŸ³èŠ‚ä¸ºä¸»è¦çš„æˆè¯å•ä½ï¼Œä½†æ˜¯å¤ä»£æ±‰è¯­ä¸­çš„ä¸€äº›å•éŸ³èŠ‚è¯ä¾ç„¶æ®‹å­˜åœ¨ç°ä»£æ±‰è¯­ä¸­ï¼Œå¹¶ä¸”åœ¨ä¸€äº›ç‰¹æ®Šè¯­ä½“ä¸­è¿˜å¹¿æ³›åœ°å­˜åœ¨ç€ã€‚å› æ­¤ï¼Œå¯¹äºé‚£äº›å•éŸ³èŠ‚æˆè¯çš„å•ä½åœ¨æ ‡æ³¨æ—¶è¦æ ¼å¤–æ³¨æ„æ ‡è®°å‡ºæ¥ï¼Œè€Œå¤„ç†å¤šéŸ³èŠ‚åºåˆ—æ—¶ï¼Œåˆ™è¦å°½é‡ä¿è¯åˆ†è¯ç»“æœä»¥åŒéŸ³èŠ‚ä¸ºä¸€ä¸ªå•ä½ã€‚ ï¼ˆ3ï¼‰è€ƒè™‘åˆ°è¯ä¹‰ä¸è¯­ç´ ç»“åˆä¹‰ã€‚æˆ‘ä»¬æ‰€è®¤å®šçš„åˆ†è¯å•ä½ï¼Œå®ƒçš„è¯ä¹‰æ˜¯å‡åˆè€Œæˆçš„ï¼Œè€Œä¸æ˜¯ä¸¤ä¸ªè¯­ç´ çš„æ„ä¹‰ç®€å•çš„ç›¸åŠ ã€‚å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªåˆ‡åˆ†å•ä½çš„è¯­ä¹‰æ˜¯å…¶åˆ‡åˆ†å•ä½æ„ä¹‰çš„ç®€å•ç›¸åŠ ï¼Œé‚£ä¹ˆå°±è¦å¯¹å…¶è¿›è¡Œåˆ‡åˆ†ã€‚è€Œåˆ¤å®šæ˜¯å¦æ˜¯è¯ä¹‰ç®€å•çš„ç›¸åŠ çš„æ–¹æ³•ä¸»è¦æœ‰â€œçš„â€æ’å…¥æ³•å’Œæ›¿æ¢æ³•ä¸¤ç§ï¼Œè¿™åœ¨åé¢å…·ä½“çš„è®²è§£ä¸­ä¼šè¿›è¡Œé˜é‡Šã€‚ ï¼ˆ4ï¼‰è¦è€ƒè™‘åˆ°åˆ‡åˆ†çš„ç»æµæ€§ã€‚åŒ—å¤§æ–¹æ¡ˆæ˜¯åˆ‡åˆ†å’Œæ ‡æ³¨åŒæ—¶è¿›è¡Œï¼Œä¸ºäº†ä¿è¯æ ‡æ³¨ç¬¦å·ä½¿ç”¨çš„ç»æµæ€§ï¼Œæ–¹æ¡ˆè¦æ±‚ï¼Œè¦ä¿è¯åˆ‡åˆ†å‡ºæ¥çš„å•ä½å°½é‡å°‘çš„æ˜¯æ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ã€‚å› æ­¤ï¼Œå¯¹äºä¸€ä¸ªåˆ‡åˆ†åºåˆ—ï¼Œå¦‚æœæˆ‘ä»¬åˆ‡åˆ†åå¤šå‡ºäº†æ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ï¼Œæ¯”å¦‚è¯´å‰æ¥æˆåˆ†ã€åæ¥æˆåˆ†ç­‰ï¼Œæˆ‘ä»¬å°½å¯èƒ½åœ°ä¸å»åˆ‡åˆ†å®ƒã€‚ 2.åˆ†è¯å®é™…æƒ…å†µä¸­çš„åº”ç”¨æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¯¹åˆ†è¯æ–¹æ¡ˆçš„ç¬¬å››ç« ã€ç¬¬äº”ç« ç»“åˆæˆ‘ä»¬æ€»ç»“å‡ºæ¥çš„è§„åˆ™è¿›è¡Œç²¾ç®€å¼çš„è¯´æ˜ã€‚ ï¼ˆ1ï¼‰äººå å¯¹äºäººåçš„åˆ‡åˆ†ï¼Œæ–¹æ¡ˆç»™å‡ºçš„åˆ‡åˆ†æ ‡å‡†æ˜¯å§“å’Œååˆ‡åˆ†å¼€ã€‚è€Œå¯¹äºå…¶ä»–ç§°å‘¼æ˜¯å¦åˆ‡åˆ†ï¼Œå¯ä»¥ç”¨è¯­ä¹‰è§„åˆ™æ¥è§£é‡Šã€‚ç¬¬äºŒæ¡è§„åˆ™ï¼šå§“ååçš„èŒåŠ¡ã€èŒç§°æˆ–ç§°å‘¼è¦åˆ†å¼€ã€‚ç¬¬å››æ¡è§„åˆ™ï¼šå¸¦æ˜æ˜¾æ’è¡Œçš„äº²å±ç§°è°“è¦åˆ‡åˆ†å¼€ã€‚è¿™ä¸¤æ¡è§„åˆ™æ˜¯å› ä¸ºç»„æˆçš„åˆ‡åˆ†åºåˆ—çš„æ„æ€å³æ˜¯å„ç»„æˆæˆåˆ†çš„ç»„åˆä¹‰ï¼Œå› æ­¤è¦åˆ‡åˆ†ã€‚è€Œç¬¬ä¸‰æ¡è§„åˆ™ï¼šå¯¹äººçš„ç®€ç§°ã€å°Šç§°è‹¥ä¸ºä¸¤ä¸ªå­—ï¼Œåˆ™åˆä¸ºä¸€ä¸ªåˆ‡åˆ†å•ä½ã€‚ä¸ä»…æ˜¯å› ä¸ºè¿™äº›åˆ‡åˆ†åºåˆ—çš„å«ä¹‰ä¸æ˜¯å…¶ç»„æˆæˆåˆ†çš„ç»„åˆä¹‰ï¼Œè‡³å°‘æœ‰è¡¨ç¤ºå°Šæ•¬çš„ç¤¾ä¼šå«ä¹‰ï¼Œè¿˜æ˜¯å› ä¸ºå¦‚æœåˆ‡åˆ†ï¼Œä¼šå¤šå‡ºæ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ï¼Œå› æ­¤æŠŠè¿™äº›åŒéŸ³èŠ‚ä½œä¸ºä¸€ä¸ªåˆ‡åˆ†å•ä½ã€‚è€Œå¯¹äºå¤–å›½äººåå’Œç¬”åã€è‘—åäººåï¼Œæˆ‘ä»¬ä¸åšåˆ‡åˆ†ï¼Œä¸€æ˜¯å› ä¸ºè¿™ç§å‘½åæ˜¯éšæ„çš„ï¼Œåˆ‡åˆ†ä¸‹æ¥çš„æ„ä¹‰ä¸å¤§ï¼›äºŒæ˜¯å› ä¸ºè‘—åäººåæ˜¯åœ¨è¯­æ³•è¯å…¸ä¸­å°±è§„å®šäº†çš„å†…å®¹ã€‚ ï¼ˆ2ï¼‰åœ°å å¤§éƒ¨åˆ†åœ°åéƒ½æ˜¯åœ¨è¯­æ³•è¯å…¸ä¸­äº‹å…ˆè§„å®šäº†çš„ï¼Œé™¤æ­¤ä»¥å¤–çš„åˆ‡åˆ†åŸåˆ™ä¸»è¦æ˜¯å’ŒéŸ³èŠ‚æœ‰å…³ï¼Œå¦‚æœåœ°ååæ¥çš„æ˜¯å•éŸ³èŠ‚è¯­ç´ ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼›å¦‚æœæ¥çš„æ˜¯åŒéŸ³èŠ‚æˆ–å¤šéŸ³èŠ‚è¯­ç´ ï¼Œåˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ3ï¼‰å›¢ä½“ã€æœºæ„ã€ç»„ç»‡çš„ä¸“æœ‰åç§° å¯¹äºå›¢ä½“ã€æœºæ„ã€ç»„ç»‡çš„ä¸“æœ‰åç§°ï¼Œå¦‚æœå®ƒä»¬è¢«è¯­æ³•è¯å…¸æ”¶å½•ï¼Œé‚£ä¹ˆè‚¯å®šä¸åˆ‡åˆ†ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ï¼ˆå¦‚æœæ‰¾ä¸åˆ°è¿™æ ·åˆé€‚çš„è¯å…¸ï¼Œä¸€ä¸ªPLAN Bçš„å»ºè®®ï¼šæŒ‰ç…§æ™®é€šè¯ç»„åˆ‡åˆ†ï¼Œå†ä¸Šæ¸¸ä»»åŠ¡ä¸­å†è¯†åˆ«å‡ºæ¥ï¼‰ ï¼ˆ4ï¼‰é™¤äººåã€å›½åã€åœ°åã€å›¢ä½“ã€æœºæ„ã€ç»„ç»‡ä»¥å¤–çš„å…¶ä»–ä¸“å é¦–å…ˆï¼Œæˆ‘ä»¬è¿˜æ˜¯è¦è€ƒè™‘å…¶æ˜¯å¦è¢«è¯­æ³•è¯å…¸æ”¶å½•ã€‚ç„¶åè¦è€ƒè™‘å…¶åæ¥è¯­ç´ çš„éŸ³èŠ‚ï¼Œå¦‚æœæ˜¯å•éŸ³èŠ‚çš„ï¼Œå¦‚â€œäººâ€â€œæ—â€è¿™æ ·çš„ï¼Œä¸åˆ‡åˆ†ï¼Œå¦‚æœæ˜¯å¤šéŸ³èŠ‚çš„ï¼Œåˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ5ï¼‰æ•°è¯ä¸æ•°é‡è¯ç»„ æ•°è¯ä¸æ•°é‡è¯ç»„çš„è§„å®šæ˜¯å¦å¤–çš„ã€‚è¯¦è§æ–¹æ¡ˆã€‚ ï¼ˆ6ï¼‰æ—¶é—´è¯ æ—¶é—´è¯ä¸­ç™»å½•åœ¨è¯­æ³•è¯å…¸ä¸­çš„ï¼Œæ¯”å¦‚å†å²æœä»£çš„åç§°ï¼Œç‰¹æ®Šçš„å¹´ä»½â€œç”²åˆå¹´â€ç­‰ï¼Œä¸åšåˆ‡åˆ†ï¼Œå…¶ä»–çš„è¦æŒ‰ç…§â€œå¹´ã€æœˆã€æ—¥ã€æ—¶ã€åˆ†ã€ç§’â€çš„å±‚æ¬¡è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ7ï¼‰å•éŸ³èŠ‚ä»£è¯â€œæœ¬â€ã€â€œæ¯â€ã€â€œå„â€ã€â€œè¯¸ è‹¥åæ¥æˆåˆ†æ˜¯å•éŸ³èŠ‚åè¯ï¼Œåˆ™ä¸åšåˆ‡åˆ†ï¼Œè‹¥æ˜¯åŒéŸ³èŠ‚æˆ–å¤šéŸ³èŠ‚ï¼Œåˆ™è¦åˆ‡åˆ†å¼€ã€‚ ï¼ˆ8ï¼‰åŒºåˆ«è¯ é¦–å…ˆï¼Œæˆ‘ä»¬è¦æ˜ç¡®ä½•ä¸ºåŒºåˆ«è¯ï¼ŒåŒºåˆ«è¯æŒ‡çš„æ˜¯æˆå¯¹çš„ï¼Œæœ‰åˆ†ç±»æ€§è´¨çš„ä¸€ç±»è¯ï¼Œå®ƒä»¬åªèƒ½å¤Ÿåšå®šè¯­ï¼Œä¸èƒ½åšè°“è¯­ï¼Œæ‰€ä»¥åˆç§°ä¸ºéè°“å½¢å®¹è¯ã€‚ ä¸¾ä¾‹æ¥è¯´ï¼ŒåŒºåˆ«è¯åŒ…æ‹¬ï¼šç”·ã€å¥³ã€é›Œã€é›„ã€å•ã€åŒã€å¤ã€é‡‘ã€é“¶ã€è¥¿å¼ã€ä¸­å¼ã€å¤ä»£ã€è¿‘ä»£ã€ç°ä»£ã€å½“ä»£ã€é˜´æ€§ã€é˜³æ€§ã€å†›ç”¨ã€æ°‘ç”¨ã€å›½æœ‰ã€ç§æœ‰ã€å°å‹ã€ä¸­å‹ã€å¤§å‹ã€å¾®å‹ã€æœ‰æœŸã€æ— æœŸã€å½©è‰²ã€é»‘ç™½ã€æ€¥æ€§ã€æ…¢æ€§ã€å°å·ã€ä¸­å·ã€å¤§å·ã€é‡ç”Ÿã€å®¶å…»ã€æ­£å¼ã€éæ­£å¼ã€äººé€ ï¼ˆä»åŠ¨è¯è¿‡æ¥çš„ï¼‰ã€å¤©ç„¶ã€å†’ç‰Œã€æ­£ç‰Œã€æ­£ç‰ˆã€ç›—ç‰ˆã€ä¸‹ç­‰ã€ä¸­ç­‰ã€ä¸Šç­‰ã€åˆçº§ã€ä¸­çº§ã€é«˜çº§ã€ä¸­å¼ã€æ¬§å¼ç­‰ç­‰ã€‚ å¯¹äºå«æœ‰åŒºåˆ«è¯çš„åºåˆ—ï¼Œæˆ‘ä»¬çš„åˆ‡åˆ†åŸåˆ™ä¹Ÿæ˜¯åŒæ ·æŒ‰ç…§éŸ³èŠ‚æ¥è¿›è¡Œï¼Œå¦‚æœåŒºåˆ«è¯åæ¥ä¸€ä¸ªå•éŸ³èŠ‚åè¯ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼Œè‹¥æ¥çš„æ˜¯å¤šéŸ³èŠ‚åè¯ï¼Œåˆ™è¦åˆ‡åˆ†ã€‚ ï¼ˆ9ï¼‰è¿°è¡¥ç»“æ„ ç®€å•æ¥è¯´ï¼Œè¿°è¡¥ç»“æ„æŒ‡çš„æ˜¯æè¿°ä¸€ä¸ªåŠ¨è¯å‘ç”Ÿçš„æƒ…è²Œæˆ–ç»“æœï¼Œå³å¯¹åŠ¨è¯æ‰€ä»£è¡¨çš„äº‹ä»¶è¿›è¡Œçš„è¡¥å……ã€‚å¯¹äºåŒéŸ³èŠ‚çš„è¿°è¡¥ç»“æ„æˆ‘ä»¬çš„åˆ‡åˆ†åŸåˆ™æ˜¯ï¼Œå¦‚æœè¿›è¡Œåˆ‡åˆ†åï¼Œä¼šæœ‰æ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ å­˜åœ¨ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼Œåä¹‹ï¼Œåˆ™åˆ‡åˆ†ã€‚ è¿°è¡¥ç»“æ„ä¸­è¿˜æœ‰ä¸€ç±»å¸¸è§çš„å¤šéŸ³èŠ‚çš„â€œå¾—â€å­—è¡¥è¯­ï¼Œå¯¹äºè¿™ç±»è¿°è¡¥ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥å°†â€œå¾—â€å­—å»æ‰ï¼Œè‹¥å»æ‰åä¾ç„¶èƒ½æˆè¯ï¼Œåˆ™è¦å°†å…¶åˆ‡åˆ†ï¼›è‹¥ä¸èƒ½æˆè¯ï¼Œåˆ™â€œå¾—â€å­—è¡¥è¯­æ•´ä½“ä½œä¸ºä¸€ä¸ªåˆ†è¯å•ä½ï¼Œå†…éƒ¨ä¸åšåˆ‡åˆ†ã€‚ ï¼ˆ10ï¼‰ã€ï¼ˆ11ï¼‰ã€ï¼ˆ12ï¼‰ã€ï¼ˆ13ï¼‰ç•¥ ï¼ˆ14ï¼‰è¯­ç´ å’Œéè¯­ç´ å­—çš„å¤„ç† å¯¹äºç¦»åˆè¯çš„ç¦»æå½¢å¼ï¼Œè¦è¿›è¡Œåˆ‡åˆ†ã€‚æ‰€è°“ç¦»åˆè¯ï¼ŒæŒ‡çš„æ˜¯å¯ä»¥åœ¨ç»„åˆçš„ä¸¤ä¸ªè¯­ç´ ä¸­æ’å…¥å…¶ä»–æˆåˆ†çš„è¯ï¼Œæ¯”å¦‚â€œåƒé¥­â€ï¼Œå®ƒçš„ç¦»æå½¢å¼æœ‰ï¼Œâ€œåƒäº†é¥­â€â€œåƒäº†ä¸€ä¸ªé¥­â€ç­‰ã€‚ å¯¹äºè¡¨ç¤ºæ–¹ä½çš„åŒéŸ³èŠ‚è¯ï¼Œè‹¥åˆ‡åˆ†å‡ºæ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼Œå¦åˆ™åˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ15ï¼‰æ–‡æœ¬ä¸­éæ±‰å­—çš„å­—ç¬¦ä¸² ç•¥ ï¼ˆ16ï¼‰é‡å  é‡å æ˜¯æ±‰è¯­ç‹¬ç‰¹çš„è¯­è¨€ç°è±¡ä¹‹ä¸€ã€‚åŒ—å¤§æ–¹æ¡ˆä¸­å¯¹è¿™ç±»è¯çš„åˆ‡åˆ†çœ‹ä¼¼å¤æ‚ï¼Œå®è´¨ä¸Šæ˜¯åˆ‡åˆ†åˆ°èƒ½å¤Ÿç‹¬ç«‹ä½¿ç”¨çš„å•ä½ï¼Œå¹¶ä¸”è¦é¿å…åˆ‡åˆ†å‡ºä¸èƒ½å•ç‹¬æˆè¯çš„è¯­ç´ ã€‚ æ¯”å¦‚ï¼Œâ€œç”œç”œçš„èœ‚èœœâ€ï¼Œç”±äºâ€œç”œç”œâ€ä¸èƒ½å•ç‹¬æˆè¯ï¼Œå› æ­¤è¦åˆ‡åˆ†åˆ°â€œç”œç”œçš„â€ã€‚ è€Œâ€œè¯•è¯•çœ‹â€ç”±äºâ€œçœ‹â€è¿™é‡Œè¡¨ç¤ºåŠ¨ä½œçš„å°è¯•ï¼Œä½œä¸ºè¿™ä¸ªæ„ä¹‰å¹¶ä¸èƒ½å•ç‹¬è¿ç”¨ï¼Œå› æ­¤ä¸åˆ‡åˆ†ã€‚ ï¼ˆ17ï¼‰é™„åŠ æˆåˆ† é™„åŠ æˆåˆ†å®è´¨ä¸ŠæŒ‡çš„æ˜¯æ„è¯ä¸­çš„å‰ç¼€å’Œåç¼€ã€‚æ±‰è¯­æ„è¯æ³•ä¸­æœ‰ä¸€ç±»æ˜¯ä¾æ®è¯ç¼€åŠ è¯æ ¹è¿›è¡Œçš„æ´¾ç”Ÿæ„è¯ã€‚å¯¹äºè¿™ä¸€ç±»åˆ‡åˆ†åºåˆ—ï¼Œé™¤éå…¶æ¥å…¥æˆåˆ†å¤ªå¤šï¼Œä¼šå¯¹å…¶è¿›è¡Œåˆ‡åˆ†ï¼Œå¦åˆ™ä¸åˆ‡åˆ†ã€‚æ¯”å¦‚â€œè€å¸ˆä»¬â€å°±ä¸åšåˆ‡åˆ†ï¼Œâ€œè‹¦è‹¦è¿½æ±‚è€Œä¸å¾—è€…â€ä¸­çš„â€œè€…â€ç”±äºç»Ÿæ‘„çš„æˆåˆ†å¤ªå¤šï¼Œæ‰€ä»¥è¦å•ç‹¬åˆ‡åˆ†å¼€ã€‚ ï¼ˆ18ï¼‰å¤åˆè¯æ„è¯ åœ¨åˆ‡åˆ†å¤åˆè¯çš„é—®é¢˜ä¸Šï¼ŒåŒ—å¤§æ–¹æ¡ˆæ˜¯å­˜åœ¨è®¨è®ºçš„ä½™åœ°çš„ã€‚ç”±äºå¤åˆè¯æœ¬èº«å’ŒçŸ­è¯­ä¹‹é—´çš„ç•Œé™è¾ƒä¸ºæ¨¡ç³Šï¼Œå³ä½¿åœ¨è¯­è¨€å­¦æ„ä¹‰çš„ç•Œå®šä¸Šä¹Ÿä¼šå­˜åœ¨åˆ†æ­§ï¼Œå› æ­¤å¯¹äºå¤åˆè¯ç±»å‹çš„åˆ‡åˆ†åºåˆ—æ˜¯å¦åˆ‡åˆ†ï¼Œå®è´¨ä¸Šå¾ˆéš¾å›ç­”ã€‚åŒ—å¤§æ–¹æ¡ˆç»™å‡ºçš„è§£å†³åŠæ³•æ˜¯ï¼Œé¦–å…ˆå¦‚æœåˆ‡åˆ†åä¼šæœ‰æ— æ³•ç‹¬ç«‹æˆè¯çš„æˆåˆ†ï¼Œé‚£ä¹ˆå°±ä¸åˆ‡åˆ†ï¼›å¦å¤–è¦åˆ¤æ–­è¿™ä¸ªå¤åˆè¯çš„æ„ä¹‰æ˜¯å¦åªæ˜¯ç»„æˆæˆåˆ†çš„ç®€å•ç›¸åŠ ï¼Œå¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå°±åˆ‡åˆ†ï¼Œå¦‚æœä¸æ˜¯ï¼Œé‚£å°±è¯´æ˜ç»„æˆè¯¥è¯çš„ä¸¤ä¸ªæˆåˆ†ä¹‹é—´æ„ä¹‰æ˜¯æœ‰ç›¸äº’æ¸—é€çš„è”ç»“çš„ï¼Œå°±ä¸èƒ½åˆ‡åˆ†ã€‚ä½†æ˜¯å¦‚ä½•åˆ¤æ–­å¤åˆè¯æ„ä¹‰æ˜¯å¦æ˜¯ç»„åˆæˆåˆ†çš„ç›¸åŠ å‘¢ï¼Ÿ è¿™é‡Œçš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯åŠ â€œçš„â€æ³•ã€‚è¿™ä¸ªæ–¹æ³•ä¸»è¦é’ˆå¯¹çš„æ˜¯å®šä¸­ç»“æ„çš„å¤åˆè¯ï¼Œå³ä¸€ä¸ªè¯­ç´ ä¿®é¥°å¦ä¸€ä¸ªè¯­ç´ ã€‚æ¯”å¦‚â€œç™½èŠ±â€ï¼Œå’Œâ€œç™½çš„èŠ±â€æ„ä¹‰ä¸€è‡´ï¼Œé‚£ä¹ˆå°±è¦åˆ‡åˆ†ã€‚ ç¬¬äºŒä¸ªæ–¹æ³•æ˜¯æ›¿æ¢æ³•ï¼Œå°†å¤åˆè¯â€œABâ€çš„Aè¯­ç´ æ‹¿å‡ºæ¥è¿›è¡Œç»„è¯ï¼Œå†å°†Bè¯­ç´ æ‹¿å‡ºæ¥è¿›è¡Œç»„è¯ï¼Œè‹¥å•ç‹¬ç»„è¯åå…¶è¯ä¹‰éƒ½æ˜¯ä¸€æ ·çš„ï¼Œé‚£ä¹ˆå°±è¯´æ˜å¤åˆè¯ABçš„è¯ä¹‰æ˜¯Aè¯­ç´ ä¹‰å’ŒBè¯­ç´ ä¹‰çš„ç›¸åŠ ï¼Œå› æ­¤è¦åˆ‡åˆ†ï¼›è‹¥æœ‰Aè¯­ç´ æˆ–Bè¯­ç´ æœ‰å’Œå…¶ä»–ç»„è¯æƒ…å†µä¸­è¯­ä¹‰ä¸åŒçš„ï¼Œé‚£ä¹ˆå°±ä¸åˆ‡åˆ†å¤åˆè¯ABã€‚ ä½†æ˜¯è¿™ä¸¤ä¸ªæ–¹æ³•å¹¶ä¸èƒ½è§£å†³æ‰€æœ‰çš„å¤åˆè¯åˆ¤æ–­é—®é¢˜ï¼Œå› æ­¤åˆ°åº•æ˜¯å°†é—®é¢˜ç®€åŒ–è¿˜æ˜¯å¯¹è§„åˆ™è¿›ä¸€æ­¥ç»†è‡´ï¼Œæ˜¯å€¼å¾—æ€è€ƒçš„ã€‚ é¢—ç²’åº¦æ–¹æ¡ˆï¼ˆè°ƒæ•´ç‰ˆï¼‰è°ƒæ•´å†…å®¹ï¼š å°†åŸæ¥çš„ç¬¬ä¸€ç²’åº¦ä½œä¸ºç»†ç²’åº¦ï¼ˆéå¸¸ç»†ï¼Œå­˜åœ¨è¯­ä¹‰ä¸é€æ˜çš„è¯ç¼€ï¼‰ï¼Œå°†ç¬¬äºŒç²’åº¦å’Œç¬¬ä¸‰ç²’åº¦åˆå¹¶æˆä¸ºç²—ç²’åº¦ï¼‰ï¼Œé’ˆå¯¹ä¸“æœ‰åè¯çš„é—®é¢˜ï¼Œåˆ’å‡ºç²—ç²’åº¦2çº§ï¼ˆè¿™ä¸ªå¯ä»¥è®¨è®ºï¼Œæ˜¯åœ¨åˆ†è¯ä¸­ä¸€ä¸‹å­åˆ’åˆ†å‡ºæ¥ï¼Œè¿˜æ˜¯åœ¨ä¸Šæ¸¸ä»»åŠ¡ä¸­å†å¤„ç†ã€‚åœ¨å‚è€ƒèµ„æ–™çš„ä¸“åˆ©ä¸­ï¼Œä»–ä»¬å¾€å¾€åœ¨åˆ†è¯ä¸­å°±è§£å†³äº†ï¼‰ã€‚ ç†æ¸…å®ä½“å’Œä¸“æœ‰åè¯çš„åŒºåˆ«ç»†ç²’åº¦ å•éŸ³è¯ å•ç‹¬ä¸€ä¸ªè¯­ç´ å³å¯æˆè¯çš„ï¼Œå¦‚â€œç«ã€ä¹¦ã€æ°´â€ è¿ç»µè¯ å¿…é¡»å’Œå…¶ä»–è¯­ç´ ç»“åˆæˆè¯çš„ï¼Œä¸”ç»“åˆçš„è¯­ç´ æ˜¯å›ºå®šçš„ï¼Œå¦‚â€œè‘¡è„â€â€œä¹’ä¹“â€ éŸ³è¯‘è¯ åŒ…æ‹¬äº†å¤–å›½çš„ä¸“åï¼ˆäººåç­‰ï¼‰ æ•°è¯ é‡è¯ æ¯”å¦‚ï¼šæ¡ã€ä¸²ã€å¼  è¿™é‡Œè¦æ³¨æ„ä¸€äº›ä»åè¯å‘å±•è¿‡æ¥çš„é‡è¯ï¼Œæ¯”å¦‚â€œç¢—â€ è¿™é‡ŒåŒ…æ‹¬åº¦é‡ï¼š3/cmï¼Œ7/å¤© å¦å¤–ç»†ç²’åº¦ä¸­ï¼Œæ—¶é—´æ•°å’Œæ—¶é—´å•ä½ä¹Ÿåˆ‡åˆ†å¼€ï¼Œå¦‚ï¼š2018/å¹´ ä¸å«è¡Œæ”¿åŒºåˆ’çš„åœ°å æ¯”å¦‚ï¼šä¸Šæµ·ã€åŒ—äº¬ã€æ­¦æ±‰ ä¸“æœ‰åè¯ï¼šæœºæ„ã€å›¢ä½“ã€ç»„ç»‡ æ˜¯ä¸€ä¸ªå°é—­ç±»ï¼Œæ˜¯ä¸å¯ç±»æ¨çš„ åŒ…å«ä¸Šä¸‹éš¶å±å…³ç³»çš„å›¢ä½“æœºæ„ä¸“æœ‰åè¯ï¼Œåˆ‡åˆ†åˆ°æœ€å°çš„å›¢ä½“æœºæ„ã€‚æ¯”å¦‚â€œä¸­å›½/é“¶è¡Œ/åŒ—äº¬/åˆ†è¡Œâ€ã€‚ ç®€ç§°ç•¥è¯­ æ–¹ä½è¯ è¯­æ°”è¯ å¹è¯ å®è¯­ç´  åŒ…æ‹¬åŒ—å¤§æ–¹æ¡ˆé‡Œçš„å½¢è¯­ç´ ã€åè¯­ç´ ã€åŠ¨è¯­ç´ ã€äººåä¸­çš„å§“æ°ï¼Œæ¯”å¦‚ï¼šé”¦ï¼ˆå½¢è¯­ç´ ï¼‰ è™šè¯­ç´  å‰æ¥æˆåˆ† æ¯”å¦‚â€œé˜¿â€â€œè€â€â€œéâ€ è¿™ç±»é™¤äº†ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å‰ç¼€ï¼Œä¹Ÿè¦è€ƒè™‘ä¸€äº›ç½‘ç»œæµè¡Œè¯­çš„ä¸´æ—¶æ„è¯äº§å‡ºçš„å‰ç¼€ å‰¯è¯­ç´  ä¸»è¦æ˜¯å¦å®šå‰¯è¯ï¼Œæ¯”å¦‚â€œä¸â€â€œå¾ˆâ€ åæ¥æˆåˆ† æ¯”å¦‚ï¼šä»¬ï¼Œå„¿ï¼ˆè¡¨äº²æ˜µçš„ï¼‰ï¼Œå­ï¼Œå¤´ï¼ŒåŒ–ï¼Œè€… æˆ‘è®¤ä¸ºï¼Œè¿˜åº”åŒ…æ‹¬è¡Œæ”¿åŒºåˆ’çš„å•ä½ï¼Œæ¯”å¦‚ï¼šçœã€å¸‚ã€åŒºç­‰ï¼›å’Œè¡¨ç¤ºå°Šç§°çš„â€œè€â€â€œæ€»â€ åŠ©è¯ åŠ©åŠ¨è¯ã€åŠ©æ•°è¯ ä¹ è¯­ åŒ…æ‹¬æˆè¯­ã€å››å­—æ ¼çŸ­è¯­ã€æ­‡åè¯­ ä½†æ˜¯å¦‚æœæ­‡åè¯­æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œè¦æŒ‰ç…§æ ‡ç‚¹ç¬¦å·åˆ’åˆ† æ¯”å¦‚ï¼šâ€œä¸ç®¡ä¸‰ä¸ƒäºŒåä¸€â€â€œç™¾å°ºç«¿å¤´/ï¼Œ/æ›´è¿›ä¸€æ­¥â€ ç²—ç²’åº¦ç®€è¨€ä¹‹ï¼šåˆ‡åˆ°è¯ç»„å±‚ï¼Œä¸”æ³¨æ„éŸ³èŠ‚æ•°ï¼Œå¯¹åŒéŸ³èŠ‚æ”¾å®½ã€‚å°†ç»†ç²’åº¦ä¸­å¯æˆè¯çš„ç»„åˆæˆè¯ï¼ˆæ´¾ç”Ÿè¯ï¼‰ï¼Œå¦å°†å¯ç‹¬ç«‹æˆè¯çš„è¯æ ¹ç»“åˆæˆå¤åˆè¯ã€‚ç²—ç²’åº¦çš„åˆ‡åˆ†ç›®æ ‡æ˜¯ï¼Œä½¿å¾—æ¯ä¸€ä¸ªå®è¯æ€§çš„åˆ‡åˆ†å•ä½éƒ½æ˜¯è¡¨ä¹‰æ˜ç¡®çš„åˆ†è¯å•ä½ï¼Œä¸å­˜åœ¨è¯­ä¹‰ä¸é€æ˜çš„åˆ†è¯å•ä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿä¸èƒ½å¥¢æ±‚å®ä½“è¯†åˆ«ç­‰ä¸Šæ¸¸ä»»åŠ¡åœ¨åˆ†è¯ä»»åŠ¡ä¸­å°±å¾—ä»¥è§£å†³ã€‚ å‰æ¥æˆåˆ†+åè¯ æ¯”å¦‚ï¼šé˜¿ç‰› å‰æ¥æˆåˆ†+æ•° æ¯”å¦‚ï¼šé˜¿å¤§ åè¯+åæ¥æˆåˆ† æ¯”å¦‚ï¼šå­¦ç”Ÿä»¬ã€è€å¸ˆä»¬ã€æ‹³å¤´ã€é«˜æ¸…ç‰ˆ åŠ¨è¯+åæ¥æˆåˆ† æ¯”å¦‚ï¼šåˆ›æ–°åŒ–ï¼ˆå•ç‹¬â€œåˆ›æ–°â€è¿˜æ˜¯åˆ†åˆ°â€åˆ›æ–°â€œï¼‰ å§“æ°+å æ¯”å¦‚ï¼šå¼ ä¼Ÿ æ•°+é‡+ï¼ˆåŠ©æ•°è¯ï¼‰ æ¯”å¦‚ï¼šå››/äººï¼Œäº”ä¸ª/äºº æ—¶é—´ æŒ‰åŒ—å¤§æ–¹æ¡ˆï¼Œä¸è¦åˆå¹¶ æ¯”å¦‚ï¼š1997å¹´/9æœˆ/3æ—¥ï¼Œæ—©/å…«ç‚¹ å¤åˆè¯ åŒéŸ³èŠ‚ã€ä¸‰éŸ³èŠ‚ï¼ˆåˆ‡åˆ†åŸåˆ™è¯¦è§å¯¹åŒ—å¤§æ–¹æ¡ˆçš„è®²è§£ï¼‰ æ³¨æ„ï¼Œä¸è¦å°†è”åˆæ„è¯çš„è¯ç»„ç®—ä½œå¤åˆè¯ã€‚ åœ°å+è¡Œæ”¿åŒºåˆ’ æ¯”å¦‚ï¼šåŒ—äº¬å¸‚ã€ä¸Šæµ·å¸‚ åœ°å+è‡ªç„¶åœ°å½¢ æ¯”å¦‚ï¼šååŒ—å¹³åŸã€å—æ²™ç¾¤å²› ç²—ç²’åº¦ä¸‹çš„åˆ‡åˆ†éš¾ç‚¹1.ä¸“åå’Œå®ä½“çš„åˆ‡åˆ†ä¸“æœ‰åè¯æŒ‡çš„æ˜¯ä¸“æŒ‡æ€§çš„äººåã€åœ°åã€å›¢ä½“ã€æœºæ„ã€ç»„ç»‡ã€æ°‘æ—ã€å•†æ ‡ã€‚ äººåã€åœ°åã€æ°‘æ—ã€å•†æ ‡åŸºæœ¬ä¸Šæ²¡æœ‰å¼‚è®®ï¼Œä½†æ˜¯å“ªäº›å›¢ä½“ã€æœºæ„ã€ç»„ç»‡èƒ½ç®—ä¸“æœ‰åè¯ï¼Œå“ªäº›ä¸èƒ½ç®—æ˜¯ä¸å¤ªæ˜ç¡®çš„ã€‚ å¦å¤–ï¼Œé™¤ä¸Šé¢æŒ‡å‡ºçš„åˆ†ç±»å¤–ï¼Œå…¶ä»–çš„å…·æœ‰ä¸“æŒ‡æ€§çš„å®ä½“ï¼Œä¸èƒ½è¢«å½“åšä¸“æœ‰åè¯æ¥å¤„ç†ã€‚å…·ä½“æ¥è¯´ï¼Œä¸“æœ‰åè¯çš„åˆ‡åˆ†éš¾ç‚¹æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š ï¼ˆ1ï¼‰ ä¸“æœ‰åè¯çš„ä¸“æŒ‡æ€§æ˜¯å¿½ç•¥æ–‡æœ¬è¯­å¢ƒã€‚æ¯”å¦‚â€æ ¡é•¿åŠå…¬å®¤å‘å¸ƒé‡è¦é€šçŸ¥â€œï¼Œå³ä½¿é€šè¿‡å‰æ–‡æˆ‘ä»¬çŸ¥é“è¿™é‡ŒæŒ‡çš„æ˜¯åŒ—å¤§çš„æ ¡é•¿åŠå…¬å®¤ï¼Œæˆ‘ä»¬åªå°†å®ƒä½œä¸ºæ™®é€šåè¯çš„å¤„ç†ï¼Œè€Œä¸æ˜¯ä½œä¸ºä¸€ä¸ªä¸“æŒ‡æ€§çš„æœºæ„åæ¥å¤„ç†ã€‚ ä½†æ˜¯åœ¨å›½é™…æˆ–ä¸­å›½èŒƒå›´å†…çš„çŸ¥åçš„å”¯ä¸€çš„å›¢ä½“ã€æœºæ„ã€ç»„ç»‡ çš„åç§°æˆ‘ä»¬ä¾ç„¶å°†ä¹‹å¤„ç†ä¸ºä¸“åï¼Œæ¯”å¦‚â€œå›½åŠ¡é™¢â€ï¼Œå®ƒå’Œâ€œæ ¡é•¿åŠå…¬å®¤â€çš„åŒºåˆ«åœ¨äºâ€œå›½åŠ¡é™¢â€å…¨å›½åªæœ‰ä¸€ä¸ªï¼Œè€Œâ€œæ ¡é•¿åŠå…¬å®¤â€æœ‰å¾ˆå¤šä¸ªï¼Œå› æ­¤â€œå›½åŠ¡é™¢â€ä½œä¸ºä¸“åä¸åˆ‡åˆ†ï¼Œè€Œâ€œæ ¡é•¿åŠå…¬å®¤â€è¦åˆ‡åˆ†æˆâ€œæ ¡é•¿/åŠå…¬å®¤â€ã€‚ ï¼ˆ2ï¼‰ä¸“æœ‰åè¯çš„ç»„åˆæ€§ã€‚ä¸“æœ‰åè¯æœ‰æ—¶ä¼šå’Œå…¶ä»–åè¯ä¸€èµ·ç»„åˆæˆè¯ã€‚å¯¹äºåˆ†è¯ä»»åŠ¡è€Œè¨€ï¼Œæˆ‘ä»¬åªéœ€è€ƒè™‘å°†ä¸“æœ‰åè¯å’Œè¿™ä¸ªè¯åˆ‡å¼€åè¿™ä¸ªè¯èƒ½å¦å•ç‹¬æˆè¯ï¼Œå¦‚æœä¸èƒ½ï¼Œé‚£ä¹ˆå°±ä¸åˆ‡åˆ†ï¼Œå¦‚æœèƒ½ï¼Œé‚£ä¹ˆå°±åˆ‡åˆ†ã€‚ï¼ˆè¿™é‡Œå’ŒåŒ—å¤§æ–¹æ¡ˆä¸åŒï¼ŒåŒ—å¤§æ–¹æ¡ˆè®¤ä¸ºæ¥å•éŸ³èŠ‚å¯ä»¥åˆ‡åˆ†ï¼Œä¹Ÿå¯ä»¥ä¸åˆ‡åˆ†ã€‚ï¼‰æ¯”å¦‚â€æ»¡äººâ€œï¼Œâ€å“ˆè¨å…‹äººâ€œï¼Œâ€æ˜Œå¹³/åˆ†è¡Œâ€œï¼Œè€Œå¯¹äºä¸€äº›å¤šä¸ªåè¯ç»„åˆæˆä¸“åçš„æƒ…å†µï¼Œæ¯”å¦‚â€œå…¨å›½/æ€»/å·¥ä¼šâ€â€œå…¨å›½/äººæ°‘/ä»£è¡¨/å¤§ä¼šâ€œï¼Œåœ¨ç»†ç²’åº¦å’Œç²—ç²’åº¦ä¸­ï¼Œç”±äºå®ƒä»¬éŸ³èŠ‚æ•°è¾ƒå¤šï¼Œè§†ä¸ºæ™®é€šåè¯è¿›è¡Œåˆ‡åˆ†ã€‚æ˜¯å¦å¯ä»¥è®¾ç½®ä¸€ä¸ªç²—ç²’åº¦2çº§ï¼Œåœ¨ç²—ç²’åº¦2çº§ä¸­ï¼Œä½œä¸ºç»„ç»‡ç±»ä¸“æœ‰åè¯ï¼Œä¸åˆ‡åˆ†ã€‚ ï¼ˆ3ï¼‰ä¸“æœ‰åè¯å±‚æ¬¡æ€§ã€‚è¡¨ç¤ºæœºæ„çš„ä¸“æœ‰åè¯ä¸­æœ‰äº›æ˜¯å‰åç›¸è¿ï¼ŒåŒ…å«ä¸Šä¸‹éš¶å±å…³ç³»çš„ã€‚ä¸‹çº§æœºæ„çš„ä¸“æŒ‡æ€§æœ‰çš„æ˜¯ä»ç”±ä¸Šçº§å›¢ä½“ç»§æ‰¿æ¥çš„ï¼Œæ¯”å¦‚â€œåŒ—äº¬å¤§å­¦è®¡ç®—è¯­è¨€å­¦ç ”ç©¶æ‰€â€æ˜¯ä¸€ä¸ªä¸“æŒ‡æ€§çš„çŸ­è¯­ï¼Œå®ƒä¹‹æ‰€ä»¥æœ‰ä¸“æŒ‡æ€§ï¼Œæ˜¯å› ä¸ºâ€œåŒ—äº¬å¤§å­¦â€è¿™ä¸ªä¸“æœ‰åè¯çš„ä¸“æŒ‡æ€§ï¼Œå¦‚æœæ²¡æœ‰â€œåŒ—äº¬å¤§å­¦â€ï¼Œåˆ™â€œè®¡ç®—è¯­è¨€å­¦ç ”ç©¶æ‰€â€æŒ‰ç…§æ™®é€šåè¯è¯ç»„æ¥åˆ‡åˆ†ï¼ˆå‚ç…§ç¬¬ä¸€ç‚¹ï¼‰ï¼›æœ‰çš„æ˜¯é€šè¿‡å…¶ä»–ä¸“æœ‰åè¯ï¼Œå¦‚åœ°åã€äººåè·å¾—çš„ï¼Œæ¯”å¦‚â€œé²è¿…ç ”ç©¶é™¢â€ï¼Œâ€œåŒ—äº¬åˆ†è¡Œâ€ã€‚åœ¨ç²—ç²’åº¦ä¸­ï¼Œå¯¹äºè·å¾—ä¸“æŒ‡æ€§çš„ä¸“æœ‰åè¯ä¸åˆ‡åˆ†ï¼Œå¦‚â€œé²è¿…ç ”ç©¶é™¢â€ï¼Œâ€œåŒ—äº¬åˆ†è¡Œâ€æ˜¯å¦å¯ä»¥è®¾ç½®ç²—ç²’åº¦2çº§ï¼Œè¡¨ç¤ºä¸Šä¸‹çº§çš„ä¸“æœ‰åè¯å…¨éƒ¨çº³å…¥ï¼Ÿæ¯”å¦‚â€œåŒ—äº¬å¤§å­¦è®¡ç®—è¯­è¨€å­¦ç ”ç©¶æ‰€â€ï¼Œåœ¨ç²—ç²’åº¦2çº§ä¸­å°±ä¸åšåˆ‡åˆ†ã€‚ ï¼ˆ4ï¼‰ç”µè§†èŠ‚ç›®ã€æ–‡è‰ºä½œå“ï¼ˆä¹¦ã€æ–‡æ¡£ã€åè®®ï¼‰æ ‡é¢˜ã€ç”µè§†å‰§ã€æˆ˜äº‰åç­‰ï¼Œä¸ä½œä¸ºä¸“æœ‰åè¯ï¼ŒæŒ‰ç…§æ™®é€šåè¯åˆ’åˆ†ã€‚ä¸¾ä¾‹ï¼š ä¼Šæ‹‰å…‹/æˆ˜äº‰ è¾›äº¥/é©å‘½ å¹³æ´¥/æˆ˜å½¹ å¼€å¿ƒ/è¯å…¸ æ–°é—»/30åˆ† æ–°é—»/æ—©/8ç‚¹ ä¸­å¤®ç”µè§†å°/-/1ï¼ˆå®ƒä»¬åæœŸå¯ä»¥é€šè¿‡ä¹¦åå·å’Œå¼•å·è¯†åˆ«å‡ºæ¥ã€‚ï¼‰ 2.æ”¿æ²»è¯è¯­æ˜¯å¦ç®—ä½œä¹ è¯­ï¼Ÿï¼ˆå¯ä»¥è®¨è®ºï¼‰æ”¿æ²»å£å·å’Œæ”¿æ²»æ€æƒ³ç”±äºåœ¨ä¸€å®šçš„å†å²æ—¶æœŸä¸­é¢‘ç¹ä½¿ç”¨ï¼Œå› æ­¤ï¼Œå¦‚æœåˆ‡åˆ†è¡¨æ„å°±ä¸ä¸€æ ·ã€‚æ¯”å¦‚â€œä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³â€å’Œâ€œä¹ è¿‘å¹³æ–°æ—¶ä»£ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³â€å°±æ˜¯ä¸¤ä¸ªæ¦‚å¿µã€‚ æœ‰ä¸¤ä¸ªè§£å†³æ–¹æ¡ˆï¼Œä¸€ä¸ªæ˜¯å°†éŸ³èŠ‚è¾ƒçŸ­çš„æ”¿æ²»è¯è¯­ç®—ä½œè¯­æ³•è¯å…¸ä¸­çš„è¯ï¼Œå¦‚â€œç§‘æŠ€å¼ºå›½â€â€œç§‘æ•™å…´å›½â€â€œç»¿è‰²ç»æµâ€ï¼Œâ€œç§‘æŠ€åˆ›æ–°â€ç­‰ç­‰ï¼Œç„¶åé‡åˆ°è¿™æ ·çš„è¯ï¼Œç»†ç²’åº¦ã€ç²—ç²’åº¦é‡Œéƒ½ä¸åˆ‡åˆ†ï¼Œè€ŒéŸ³èŠ‚è¾ƒé•¿çš„ï¼Œæ¯”å¦‚â€œä¸­åæ°‘æ—ä¼Ÿå¤§å¤å…´â€å°±ä½œä¸ºæ™®é€šåè¯è¿›è¡Œåˆ‡åˆ†ï¼›ç¬¬äºŒä¸ªè§£å†³æ–¹æ¡ˆæ˜¯å…¨éƒ¨æŒ‰ç…§æ™®é€šåè¯åˆ‡åˆ†ï¼Œåˆ°å…·ä½“çš„ä»»åŠ¡éœ€æ±‚æ—¶å†å¤„ç†ã€‚ä¸è¿‡ï¼Œæˆ‘è§‰å¾—è¿™ä¸¤ä¸ªè§£å†³æ–¹æ¡ˆéƒ½ä¼šå½±å“åˆ†è¯ç²’åº¦æ•´ä½“çš„å¹³è¡¡åº¦ï¼Œå› ä¸ºæ”¿æ²»å£å·æ„è¯æœ‰æ—¶éå¸¸éå¸¸é•¿ã€‚ 3.æŸæŸç†è®ºçš„åç§°ç®—ä½œä¸“åå—ï¼ŸæŸæŸé¢†åŸŸç†è®ºä¸­çš„ä¸“ä¸šæœ¯è¯­ç®—ä½œä¸“åå—ï¼Ÿç†è®ºçš„å‘½ååŒæ ·æ˜¯ä»»æ„æ€§çš„å‘½åè¡Œä¸ºï¼Œå’Œèœåä¸€æ ·ï¼Œå¦‚æœå¯¹â€œxxxç†è®ºâ€ä¸­çš„â€œxxxâ€è¿›è¡Œåˆ‡åˆ†åï¼Œâ€œxxxâ€çš„æ„æ€æœ‰æ‰€æ”¹å˜ï¼Œé‚£ä¹ˆå°±ä¸èƒ½åˆ‡åˆ†ï¼Œå¦‚æœæ²¡æœ‰æ”¹å˜ï¼Œåˆ™å¯ä»¥åˆ‡åˆ†ã€‚æ¯”å¦‚â€œç²¾ç¥åˆ†æ/ç†è®ºâ€ï¼Œå¦‚æœåˆ‡åˆ†æˆâ€œç²¾ç¥/åˆ†æâ€ï¼Œè¿™ä¸ªâ€œç²¾ç¥â€å’Œâ€œä½ ä»Šå¤©ç²¾ç¥ä¸ä½³â€ä¸­çš„â€œç²¾ç¥â€å¹¶ä¸æ˜¯ä¸€ä¸ªæ„æ€ï¼Œå› æ­¤ä¸èƒ½åˆ‡åˆ†ã€‚è€Œâ€œç‰›é¡¿/ç¬¬äºŒ/å®šå¾‹â€åˆ‡åˆ†åæ²¡é—®é¢˜ï¼Œå› ä¸ºè¿™ä¸ªç†è®ºçš„å‘½åæœ¬èº«æ˜¯ç»„åˆè€Œæˆçš„ã€‚ é‚£ä¹ˆå„ä¸ªé¢†åŸŸä¸­çš„ä¸“ä¸šæœ¯è¯­æ˜¯å¦ç®—ä½œä¸“åå‘¢ï¼Ÿæˆ‘è®¤ä¸ºåœ¨é€šç”¨å‹çš„åˆ†è¯ä¸­ï¼ŒåªåŠ å…¥æœ€ä¸ºé‡è¦çš„ä¸€äº›ä¸“ä¸šæœ¯è¯­ï¼›è€Œåœ¨ç‰¹å®šé¢†åŸŸä¸­ï¼Œå†åœ¨è¿™æ–¹é¢è¿›è¡Œæ‹“å±•ã€‚å› æ­¤ï¼Œâ€œç¤¾ä¼šç”Ÿæ´»â€åœ¨ç¤¾ä¼šå­¦ä¸­åº”å½“ç®—ä½œä¸€ä¸ªä¸“ä¸šæœ¯è¯­ï¼Œä½†æ˜¯åœ¨é€šç”¨å‹çš„åˆ†è¯ä¸­è¿˜æ˜¯æŒ‰ç…§æ™®é€šåè¯æ¥è¿›è¡Œåˆ‡åˆ†ï¼Œå³â€œç¤¾ä¼š/ç”Ÿæ´»â€ã€‚ 4.å¹¶åˆ—æˆåˆ†å¦‚ä½•åˆ‡åˆ†ï¼Ÿå¹¶åˆ—æˆåˆ†æŒ‰ç…§é¡¿å·è¿›è¡Œåˆ‡åˆ†ï¼Œæ¯”å¦‚â€œå¹³æ´¥/ã€/è¾½æ²ˆ/æˆ˜å½¹â€ï¼Œâ€å¼ /ã€/æå®¶â€œï¼ˆè¿™é‡Œçš„â€å¼ â€œå¯ä»¥çœ‹åšæ˜¯â€å¼ å®¶â€œçš„ç¼©ç•¥å½¢å¼ï¼‰ã€‚","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://shamy1997.github.io/tags/NLP/"},{"name":"åˆ†è¯","slug":"åˆ†è¯","permalink":"http://shamy1997.github.io/tags/åˆ†è¯/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-05-30T06:23:49.000Z","updated":"2019-01-20T11:55:31.044Z","comments":true,"path":"passages/hello-world/","link":"","permalink":"http://shamy1997.github.io/passages/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}
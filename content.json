{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/05/30/hello-world/"},{"title":"ç»ˆç«¯å‘½ä»¤ç¬”è®°","text":"vi å¸¸ç”¨å‘½ä»¤1. \bè¿›å…¥ vi ç¼–è¾‘å™¨1sudo vi &lt;path&gt; 2. ä¿®æ”¹å†…å®¹è¾“å…¥iï¼Œè¿›å…¥insertæ¨¡å¼ã€‚æŒ‰escï¼Œé€€å‡ºæ¨¡å¼ã€‚ 3. ä¿å­˜ï¼Œé€€å‡º :w ä¿å­˜æ–‡ä»¶ä½†ä¸é€€å‡ºvi; :wq ä¿å­˜æ–‡ä»¶å¹¶é€€å‡ºvi; q: ä¸ä¿å­˜æ–‡ä»¶ï¼Œé€€å‡ºvi :e! æ”¾å¼ƒæ‰€æœ‰ä¿®æ”¹ï¼Œä»ä¸Šæ¬¡ä¿å­˜æ–‡ä»¶å¼€å§‹å†ç¼–è¾‘ viå‘½ä»¤å¤§å…¨ Jupyter or conda not foundæˆ‘å®‰è£…å¥½anacondaåï¼Œæ‰“ç®—ç”¨å‘½ä»¤è¡Œç›´æ¥æ‰“å¼€jupyter notebookï¼Œç»“æœå´æ²¡æœ‰æˆåŠŸï¼Œç½‘ä¸Šä¸€èˆ¬çš„è§£é‡Šæ˜¯è¦æŠŠanacondaé…ç½®åˆ°ç¯å¢ƒå˜é‡é‡Œï¼šåœ¨ç»ˆç«¯ä¸­è¾“å…¥ï¼š 1sudo vi ~/.bash_profile æ‰“å¼€ååœ¨æœ«å°¾åŠ ä¸Šï¼š 1234567export PATH='~/anaconda/bin:$PATH'# è¿™é‡Œçš„pathè¦æ ¹æ®anacondaæ‰€åœ¨çš„ä½ç½®å®šä¹‰source ~/.bash_profile# è¡¨ç¤ºä¿®æ”¹ç«‹å³ç”Ÿæ•ˆ ä½†æ˜¯å‘¢ï¼Œæˆ‘è¯•äº†å¥½å‡ æ¬¡éƒ½æ²¡æœ‰æˆåŠŸï¼Œäº‹å®ä¸Šï¼Œæ˜¯æˆ‘é…ç½®äº†oh-my-zshçš„åŸå› ã€‚ å› æ­¤ï¼Œæ­£ç¡®çš„è§£å†³æ–¹æ³•æ˜¯ï¼Œæ‰“å¼€~/.zshrcï¼Œç„¶ååœ¨æ–‡ä»¶æœ€åä¸€è¡Œæ·»åŠ ï¼š 1export PATH=$PATH:$HOME/anaconda/bin ä¿å­˜æ–‡ä»¶åï¼Œå…³é—­çª—å£\bï¼Œé‡æ–°å¼€å¯çª—å£æ—¶ï¼Œè¾“å…¥å‘½ä»¤conda --væ¥æ£€æµ‹æ˜¯å¦æˆåŠŸã€‚ å‚è€ƒé“¾æ¥ğŸ”— zsh not found1exec /bin/zsh å‚è€ƒèµ„æ–™ğŸ”—","link":"/2018/12/22/about-terminal/"},{"title":"TensorFlowå°è¯•ç‰›åˆ€","text":"æ­¤æ—¥å¿—ä¸ºå‚ç…§Udacityè¯¾ç¨‹ä¸­ã€ŠIntro to tensorflowã€‹çš„jupyter notebookæ‰€åšçš„åˆ†è§£æºç ï¼Œç›®çš„åœ¨äºç†è§£ä»£ç é€»è¾‘ï¼Œç†Ÿæ‚‰åˆ›å»ºæµç¨‹å’Œå¥—è·¯ã€‚å…¶ä¸­å‚è€ƒäº†ä¸å°‘åšæ–‡é“¾æ¥ï¼Œéå¸¸æ„Ÿè°¢ï¼Œå…¨éƒ¨æ”¾åœ¨æ–‡æœ«ï¼Œåœ¨åŸæ–‡ä¸­ä¸å†æŒ‡å‡ºã€‚ æ•°æ®é“¾æ¥ï¼šç™¾åº¦äº‘ï¼šNoMNIST å¯†ç ï¼šfsks P1:é¢„å¤„ç†æ•°æ®123456789101112import hashlibimport osimport picklefrom urllib.request import urlretrieveimport numpy as npfrom PIL import Imagefrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.utils import resamplefrom tqdm import tqdmfrom zipfile import ZipFile è§£å‹å›¾ç‰‡æ–‡ä»¶1234567891011121314151617181920212223242526272829303132333435363738394041def uncompress_features_labels(file): \"\"\" Uncompress features and labels from a zip file :param file: The zip file to extract the data from \"\"\" features = [] labels = [] with ZipFile(file) as zipf: # Progress Bar filenames_pbar = tqdm(zipf.namelist(), unit='files') # Get features and labels from all files for filename in filenames_pbar: # Check if the file is a directory if not filename.endswith('/'): with zipf.open(filename) as image_file: image = Image.open(image_file) image.load() # Load image data as 1 dimensional array # We're using float32 to save on memory space feature = np.array(image, dtype=np.float32).flatten() # Get the the letter from the filename. This is the letter of the image. label = os.path.split(filename)[1][0] features.append(feature) labels.append(label) return np.array(features), np.array(labels)# Get the features and labels from the zip filestrain_features, train_labels = uncompress_features_labels('notMNIST_train.zip')test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')# Limit the amount of data to work with a docker containerdocker_size_limit = 150000train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)# Set flags for feature engineering. This will prevent you from skipping an important step.is_features_normal = Falseis_labels_encod = False 12100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210001/210001 [00:54&lt;00:00, 3832.78files/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10001/10001 [00:03&lt;00:00, 3207.15files/s] Min-Max ScalingImplement Min-Max scaling in the normalize_grayscale() function to a range of a=0.1 and b=0.9. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9. Since the raw notMNIST image data is in grayscale, the current values range from a min of 0 to a max of 255. Min-Max Scaling:$Xâ€™=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}$ 123456789101112def normalize_grayscale(image_data): \"\"\" Normalize the image data with Min-Max scaling to a range of [0.1, 0.9] :param image_data: The image data to be normalized :return: Normalized image data \"\"\" a = 0.1 b = 0.9 max_grayscale = 255 min_grayscale = 0 return a+((image_data-min_grayscale))*(b-a)/(max_grayscale-min_grayscale) 12train_features = normalize_grayscale(train_features)test_features = normalize_grayscale(test_features) æ ‡ç­¾äºŒå€¼åŒ–LabelBinarizer()æ˜¯sklearn.preprocessionä¸­ç”¨æ¥å°†éæ•°å€¼ç±»æ ‡ç­¾è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç å‘é‡çš„å‡½æ•°ã€‚ 123456789# Create the encoder åˆ›å»ºç¼–ç å™¨encoder = LabelBinarizer()# ç¼–ç å™¨æ‰¾åˆ°ç±»åˆ«å¹¶åˆ†é… one-hot å‘é‡encoder.fit(train_labels)#æœ€åæŠŠç›®æ ‡ï¼ˆlablesï¼‰è½¬æ¢æˆç‹¬çƒ­ç¼–ç çš„ï¼ˆone-hot encodedï¼‰å‘é‡train_labels = encoder.transform(train_labels)test_labels = encoder.transform(test_labels) è½¬æ¢æ•°æ®ç±»å‹ï¼Œè¿™æ ·åé¢å…¬å¼ä¸­æ‰å¯ä»¥è¿›è¡Œè¿ç®—ã€‚ 12train_labels = train_labels.astype(np.float32)test_labels = test_labels.astype(np.float32) éšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¸¸è§å½¢å¼ä¸ºï¼šX_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0) å‚æ•°è§£é‡Šï¼š train_dataï¼šæ‰€è¦åˆ’åˆ†çš„æ ·æœ¬ç‰¹å¾é›† train_targetï¼šæ‰€è¦åˆ’åˆ†çš„æ ·æœ¬ç»“æœ test_sizeï¼šæ ·æœ¬å æ¯”ï¼Œå¦‚æœæ˜¯æ•´æ•°çš„è¯å°±æ˜¯æ ·æœ¬çš„æ•°é‡ random_stateï¼šæ˜¯éšæœºæ•°çš„ç§å­ã€‚ éšæœºæ•°ç§å­ï¼šå…¶å®å°±æ˜¯è¯¥ç»„éšæœºæ•°çš„ç¼–å·ï¼Œåœ¨éœ€è¦é‡å¤è¯•éªŒçš„æ—¶å€™ï¼Œä¿è¯å¾—åˆ°ä¸€ç»„ä¸€æ ·çš„éšæœºæ•°ã€‚æ¯”å¦‚ä½ æ¯æ¬¡éƒ½å¡«1ï¼Œå…¶ä»–å‚æ•°ä¸€æ ·çš„æƒ…å†µä¸‹ä½ å¾—åˆ°çš„éšæœºæ•°ç»„æ˜¯ä¸€æ ·çš„ã€‚ä½†å¡«0æˆ–ä¸å¡«ï¼Œæ¯æ¬¡éƒ½ä¼šä¸ä¸€æ ·ã€‚ 123456# Get randomized datasets for training and validationtrain_features, valid_features, train_labels, valid_labels = train_test_split( train_features, train_labels, test_size=0.05, random_state=832289) æ‰“åŒ…æ•°æ®æ–¹ä¾¿ä¸‹æ¬¡å–ç”¨åºåˆ—åŒ–çš„æ–¹æ³•ä¸º pickle.dump()ï¼Œè¯¥æ–¹æ³•çš„ç›¸å…³å‚æ•°å¦‚ä¸‹ï¼špickle.dump(obj, file, protocol=None,*,fix_imports=True) 1234567891011121314151617181920# æ–°å»ºpickle_file# å‚æ•°fileå¿…é¡»æ˜¯ä»¥äºŒè¿›åˆ¶çš„å½¢å¼è¿›è¡Œæ“ä½œ,å³ã€Œwbã€pickle_file = 'notMNIST.pickle'if not os.path.isfile(pickle_file): print('Saving data to pickle file...') try: with open('notMNIST.pickle', 'wb') as pfile: pickle.dump( { 'train_dataset': train_features, 'train_labels': train_labels, 'valid_dataset': valid_features, 'valid_labels': valid_labels, 'test_dataset': test_features, 'test_labels': test_labels, }, pfile, pickle.HIGHEST_PROTOCOL) except Exception as e: print('Unable to save data to', pickle_file, ':', e) raise P2:ä»é¢„å¤„ç†å¥½çš„pickleä¸­è¯»å–æ•°æ®12345678910111213141516171819202122%matplotlib inline# Load the modulesimport pickleimport mathimport numpy as npimport tensorflow as tffrom tqdm import tqdmimport matplotlib.pyplot as plt# Reload the datapickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: pickle_data = pickle.load(f) train_features = pickle_data['train_dataset'] train_labels = pickle_data['train_labels'] valid_features = pickle_data['valid_dataset'] valid_labels = pickle_data['valid_labels'] test_features = pickle_data['test_dataset'] test_labels = pickle_data['test_labels'] del pickle_data # Free up memory C:\\Users\\10677\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters ä½¿ç”¨TFåˆ›å»ºå•å±‚ç¥ç»ç½‘ç»œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨TensorFlowåˆ›å»ºä¸€ä¸ªåªæœ‰ä¸€ä¸ªè¾“å…¥å±‚å’Œè¾“å‡ºå±‚çš„ç¥ç»ç½‘ç»œï¼Œæ¿€æ´»å‡½æ•°ä¸ºsoftmaxã€‚åœ¨TensorFlowä¸­ï¼Œæ•°æ®ä¸æ˜¯ä»¥æ•´æ•°ã€æµ®ç‚¹æ•°æˆ–å­—ç¬¦ä¸²çš„å½¢å¼å­˜å‚¨çš„ï¼Œè€Œæ˜¯ä»¥tensorå¯¹è±¡çš„å½¢å¼è¢«å­˜å‚¨çš„ã€‚ åœ¨tensorä¸­ä¼ é€’å€¼æœ‰ä¸¤ç§æ–¹æ³•ï¼š ä½¿ç”¨tf.constant()ï¼Œä¼ å…¥å˜é‡ï¼Œä½†æ˜¯ä¼ å…¥ä¹‹åå°±ä¸å¯å˜äº† å¦‚æœè¦ä½¿æ•°æ®å¯å˜ï¼Œç»“åˆtf.placeholder()å’Œtf.feed_dictæ¥è¾“å…¥ 1234567891011121314151617181920212223242526272829303132333435363738394041# All the pixels in the image (28 * 28 = 784)features_count = 784# All the labels (\"A,B...J\")labels_count = 10features = tf.placeholder(tf.float32)labels = tf.placeholder(tf.float32)# Set the weights and biases tensors# tf.truncated_normal:ç”Ÿæˆæ­£æ€åˆ†å¸ƒçš„éšæœºå€¼# weightså·²ç»éšæœºåŒ–ï¼Œbiaseså°±ä¸å¿…éšæœºï¼Œç®€åŒ–ä¸º0å³å¯weights = tf.Variable(tf.truncated_normal((features_count,labels_count)))biases = tf.Variable(tf.zeros(labels_count))# Feed dicts for training, validation, and test sessiontrain_feed_dict = {features: train_features, labels: train_labels}valid_feed_dict = {features: valid_features, labels: valid_labels}test_feed_dict = {features: test_features, labels: test_labels}# Linear Function WX + blogits = tf.matmul(features, weights) + biasesprediction = tf.nn.softmax(logits)# Cross entropycross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)# Training lossloss = tf.reduce_mean(cross_entropy)# Create an operation that initializes all variablesinit = tf.global_variables_initializer()# Test Caseswith tf.Session() as session: session.run(init) session.run(loss, feed_dict=train_feed_dict) session.run(loss, feed_dict=valid_feed_dict) session.run(loss, feed_dict=test_feed_dict) biases_data = session.run(biases) 12is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32)) P3:è®­ç»ƒç¥ç»ç½‘ç»œ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Change if you have memory restrictionsbatch_size = 128# Find the best parameters for each configurationepochs = 4learning_rate = 0.2# Gradient Descent# ä½¿ç”¨æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # The accuracy measured against the validation setvalidation_accuracy = 0.0# Measurements use for graphing loss and accuracylog_batch_step = 50batches = []loss_batch = []train_acc_batch = []valid_acc_batch = []with tf.Session() as session: session.run(init) batch_count = int(math.ceil(len(train_features)/batch_size)) for epoch_i in range(epochs): # Progress bar batches_pbar = tqdm(range(batch_count), desc='Epoch {:&gt;2}/{}'.format(epoch_i+1, epochs), unit='batches') # The training cycle for batch_i in batches_pbar: # Get a batch of training features and labels batch_start = batch_i*batch_size batch_features = train_features[batch_start:batch_start + batch_size] batch_labels = train_labels[batch_start:batch_start + batch_size] # Run optimizer and get loss _, l = session.run( [optimizer, loss], feed_dict={features: batch_features, labels: batch_labels}) # Log every 50 batches if not batch_i % log_batch_step: # Calculate Training and Validation accuracy training_accuracy = session.run(accuracy, feed_dict=train_feed_dict) validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict) # Log batches previous_batch = batches[-1] if batches else 0 batches.append(log_batch_step + previous_batch) loss_batch.append(l) train_acc_batch.append(training_accuracy) valid_acc_batch.append(validation_accuracy) # Check accuracy against Validation data validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)loss_plot = plt.subplot(211)loss_plot.set_title('Loss')loss_plot.plot(batches, loss_batch, 'g')loss_plot.set_xlim([batches[0], batches[-1]])acc_plot = plt.subplot(212)acc_plot.set_title('Accuracy')acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')acc_plot.set_ylim([0, 1.0])acc_plot.set_xlim([batches[0], batches[-1]])acc_plot.legend(loc=4)plt.tight_layout()plt.show()print('Validation accuracy at {}'.format(validation_accuracy)) Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:11&lt;00:00, 101.27batches/s] Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:10&lt;00:00, 101.99batches/s] Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:10&lt;00:00, 101.38batches/s] Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:12&lt;00:00, 92.55batches/s] Validation accuracy at 0.7662666440010071 P4:æ£€æµ‹12345678910111213141516171819202122232425262728test_accuracy = 0.0with tf.Session() as session: session.run(init) batch_count = int(math.ceil(len(train_features)/batch_size)) for epoch_i in range(epochs): # Progress bar batches_pbar = tqdm(range(batch_count), desc='Epoch {:&gt;2}/{}'.format(epoch_i+1, epochs), unit='batches') # The training cycle for batch_i in batches_pbar: # Get a batch of training features and labels batch_start = batch_i*batch_size batch_features = train_features[batch_start:batch_start + batch_size] batch_labels = train_labels[batch_start:batch_start + batch_size] # Run optimizer _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels}) # Check accuracy against Test data test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)assert test_accuracy &gt;= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)print('Nice Job! Test Accuracy is {}'.format(test_accuracy)) Epoch 1/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 588.57batches/s] Epoch 2/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 634.64batches/s] Epoch 3/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 633.74batches/s] Epoch 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1114/1114 [00:01&lt;00:00, 638.60batches/s] Nice Job! Test Accuracy is 0.8468999862670898 å‚è€ƒé“¾æ¥ï¼š python tqdmæ¨¡å—åˆ†æ Sklearn-train_test_splitéšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† numpy_ndarray.flatten sklearn.LabelBinarizer","link":"/2018/09/03/tfå°è¯•ç‰›åˆ€/"},{"title":"Slot Filling with SimpleRNN","text":"ä»€ä¹ˆæ˜¯Slot Fillingï¼ŸSlot Fillingæ˜¯è‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œæ˜¯å¯¹è¯­è¨€å«ä¹‰çš„ç®€å•åŒ–å¤„ç†ï¼Œå®ƒçš„æ€æƒ³ç±»ä¼¼äºè¯­è¨€å­¦ä¸­æ¡†æ¶ä¸»ä¹‰çš„ä¸€æ´¾ï¼Œå…ˆè®¾å®šå¥½ç‰¹å®šçš„è¯­è¨€ç±»å‹æ§½ï¼Œå†å°†è¾“å…¥çš„å•è¯ä¸€ä¸€å¡«å…¥æ§½å†…ï¼Œè€Œè·å–è¨€è¯­å«ä¹‰çš„æ—¶å€™å³æ˜¯æ ¹æ®è¯­ä¹‰æ§½çš„å«ä¹‰è¿›è¡Œæå–å’Œæ£€ç´¢ã€‚æˆ‘ä»¬è¿™é‡Œçš„ä»»åŠ¡å°±æ˜¯å°†è¡¨ç¤ºå®šè´­èˆªç­ï¼ˆATISæ•°æ®é›†ï¼‰è¿™ä¸€è¨€è¯­è¡Œä¸ºçš„ä¸€ç³»åˆ—è¯­å¥å¡«å…¥å„ç§ç±»å‹çš„è¯­ä¹‰æ§½ä¸­ã€‚ ä¸ºä»€ä¹ˆä½¿ç”¨SimpleRNN?Slot Fillingå±äºRNNåº”ç”¨ä¸­ä¸€å¯¹ä¸€çš„åº”ç”¨ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ï¼Œæ¯ä¸ªè¯éƒ½èƒ½è¢«å¡«åˆ°åˆé€‚çš„æ§½ä¸­ã€‚RNNå’Œä¸€èˆ¬çš„ç¥ç»ç½‘ç»œçš„ä¸åŒåœ¨äºï¼Œåœ¨RNNä¸­ï¼Œæˆ‘ä»¬åœ¨æ—¶é—´tçš„è¾“å‡ºä¸ä»…å–å†³äºå½“å‰çš„è¾“å…¥å’Œæƒé‡ï¼Œè¿˜å–å†³äºä¹‹å‰çš„è¾“å…¥ï¼Œè€Œå¯¹äºå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ¯ä¸ªæ—¶åˆ»çš„è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯ç‹¬ç«‹è€Œéšæœºçš„ï¼Œæ²¡æœ‰ç›¸å…³æ€§ã€‚æ”¾åˆ°æˆ‘ä»¬è¦å¤„ç†è¯­ä¹‰ç†è§£çš„é—®é¢˜ä¸Šçœ‹ï¼Œè¯­è¨€ä½œä¸ºä¸€ç§åŸºäºæ—¶é—´çš„çº¿æ€§è¾“å‡ºï¼Œæ˜¾ç„¶ä¼šå—åˆ°å‰è¯çš„å½±å“ï¼Œå› æ­¤æˆ‘ä»¬é€‰å–RNNæ¨¡å‹æ¥è¿›è¡Œè§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™é‡Œé€‰å–SimpleRNN,æ˜¯å› ä¸ºè¿™ä¸ªRNNæ¯”è¾ƒç®€å•ï¼Œèƒ½è¾¾åˆ°ç†Ÿæ‚‰æ¡†æ¶çš„ç»ƒä¹ æ•ˆæœï¼Œä¹‹åå¯ä»¥é€‰å–å…¶ä»–æœ‰æ•ˆçš„RNNæ¨¡å‹ï¼Œå¦‚LSTMSè¿›è¡Œä¼˜åŒ–ã€‚ æ„å»ºæ€è·¯ä¸€è§ˆï¼š è½½å…¥æ•°æ®ï¼Œä½¿ç”¨çš„æ˜¯chsasankä¿®æ”¹çš„mesnilgrçš„load.pyã€‚ å®šä¹‰æ¨¡å‹ã€‚é‡‡å–Kerasä¸­çš„åºåˆ—æ¨¡å‹æ­å»ºï¼Œé¦–å…ˆä½¿ç”¨ä¸€ä¸ª100ç»´çš„word embeddingå±‚å°†è¾“å…¥çš„å•è¯è½¬åŒ–ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªå‘é‡ï¼ˆåœ¨è¿™ä¸ªç©ºé—´ä¸­ï¼Œè¯­ä¹‰å’Œè¯­æ³•ä½ç½®è¶Šè¿‘çš„å•è¯çš„è·ç¦»è¶Šå°ï¼‰ï¼Œç„¶åæˆ‘ä»¬æ„å»ºä¸€ä¸ªdropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè®¾ç½®SimpleRNNå±‚ï¼Œè®¾ç½®TimeDistributedå±‚ä»¥å®ŒæˆåŸºäºæ—¶é—´çš„åå‘ä¼ æ’­ã€‚æœ€åæˆ‘ä»¬å°†è¿™äº›å±‚ç»„ç»‡åœ¨ä¸€èµ·ï¼Œå¹¶ç¡®å®šoptimizerå’Œloss functionã€‚æˆ‘ä»¬é€‰å–çš„optimizeræ˜¯rmsprop,è¿™æ ·åœ¨è®­ç»ƒåæœŸä¾ç„¶èƒ½æ‰¾åˆ°è¾ƒæœ‰é¡¹ï¼Œè€Œé€‰å–categorical_crossentropyä½œä¸ºæŸå¤±å‡½æ•°ï¼Œåˆ™æ˜¯å› ä¸ºå¤„ç†çš„é—®é¢˜æ€§è´¨é€‚åˆäºæ­¤ã€‚ è®­ç»ƒæ¨¡å‹ã€‚å‡ºäºå¯¹è®¡ç®—èµ„æºçš„è€ƒè™‘ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨minibtachçš„æ–¹æ³•æ‰¹é‡å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚ä½†æ˜¯æˆ‘ä»¬è¿™é‡Œçš„æ•°æ®æ˜¯ä¸€å¥å¥è¯ï¼Œå¦‚æœæŒ‰ç…§ä¸€ä¸ªå›ºå®šçš„batch_sizeå°†å…¶åˆ†è£‚ï¼Œå¯èƒ½å¢åŠ äº†ä¸å¿…è¦çš„è”ç³»ï¼ˆå› ä¸ºä¸Šä¸‹ä¸¤å¥è¯æ˜¯ç‹¬ç«‹çš„ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸€å¥è¯ä½œä¸ºä¸€ä¸ªbatchå»è¿›è¡Œè®­ç»ƒã€éªŒè¯ä»¥åŠé¢„æµ‹ï¼Œå¹¶æ‰‹åŠ¨ç®—å‡ºä¸€ä¸ªepochçš„å¹³å‡è¯¯å·®ã€‚ è¯„ä¼°å’Œé¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡è§‚å¯ŸéªŒè¯è¯¯å·®å’Œé¢„æµ‹F1ç²¾åº¦æ¥å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚é¢„æµ‹F1ç²¾åº¦ä½¿ç”¨çš„æ˜¯signsmileç¼–å†™çš„conlleval.pyã€‚ ä¿å­˜æ¨¡å‹ã€‚ 123456789101112import numpy as npimport picklefrom keras.models import Sequentialfrom keras.layers.embeddings import Embeddingfrom keras.layers.recurrent import SimpleRNNfrom keras.layers.core import Dense,Dropoutfrom keras.utils import to_categoricalfrom keras.layers.wrappers import TimeDistributedfrom matplotlib import pyplot as pltimport data.loadfrom metrics.accuracy import evaluate Using TensorFlow backend. Load Data123456train_set,valid_set,dicts = data.load.atisfull()# print(train_set[:1])# dicts = {'label2idx':{},'words2idx':{},'table2idx':{}}w2idx,labels2idx = dicts['words2idx'],dicts['labels2idx']train_x,_,train_label = train_setval_x,_,val_label = valid_set 12idx2w = {w2idx[i]:i for i in w2idx}idx2lab = {labels2idx[i]:i for i in labels2idx} 12n_classes = len(idx2lab)n_vocab = len(idx2w) 123456789101112131415words_train = [[idx2w[i] for i in w[:]] for w in train_x]labels_train = [[idx2lab[i] for i in w[:]] for w in train_label]words_val = [[idx2w[i] for i in w[:]] for w in val_x]# labels_val = [[idx2lab[i] for i in w[:]] for w in val_label]labels_val =[]for w in val_label: for i in w[:]: labels_val.append(idx2lab[i])print('Real Sentence : {}'.format(words_train[0]))print('Encoded Form : {}'.format(train_x[0]))print('='*40)print('Real Label : {}'.format(labels_train[0]))print('Encoded Form : {}'.format(train_label[0])) Real Sentence : [&apos;i&apos;, &apos;want&apos;, &apos;to&apos;, &apos;fly&apos;, &apos;from&apos;, &apos;boston&apos;, &apos;at&apos;, &apos;DIGITDIGITDIGIT&apos;, &apos;am&apos;, &apos;and&apos;, &apos;arrive&apos;, &apos;in&apos;, &apos;denver&apos;, &apos;at&apos;, &apos;DIGITDIGITDIGITDIGIT&apos;, &apos;in&apos;, &apos;the&apos;, &apos;morning&apos;] Encoded Form : [232 542 502 196 208 77 62 10 35 40 58 234 137 62 11 234 481 321] ======================================== Real Label : [&apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;B-fromloc.city_name&apos;, &apos;O&apos;, &apos;B-depart_time.time&apos;, &apos;I-depart_time.time&apos;, &apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;B-toloc.city_name&apos;, &apos;O&apos;, &apos;B-arrive_time.time&apos;, &apos;O&apos;, &apos;O&apos;, &apos;B-arrive_time.period_of_day&apos;] Encoded Form : [126 126 126 126 126 48 126 35 99 126 126 126 78 126 14 126 126 12] Define and Compile the model1234567model = Sequential()model.add(Embedding(n_vocab,100))model.add(Dropout(0.25))model.add(SimpleRNN(100,return_sequences=True))model.add(TimeDistributed(Dense(n_classes,activation='softmax')))model.compile(optimizer = 'rmsprop',loss = 'categorical_crossentropy')model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, None, 100) 57200 _________________________________________________________________ dropout_1 (Dropout) (None, None, 100) 0 _________________________________________________________________ simple_rnn_1 (SimpleRNN) (None, None, 100) 20100 _________________________________________________________________ time_distributed_1 (TimeDist (None, None, 127) 12827 ================================================================= Total params: 90,127 Trainable params: 90,127 Non-trainable params: 0 _________________________________________________________________ Train the model1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def train_the_model(n_epochs,train_x,train_label,val_x,val_label): epoch,train_avgloss,val_avgloss,f1s = [],[],[],[] for i in range(1,n_epochs+1): epoch.append(i) ## training train_avg_loss =0 for n_batch,sent in enumerate(train_x): label = train_label[n_batch] # label to one-hot label = to_categorical(label,num_classes=n_classes)[np.newaxis,:] sent = sent[np.newaxis,:] loss = model.train_on_batch(sent,label) train_avg_loss += loss train_avg_loss = train_avg_loss/n_batch train_avgloss.append(train_avg_loss) ## evaluate&amp;predict val_pred_label,pred_label_val,val_avg_loss = [],[],0 for n_batch,sent in enumerate(val_x): label = val_label[n_batch] label = to_categorical(label,num_classes=n_classes)[np.newaxis,:] sent = sent[np.newaxis,:] loss = model.test_on_batch(sent,label) val_avg_loss += loss pred = model.predict_on_batch(sent) pred = np.argmax(pred,-1)[0] val_pred_label.append(pred) val_avg_loss = val_avg_loss/n_batch val_avgloss.append(val_avg_loss) for w in val_pred_label: for k in w[:]: pred_label_val.append(idx2lab[k]) prec, rec, f1 = evaluate(labels_val,pred_label_val, verbose=False) print('Training epoch {}\\t train_avg_loss = {} \\t val_avg_loss = {}'.format(i,train_avg_loss,val_avg_loss)) print('precision: {:.2f}% \\t recall: {:.2f}% \\t f1 :{:.2f}%'.format(prec,rec,f1)) print('-'*60) f1s.append(f1) # return epoch,pred_label_train,train_avgloss,pred_label_val,val_avgloss return epoch,f1s,val_avgloss,train_avgloss 1epoch,f1s,val_avgloss,train_avgloss = train_the_model(40,train_x,train_label,val_x,val_label) è¾“å‡ºï¼š1234567891011121314 Training epoch 1 train_avg_loss = 0.5546463992293973 val_avg_loss = 0.4345020865901363 precision: 84.79% recall: 80.79% f1 :82.74% ------------------------------------------------------------ Training epoch 2 train_avg_loss = 0.2575569036037627 val_avg_loss = 0.36228470020366654 precision: 86.64% recall: 83.86% f1 :85.22% ------------------------------------------------------------ Training epoch 3 train_avg_loss = 0.2238766908014994 val_avg_loss = 0.33974187403771694 precision: 88.03% recall: 85.55% f1 :86.77% ------------------------------------------------------------â€¦â€¦ ------------------------------------------------------------ Training epoch 40 train_avg_loss = 0.09190682124901069 val_avg_loss = 0.2697056618613356 precision: 92.51% recall: 91.47% f1 :91.99% ------------------------------------------------------------ å¯è§†åŒ–è§‚å¯ŸéªŒè¯è¯¯å·®ï¼Œé€‰å–åˆé€‚çš„epochã€‚ 123456%matplotlib inlineplt.xlabel=('epoch')plt.ylabel=('loss')plt.plot(epoch,train_avgloss,'b')plt.plot(epoch,val_avgloss,'r',label=('validation error'))plt.show() 1print('æœ€å¤§f1å€¼ä¸º {:.2f}%'.format(max(f1s))) æœ€å¤§f1å€¼ä¸º 92.56% ä¿å­˜æ¨¡å‹1model.save('slot_filling_with_simpleRNN.h5') ç»“æœåˆ†æä½¿ç”¨SimpleRNNæœ€ç»ˆå¾—åˆ°çš„F1å€¼ä¸º92.56%ï¼Œå’Œå¸ˆå…„çš„95.47%ç›¸æ¯”ç¡®å®è¿˜ç›¸å·®å¾ˆå¤šã€‚è¿™ä¸»è¦æ˜¯å’Œæˆ‘ä»¬æ¨¡å‹çš„é€‰å–æœ‰å…³ï¼ŒSimpleRNNåªèƒ½å°†å‰è¯çš„å½±å“å¸¦å…¥åˆ°æ¨¡å‹ä¸­ï¼Œä½†æ˜¯è¯­è¨€ä¸­åè¯å¯¹å‰è¯ä¹Ÿä¼šæœ‰ä¸€å®šçš„å½±å“ï¼Œå› æ­¤å¯ä»¥é€šè¿‡é€‰æ‹©æ›´åŠ å¤æ‚çš„æ¨¡å‹æˆ–è€…å¢åŠ èƒ½å¤Ÿæ•æ‰åˆ°åè¯ä¿¡æ¯çš„å±‚æ¥è¿›è¡Œä¼˜åŒ–ã€‚ å‚è€ƒèµ„æ–™ Keras Tutorial - Spoken Language Understanding pytorch-slot-filling liu946 AtisSlotLabeling ã€Kerasæƒ…æ„Ÿåˆ†ç±»ã€‘è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°çš„é—®é¢˜æ±‡æ€» keras-SimpleRNN æœºå™¨å­¦ä¹ ä¸­è¿‡æ‹Ÿåˆçš„è§£å†³åŠæ³•","link":"/2018/09/10/slot-filling/"},{"title":"åŒ—å¤§åˆ†è¯æ–¹æ¡ˆè§£è¯»åŠé¢—ç²’åº¦åˆ†è¯æ–¹æ¡ˆ","text":"ä¸€ã€è°ƒç ”èµ„æ–™ åŒ—å¤§ç°ä»£æ±‰è¯­è¯­æ–™åº“åŸºæœ¬åŠ å·¥è§„èŒƒ è®¡ç®—æ‰€æ±‰è¯­è¯æ€§æ ‡æ³¨é›† å‡ ä¸ªå¼€æºåˆ†è¯ç³»ç»Ÿæ‰€ä½¿ç”¨æ ‡æ³¨é›†çš„æ¥æº æµ·é‡ä¸­æ–‡æ™ºèƒ½åˆ†è¯æ¥å£æ‰‹å†Œ é˜¿é‡Œå¤šç²’åº¦åˆ†è¯ä¸“åˆ© è…¾è®¯å¤šç²’åº¦åˆ†è¯ä¸“åˆ© ç™¾åº¦å¤šç²’åº¦åˆ†è¯ä¸“åˆ© KTDictSeg åˆ†è¯ç»„ä»¶1.3ç‰ˆæœ¬ éƒ¨åˆ†ç®—æ³•è®¨è®º â€“ åˆ†è¯ç²’åº¦ äºŒã€è°ƒç ”ç›®çš„åˆ†è¯å•ä½ä¸åŒäºè¯­è¨€å­¦ä¸­çš„â€œè¯â€ï¼Œä¸åŒçš„ç®—æ³•ä¸‹çš„åˆ†è¯ç»“æœåƒå·®ä¸‡åˆ«ï¼Œæœ‰çš„åˆ†å‡ºçš„æ˜¯è¯­è¨€å­¦æ„ä¹‰ä¸Šçš„è¯ï¼Œè€Œæœ‰çš„åˆ†å‡ºçš„æ˜¯è¯­è¨€å­¦æ„ä¹‰ä¸Šçš„â€œçŸ­è¯­â€ï¼ˆæˆ–è€…è¯´â€œè¯ç»„â€ï¼‰å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å¯»æ‰¾ä¸€ä¸ªå¯ç†è§£çš„ç»Ÿä¸€çš„ç²’åº¦æ ‡å‡†ï¼Œè€Œè¿™ä¸ªç²’åº¦æ ‡å‡†èƒ½å¤Ÿå®ç°å¯¹ä¸åŒåˆ†è¯ä»»åŠ¡çš„ä¸åŒå±‚æ¬¡çš„åˆ†è¯ã€‚ä¸ºè¯å®å¤šé¢—ç²’åº¦çš„åˆ†è¯æ ‡æ³¨ç¡®å®èƒ½æé«˜ç‰¹å®šçš„åˆ†è¯ä»»åŠ¡çš„å‡†ç¡®ç‡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¿™æ ·çš„å‰æœŸè°ƒç ”ã€‚é€šè¿‡æœé›†èµ„æ–™ï¼Œæˆ‘ä»¬ä»¥åŒ—å¤§æ–¹æ¡ˆä¸ºè“æœ¬ï¼Œä»¥ä¸€å®šçš„è¯­è¨€å­¦çŸ¥è¯†ä¸ºåŸºç¡€ï¼Œå¯¹åˆ†è¯é¢—ç²’è¿›è¡Œä¸åŒç²’åº¦çš„åˆ’åˆ†ã€‚é¦–å…ˆå¯¹åŒ—å¤§åˆ†è¯æ–¹æ¡ˆè¿›è¡Œè§£è¯»ï¼Œç„¶åå†é˜é‡Šæˆ‘å¯¹åˆ†è¯ç²’åº¦åˆæ­¥çš„æ„å»ºæƒ³æ³•ã€‚ æ³¨ï¼šé¢—ç²’åº¦æ–¹æ¡ˆåªè€ƒè™‘åˆ†è¯é—®é¢˜ï¼Œä¸è€ƒè™‘è¯æ€§æ ‡æ³¨ã€‚ ä¸‰ã€åŒ—å¤§åˆ†è¯æ–¹æ¡ˆè®²è§£1. åˆ†è¯å•ä½çš„æ¦‚å¿µç•Œå®šåˆ†è¯å•ä½ï¼Œâ€œæŒ‡ä¿¡æ¯å¤„ç†ä¸­ä½¿ç”¨çš„ã€å…·æœ‰ç¡®å®šçš„è¯­ä¹‰å’Œè¯­æ³•åŠŸèƒ½çš„åŸºæœ¬å•ä½â€ï¼Œè¯¥æ¦‚å¿µæ˜ç¡®äº†å…¶ä½¿ç”¨çš„ç‰¹å®šç¯å¢ƒâ€”â€”â€œä¿¡æ¯å¤„ç†ä»»åŠ¡â€ï¼Œä»¥åŠå…¶è¯­ä¹‰å’Œè¯­æ³•åŠŸèƒ½æ˜ç¡®çš„ç‰¹ç‚¹ã€‚ åŸºäºè¿™æ ·çš„æ¦‚å¿µåˆ’åˆ†ï¼ŒåŒ—å¤§æ–¹æ¡ˆè®¤å®šçš„åˆ†è¯å•ä½é‡Œä¸ä»…åŒ…æ‹¬äº†è¯ï¼Œè¿˜â€œåŒ…æ‹¬äº†ä¸€éƒ¨åˆ†ç»“åˆç´§å¯†ã€ä½¿ç”¨ç¨³å®šçš„è¯ç»„â€ï¼Œå¹¶ä¸”â€œåœ¨æŸäº›ç‰¹æ®Šæƒ…å†µå­¤ç«‹çš„è¯­ç´ æˆ–éè¯­ç´ å­—â€ã€‚ äº‹å®ä¸Šï¼Œæˆ‘ä»¬æ’‡å¼€åŒ—å¤§æ–¹æ¡ˆæ¥çœ‹è¯è¿™ä¸ªæ•´ä½“ï¼Œæ ¹æ®æœ±å¾·ç†™å…ˆç”Ÿçš„åˆ’åˆ†ï¼Œå¯ä»¥åˆ†ä¸ºå¯ç©·å°½çš„è™šè¯ç±»å’Œä¸å¯ç©·å°½çš„å®è¯ç±»ã€‚è™šè¯ç±»ï¼Œä¸¾ä¾‹æ¥è¯´ï¼ŒåŒ…æ‹¬è¿è¯ã€è¯­æ°”è¯ã€ä»‹è¯ç­‰ï¼Œè¿™ç±»è¯å¯ä»¥åœ¨è¯­æ³•è¯å…¸ä¸­è¢«æšä¸¾å‡ºæ¥ï¼Œå› æ­¤åœ¨è¿›è¡Œåˆ†è¯æ—¶éš¾åº¦è¾ƒå°ã€‚å› æ­¤ï¼Œåˆ†è¯çš„å›°éš¾å¸¸å¸¸å‡ºç°åœ¨å®è¯çš„åˆ‡åˆ†ä¸Šã€‚ ç»“åˆåŒ—å¤§æ–¹æ¡ˆçš„åˆ’åˆ†ï¼Œæˆ‘è®¤ä¸ºå¯¹å®è¯åºåˆ—è¿›è¡Œåˆ’åˆ†æ—¶ï¼Œä¸€èˆ¬å¯ä»¥éµç…§ä»¥ä¸‹åŸåˆ™ï¼š ï¼ˆ1ï¼‰ä¾æ®è¯­æ³•è¯å…¸æ¥åˆ’åˆ†ï¼Œå¦‚æœè¯­æ³•è¯å…¸ä¸­è¿›è¡Œè§„å®šï¼Œé‚£ä¹ˆå°±ä¸åšåˆ’åˆ†ã€‚è¯­è¨€æ˜¯çº¦å®šä¿—æˆçš„äº§ç‰©ï¼Œå½“æŸä¸ªè¯è¯­ç»„åˆè¢«å¹¿æ³›è€Œç¨³å®šåœ°ä½¿ç”¨æ—¶ï¼Œé‚£ä¹ˆç¤¾ä¼šå›¢ä½“ä¾¿ä¼šæ¥å—è¿™æ ·çš„ä¸€ä¸ªâ€œæ–°è¯â€ï¼Œå› æ­¤è¿™æ ·çš„ä¸€ä¸ªè¯è¯­ç»„åˆä¹Ÿå¯ä»¥è¢«è§†ä½œæ˜¯ä¸€ä¸ªåˆ†è¯å•ä½ã€‚è€Œåˆ¤æ–­ç¤¾ä¼šå›¢ä½“æ˜¯å¦å·²ç»æ¥å—è¿™ä¸€è¯­è¨€ç°è±¡å¾ˆæ˜¾æ€§çš„ä¸€å¤§æ ‡å¿—ä¾¿æ˜¯è¯å…¸æ”¶å½•äº†è¯¥è¯æ¡ã€‚é‚£ä¹ˆé—®é¢˜å°±è½¬å˜ä¸ºï¼Œä»€ä¹ˆæ ·çš„è¯å…¸å¯ä»¥æˆä¸ºå¯ä¾›åˆ’åˆ†çš„è¯­æ³•è¯å…¸ã€‚ ï¼ˆ2ï¼‰è€ƒè™‘åˆ‡åˆ†åºåˆ—çš„éŸ³èŠ‚ç»„åˆã€‚æ±‰è¯­åœ¨å‘å±•è¿‡ç¨‹ä¸­ç»å†äº†ä¸€ä¸ªä»å•éŸ³èŠ‚å‘åŒéŸ³èŠ‚çš„å‘å±•è¿‡ç¨‹ã€‚è™½ç„¶ç°ä»£æ±‰è¯­ä»¥åŒéŸ³èŠ‚ä¸ºä¸»è¦çš„æˆè¯å•ä½ï¼Œä½†æ˜¯å¤ä»£æ±‰è¯­ä¸­çš„ä¸€äº›å•éŸ³èŠ‚è¯ä¾ç„¶æ®‹å­˜åœ¨ç°ä»£æ±‰è¯­ä¸­ï¼Œå¹¶ä¸”åœ¨ä¸€äº›ç‰¹æ®Šè¯­ä½“ä¸­è¿˜å¹¿æ³›åœ°å­˜åœ¨ç€ã€‚å› æ­¤ï¼Œå¯¹äºé‚£äº›å•éŸ³èŠ‚æˆè¯çš„å•ä½åœ¨æ ‡æ³¨æ—¶è¦æ ¼å¤–æ³¨æ„æ ‡è®°å‡ºæ¥ï¼Œè€Œå¤„ç†å¤šéŸ³èŠ‚åºåˆ—æ—¶ï¼Œåˆ™è¦å°½é‡ä¿è¯åˆ†è¯ç»“æœä»¥åŒéŸ³èŠ‚ä¸ºä¸€ä¸ªå•ä½ã€‚ ï¼ˆ3ï¼‰è€ƒè™‘åˆ°è¯ä¹‰ä¸è¯­ç´ ç»“åˆä¹‰ã€‚æˆ‘ä»¬æ‰€è®¤å®šçš„åˆ†è¯å•ä½ï¼Œå®ƒçš„è¯ä¹‰æ˜¯å‡åˆè€Œæˆçš„ï¼Œè€Œä¸æ˜¯ä¸¤ä¸ªè¯­ç´ çš„æ„ä¹‰ç®€å•çš„ç›¸åŠ ã€‚å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªåˆ‡åˆ†å•ä½çš„è¯­ä¹‰æ˜¯å…¶åˆ‡åˆ†å•ä½æ„ä¹‰çš„ç®€å•ç›¸åŠ ï¼Œé‚£ä¹ˆå°±è¦å¯¹å…¶è¿›è¡Œåˆ‡åˆ†ã€‚è€Œåˆ¤å®šæ˜¯å¦æ˜¯è¯ä¹‰ç®€å•çš„ç›¸åŠ çš„æ–¹æ³•ä¸»è¦æœ‰â€œçš„â€æ’å…¥æ³•å’Œæ›¿æ¢æ³•ä¸¤ç§ï¼Œè¿™åœ¨åé¢å…·ä½“çš„è®²è§£ä¸­ä¼šè¿›è¡Œé˜é‡Šã€‚ ï¼ˆ4ï¼‰è¦è€ƒè™‘åˆ°åˆ‡åˆ†çš„ç»æµæ€§ã€‚åŒ—å¤§æ–¹æ¡ˆæ˜¯åˆ‡åˆ†å’Œæ ‡æ³¨åŒæ—¶è¿›è¡Œï¼Œä¸ºäº†ä¿è¯æ ‡æ³¨ç¬¦å·ä½¿ç”¨çš„ç»æµæ€§ï¼Œæ–¹æ¡ˆè¦æ±‚ï¼Œè¦ä¿è¯åˆ‡åˆ†å‡ºæ¥çš„å•ä½å°½é‡å°‘çš„æ˜¯æ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ã€‚å› æ­¤ï¼Œå¯¹äºä¸€ä¸ªåˆ‡åˆ†åºåˆ—ï¼Œå¦‚æœæˆ‘ä»¬åˆ‡åˆ†åå¤šå‡ºäº†æ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ï¼Œæ¯”å¦‚è¯´å‰æ¥æˆåˆ†ã€åæ¥æˆåˆ†ç­‰ï¼Œæˆ‘ä»¬å°½å¯èƒ½åœ°ä¸å»åˆ‡åˆ†å®ƒã€‚ 2.åˆ†è¯å®é™…æƒ…å†µä¸­çš„åº”ç”¨æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¯¹åˆ†è¯æ–¹æ¡ˆçš„ç¬¬å››ç« ã€ç¬¬äº”ç« ç»“åˆæˆ‘ä»¬æ€»ç»“å‡ºæ¥çš„è§„åˆ™è¿›è¡Œç²¾ç®€å¼çš„è¯´æ˜ã€‚ ï¼ˆ1ï¼‰äººå å¯¹äºäººåçš„åˆ‡åˆ†ï¼Œæ–¹æ¡ˆç»™å‡ºçš„åˆ‡åˆ†æ ‡å‡†æ˜¯å§“å’Œååˆ‡åˆ†å¼€ã€‚è€Œå¯¹äºå…¶ä»–ç§°å‘¼æ˜¯å¦åˆ‡åˆ†ï¼Œå¯ä»¥ç”¨è¯­ä¹‰è§„åˆ™æ¥è§£é‡Šã€‚ç¬¬äºŒæ¡è§„åˆ™ï¼šå§“ååçš„èŒåŠ¡ã€èŒç§°æˆ–ç§°å‘¼è¦åˆ†å¼€ã€‚ç¬¬å››æ¡è§„åˆ™ï¼šå¸¦æ˜æ˜¾æ’è¡Œçš„äº²å±ç§°è°“è¦åˆ‡åˆ†å¼€ã€‚è¿™ä¸¤æ¡è§„åˆ™æ˜¯å› ä¸ºç»„æˆçš„åˆ‡åˆ†åºåˆ—çš„æ„æ€å³æ˜¯å„ç»„æˆæˆåˆ†çš„ç»„åˆä¹‰ï¼Œå› æ­¤è¦åˆ‡åˆ†ã€‚è€Œç¬¬ä¸‰æ¡è§„åˆ™ï¼šå¯¹äººçš„ç®€ç§°ã€å°Šç§°è‹¥ä¸ºä¸¤ä¸ªå­—ï¼Œåˆ™åˆä¸ºä¸€ä¸ªåˆ‡åˆ†å•ä½ã€‚ä¸ä»…æ˜¯å› ä¸ºè¿™äº›åˆ‡åˆ†åºåˆ—çš„å«ä¹‰ä¸æ˜¯å…¶ç»„æˆæˆåˆ†çš„ç»„åˆä¹‰ï¼Œè‡³å°‘æœ‰è¡¨ç¤ºå°Šæ•¬çš„ç¤¾ä¼šå«ä¹‰ï¼Œè¿˜æ˜¯å› ä¸ºå¦‚æœåˆ‡åˆ†ï¼Œä¼šå¤šå‡ºæ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ï¼Œå› æ­¤æŠŠè¿™äº›åŒéŸ³èŠ‚ä½œä¸ºä¸€ä¸ªåˆ‡åˆ†å•ä½ã€‚è€Œå¯¹äºå¤–å›½äººåå’Œç¬”åã€è‘—åäººåï¼Œæˆ‘ä»¬ä¸åšåˆ‡åˆ†ï¼Œä¸€æ˜¯å› ä¸ºè¿™ç§å‘½åæ˜¯éšæ„çš„ï¼Œåˆ‡åˆ†ä¸‹æ¥çš„æ„ä¹‰ä¸å¤§ï¼›äºŒæ˜¯å› ä¸ºè‘—åäººåæ˜¯åœ¨è¯­æ³•è¯å…¸ä¸­å°±è§„å®šäº†çš„å†…å®¹ã€‚ ï¼ˆ2ï¼‰åœ°å å¤§éƒ¨åˆ†åœ°åéƒ½æ˜¯åœ¨è¯­æ³•è¯å…¸ä¸­äº‹å…ˆè§„å®šäº†çš„ï¼Œé™¤æ­¤ä»¥å¤–çš„åˆ‡åˆ†åŸåˆ™ä¸»è¦æ˜¯å’ŒéŸ³èŠ‚æœ‰å…³ï¼Œå¦‚æœåœ°ååæ¥çš„æ˜¯å•éŸ³èŠ‚è¯­ç´ ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼›å¦‚æœæ¥çš„æ˜¯åŒéŸ³èŠ‚æˆ–å¤šéŸ³èŠ‚è¯­ç´ ï¼Œåˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ3ï¼‰å›¢ä½“ã€æœºæ„ã€ç»„ç»‡çš„ä¸“æœ‰åç§° å¯¹äºå›¢ä½“ã€æœºæ„ã€ç»„ç»‡çš„ä¸“æœ‰åç§°ï¼Œå¦‚æœå®ƒä»¬è¢«è¯­æ³•è¯å…¸æ”¶å½•ï¼Œé‚£ä¹ˆè‚¯å®šä¸åˆ‡åˆ†ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ï¼ˆå¦‚æœæ‰¾ä¸åˆ°è¿™æ ·åˆé€‚çš„è¯å…¸ï¼Œä¸€ä¸ªPLAN Bçš„å»ºè®®ï¼šæŒ‰ç…§æ™®é€šè¯ç»„åˆ‡åˆ†ï¼Œå†ä¸Šæ¸¸ä»»åŠ¡ä¸­å†è¯†åˆ«å‡ºæ¥ï¼‰ ï¼ˆ4ï¼‰é™¤äººåã€å›½åã€åœ°åã€å›¢ä½“ã€æœºæ„ã€ç»„ç»‡ä»¥å¤–çš„å…¶ä»–ä¸“å é¦–å…ˆï¼Œæˆ‘ä»¬è¿˜æ˜¯è¦è€ƒè™‘å…¶æ˜¯å¦è¢«è¯­æ³•è¯å…¸æ”¶å½•ã€‚ç„¶åè¦è€ƒè™‘å…¶åæ¥è¯­ç´ çš„éŸ³èŠ‚ï¼Œå¦‚æœæ˜¯å•éŸ³èŠ‚çš„ï¼Œå¦‚â€œäººâ€â€œæ—â€è¿™æ ·çš„ï¼Œä¸åˆ‡åˆ†ï¼Œå¦‚æœæ˜¯å¤šéŸ³èŠ‚çš„ï¼Œåˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ5ï¼‰æ•°è¯ä¸æ•°é‡è¯ç»„ æ•°è¯ä¸æ•°é‡è¯ç»„çš„è§„å®šæ˜¯å¦å¤–çš„ã€‚è¯¦è§æ–¹æ¡ˆã€‚ ï¼ˆ6ï¼‰æ—¶é—´è¯ æ—¶é—´è¯ä¸­ç™»å½•åœ¨è¯­æ³•è¯å…¸ä¸­çš„ï¼Œæ¯”å¦‚å†å²æœä»£çš„åç§°ï¼Œç‰¹æ®Šçš„å¹´ä»½â€œç”²åˆå¹´â€ç­‰ï¼Œä¸åšåˆ‡åˆ†ï¼Œå…¶ä»–çš„è¦æŒ‰ç…§â€œå¹´ã€æœˆã€æ—¥ã€æ—¶ã€åˆ†ã€ç§’â€çš„å±‚æ¬¡è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ7ï¼‰å•éŸ³èŠ‚ä»£è¯â€œæœ¬â€ã€â€œæ¯â€ã€â€œå„â€ã€â€œè¯¸ è‹¥åæ¥æˆåˆ†æ˜¯å•éŸ³èŠ‚åè¯ï¼Œåˆ™ä¸åšåˆ‡åˆ†ï¼Œè‹¥æ˜¯åŒéŸ³èŠ‚æˆ–å¤šéŸ³èŠ‚ï¼Œåˆ™è¦åˆ‡åˆ†å¼€ã€‚ ï¼ˆ8ï¼‰åŒºåˆ«è¯ é¦–å…ˆï¼Œæˆ‘ä»¬è¦æ˜ç¡®ä½•ä¸ºåŒºåˆ«è¯ï¼ŒåŒºåˆ«è¯æŒ‡çš„æ˜¯æˆå¯¹çš„ï¼Œæœ‰åˆ†ç±»æ€§è´¨çš„ä¸€ç±»è¯ï¼Œå®ƒä»¬åªèƒ½å¤Ÿåšå®šè¯­ï¼Œä¸èƒ½åšè°“è¯­ï¼Œæ‰€ä»¥åˆç§°ä¸ºéè°“å½¢å®¹è¯ã€‚ ä¸¾ä¾‹æ¥è¯´ï¼ŒåŒºåˆ«è¯åŒ…æ‹¬ï¼šç”·ã€å¥³ã€é›Œã€é›„ã€å•ã€åŒã€å¤ã€é‡‘ã€é“¶ã€è¥¿å¼ã€ä¸­å¼ã€å¤ä»£ã€è¿‘ä»£ã€ç°ä»£ã€å½“ä»£ã€é˜´æ€§ã€é˜³æ€§ã€å†›ç”¨ã€æ°‘ç”¨ã€å›½æœ‰ã€ç§æœ‰ã€å°å‹ã€ä¸­å‹ã€å¤§å‹ã€å¾®å‹ã€æœ‰æœŸã€æ— æœŸã€å½©è‰²ã€é»‘ç™½ã€æ€¥æ€§ã€æ…¢æ€§ã€å°å·ã€ä¸­å·ã€å¤§å·ã€é‡ç”Ÿã€å®¶å…»ã€æ­£å¼ã€éæ­£å¼ã€äººé€ ï¼ˆä»åŠ¨è¯è¿‡æ¥çš„ï¼‰ã€å¤©ç„¶ã€å†’ç‰Œã€æ­£ç‰Œã€æ­£ç‰ˆã€ç›—ç‰ˆã€ä¸‹ç­‰ã€ä¸­ç­‰ã€ä¸Šç­‰ã€åˆçº§ã€ä¸­çº§ã€é«˜çº§ã€ä¸­å¼ã€æ¬§å¼ç­‰ç­‰ã€‚ å¯¹äºå«æœ‰åŒºåˆ«è¯çš„åºåˆ—ï¼Œæˆ‘ä»¬çš„åˆ‡åˆ†åŸåˆ™ä¹Ÿæ˜¯åŒæ ·æŒ‰ç…§éŸ³èŠ‚æ¥è¿›è¡Œï¼Œå¦‚æœåŒºåˆ«è¯åæ¥ä¸€ä¸ªå•éŸ³èŠ‚åè¯ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼Œè‹¥æ¥çš„æ˜¯å¤šéŸ³èŠ‚åè¯ï¼Œåˆ™è¦åˆ‡åˆ†ã€‚ ï¼ˆ9ï¼‰è¿°è¡¥ç»“æ„ ç®€å•æ¥è¯´ï¼Œè¿°è¡¥ç»“æ„æŒ‡çš„æ˜¯æè¿°ä¸€ä¸ªåŠ¨è¯å‘ç”Ÿçš„æƒ…è²Œæˆ–ç»“æœï¼Œå³å¯¹åŠ¨è¯æ‰€ä»£è¡¨çš„äº‹ä»¶è¿›è¡Œçš„è¡¥å……ã€‚å¯¹äºåŒéŸ³èŠ‚çš„è¿°è¡¥ç»“æ„æˆ‘ä»¬çš„åˆ‡åˆ†åŸåˆ™æ˜¯ï¼Œå¦‚æœè¿›è¡Œåˆ‡åˆ†åï¼Œä¼šæœ‰æ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ å­˜åœ¨ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼Œåä¹‹ï¼Œåˆ™åˆ‡åˆ†ã€‚ è¿°è¡¥ç»“æ„ä¸­è¿˜æœ‰ä¸€ç±»å¸¸è§çš„å¤šéŸ³èŠ‚çš„â€œå¾—â€å­—è¡¥è¯­ï¼Œå¯¹äºè¿™ç±»è¿°è¡¥ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥å°†â€œå¾—â€å­—å»æ‰ï¼Œè‹¥å»æ‰åä¾ç„¶èƒ½æˆè¯ï¼Œåˆ™è¦å°†å…¶åˆ‡åˆ†ï¼›è‹¥ä¸èƒ½æˆè¯ï¼Œåˆ™â€œå¾—â€å­—è¡¥è¯­æ•´ä½“ä½œä¸ºä¸€ä¸ªåˆ†è¯å•ä½ï¼Œå†…éƒ¨ä¸åšåˆ‡åˆ†ã€‚ ï¼ˆ10ï¼‰ã€ï¼ˆ11ï¼‰ã€ï¼ˆ12ï¼‰ã€ï¼ˆ13ï¼‰ç•¥ ï¼ˆ14ï¼‰è¯­ç´ å’Œéè¯­ç´ å­—çš„å¤„ç† å¯¹äºç¦»åˆè¯çš„ç¦»æå½¢å¼ï¼Œè¦è¿›è¡Œåˆ‡åˆ†ã€‚æ‰€è°“ç¦»åˆè¯ï¼ŒæŒ‡çš„æ˜¯å¯ä»¥åœ¨ç»„åˆçš„ä¸¤ä¸ªè¯­ç´ ä¸­æ’å…¥å…¶ä»–æˆåˆ†çš„è¯ï¼Œæ¯”å¦‚â€œåƒé¥­â€ï¼Œå®ƒçš„ç¦»æå½¢å¼æœ‰ï¼Œâ€œåƒäº†é¥­â€â€œåƒäº†ä¸€ä¸ªé¥­â€ç­‰ã€‚ å¯¹äºè¡¨ç¤ºæ–¹ä½çš„åŒéŸ³èŠ‚è¯ï¼Œè‹¥åˆ‡åˆ†å‡ºæ— æ³•ç‹¬ç«‹æˆè¯çš„è¯­ç´ ï¼Œåˆ™ä¸åˆ‡åˆ†ï¼Œå¦åˆ™åˆ™è¦è¿›è¡Œåˆ‡åˆ†ã€‚ ï¼ˆ15ï¼‰æ–‡æœ¬ä¸­éæ±‰å­—çš„å­—ç¬¦ä¸² ç•¥ ï¼ˆ16ï¼‰é‡å  é‡å æ˜¯æ±‰è¯­ç‹¬ç‰¹çš„è¯­è¨€ç°è±¡ä¹‹ä¸€ã€‚åŒ—å¤§æ–¹æ¡ˆä¸­å¯¹è¿™ç±»è¯çš„åˆ‡åˆ†çœ‹ä¼¼å¤æ‚ï¼Œå®è´¨ä¸Šæ˜¯åˆ‡åˆ†åˆ°èƒ½å¤Ÿç‹¬ç«‹ä½¿ç”¨çš„å•ä½ï¼Œå¹¶ä¸”è¦é¿å…åˆ‡åˆ†å‡ºä¸èƒ½å•ç‹¬æˆè¯çš„è¯­ç´ ã€‚ æ¯”å¦‚ï¼Œâ€œç”œç”œçš„èœ‚èœœâ€ï¼Œç”±äºâ€œç”œç”œâ€ä¸èƒ½å•ç‹¬æˆè¯ï¼Œå› æ­¤è¦åˆ‡åˆ†åˆ°â€œç”œç”œçš„â€ã€‚ è€Œâ€œè¯•è¯•çœ‹â€ç”±äºâ€œçœ‹â€è¿™é‡Œè¡¨ç¤ºåŠ¨ä½œçš„å°è¯•ï¼Œä½œä¸ºè¿™ä¸ªæ„ä¹‰å¹¶ä¸èƒ½å•ç‹¬è¿ç”¨ï¼Œå› æ­¤ä¸åˆ‡åˆ†ã€‚ ï¼ˆ17ï¼‰é™„åŠ æˆåˆ† é™„åŠ æˆåˆ†å®è´¨ä¸ŠæŒ‡çš„æ˜¯æ„è¯ä¸­çš„å‰ç¼€å’Œåç¼€ã€‚æ±‰è¯­æ„è¯æ³•ä¸­æœ‰ä¸€ç±»æ˜¯ä¾æ®è¯ç¼€åŠ è¯æ ¹è¿›è¡Œçš„æ´¾ç”Ÿæ„è¯ã€‚å¯¹äºè¿™ä¸€ç±»åˆ‡åˆ†åºåˆ—ï¼Œé™¤éå…¶æ¥å…¥æˆåˆ†å¤ªå¤šï¼Œä¼šå¯¹å…¶è¿›è¡Œåˆ‡åˆ†ï¼Œå¦åˆ™ä¸åˆ‡åˆ†ã€‚æ¯”å¦‚â€œè€å¸ˆä»¬â€å°±ä¸åšåˆ‡åˆ†ï¼Œâ€œè‹¦è‹¦è¿½æ±‚è€Œä¸å¾—è€…â€ä¸­çš„â€œè€…â€ç”±äºç»Ÿæ‘„çš„æˆåˆ†å¤ªå¤šï¼Œæ‰€ä»¥è¦å•ç‹¬åˆ‡åˆ†å¼€ã€‚ ï¼ˆ18ï¼‰å¤åˆè¯æ„è¯ åœ¨åˆ‡åˆ†å¤åˆè¯çš„é—®é¢˜ä¸Šï¼ŒåŒ—å¤§æ–¹æ¡ˆæ˜¯å­˜åœ¨è®¨è®ºçš„ä½™åœ°çš„ã€‚ç”±äºå¤åˆè¯æœ¬èº«å’ŒçŸ­è¯­ä¹‹é—´çš„ç•Œé™è¾ƒä¸ºæ¨¡ç³Šï¼Œå³ä½¿åœ¨è¯­è¨€å­¦æ„ä¹‰çš„ç•Œå®šä¸Šä¹Ÿä¼šå­˜åœ¨åˆ†æ­§ï¼Œå› æ­¤å¯¹äºå¤åˆè¯ç±»å‹çš„åˆ‡åˆ†åºåˆ—æ˜¯å¦åˆ‡åˆ†ï¼Œå®è´¨ä¸Šå¾ˆéš¾å›ç­”ã€‚åŒ—å¤§æ–¹æ¡ˆç»™å‡ºçš„è§£å†³åŠæ³•æ˜¯ï¼Œé¦–å…ˆå¦‚æœåˆ‡åˆ†åä¼šæœ‰æ— æ³•ç‹¬ç«‹æˆè¯çš„æˆåˆ†ï¼Œé‚£ä¹ˆå°±ä¸åˆ‡åˆ†ï¼›å¦å¤–è¦åˆ¤æ–­è¿™ä¸ªå¤åˆè¯çš„æ„ä¹‰æ˜¯å¦åªæ˜¯ç»„æˆæˆåˆ†çš„ç®€å•ç›¸åŠ ï¼Œå¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå°±åˆ‡åˆ†ï¼Œå¦‚æœä¸æ˜¯ï¼Œé‚£å°±è¯´æ˜ç»„æˆè¯¥è¯çš„ä¸¤ä¸ªæˆåˆ†ä¹‹é—´æ„ä¹‰æ˜¯æœ‰ç›¸äº’æ¸—é€çš„è”ç»“çš„ï¼Œå°±ä¸èƒ½åˆ‡åˆ†ã€‚ä½†æ˜¯å¦‚ä½•åˆ¤æ–­å¤åˆè¯æ„ä¹‰æ˜¯å¦æ˜¯ç»„åˆæˆåˆ†çš„ç›¸åŠ å‘¢ï¼Ÿ è¿™é‡Œçš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯åŠ â€œçš„â€æ³•ã€‚è¿™ä¸ªæ–¹æ³•ä¸»è¦é’ˆå¯¹çš„æ˜¯å®šä¸­ç»“æ„çš„å¤åˆè¯ï¼Œå³ä¸€ä¸ªè¯­ç´ ä¿®é¥°å¦ä¸€ä¸ªè¯­ç´ ã€‚æ¯”å¦‚â€œç™½èŠ±â€ï¼Œå’Œâ€œç™½çš„èŠ±â€æ„ä¹‰ä¸€è‡´ï¼Œé‚£ä¹ˆå°±è¦åˆ‡åˆ†ã€‚ ç¬¬äºŒä¸ªæ–¹æ³•æ˜¯æ›¿æ¢æ³•ï¼Œå°†å¤åˆè¯â€œABâ€çš„Aè¯­ç´ æ‹¿å‡ºæ¥è¿›è¡Œç»„è¯ï¼Œå†å°†Bè¯­ç´ æ‹¿å‡ºæ¥è¿›è¡Œç»„è¯ï¼Œè‹¥å•ç‹¬ç»„è¯åå…¶è¯ä¹‰éƒ½æ˜¯ä¸€æ ·çš„ï¼Œé‚£ä¹ˆå°±è¯´æ˜å¤åˆè¯ABçš„è¯ä¹‰æ˜¯Aè¯­ç´ ä¹‰å’ŒBè¯­ç´ ä¹‰çš„ç›¸åŠ ï¼Œå› æ­¤è¦åˆ‡åˆ†ï¼›è‹¥æœ‰Aè¯­ç´ æˆ–Bè¯­ç´ æœ‰å’Œå…¶ä»–ç»„è¯æƒ…å†µä¸­è¯­ä¹‰ä¸åŒçš„ï¼Œé‚£ä¹ˆå°±ä¸åˆ‡åˆ†å¤åˆè¯ABã€‚ ä½†æ˜¯è¿™ä¸¤ä¸ªæ–¹æ³•å¹¶ä¸èƒ½è§£å†³æ‰€æœ‰çš„å¤åˆè¯åˆ¤æ–­é—®é¢˜ï¼Œå› æ­¤åˆ°åº•æ˜¯å°†é—®é¢˜ç®€åŒ–è¿˜æ˜¯å¯¹è§„åˆ™è¿›ä¸€æ­¥ç»†è‡´ï¼Œæ˜¯å€¼å¾—æ€è€ƒçš„ã€‚ é¢—ç²’åº¦æ–¹æ¡ˆï¼ˆè°ƒæ•´ç‰ˆï¼‰è°ƒæ•´å†…å®¹ï¼š å°†åŸæ¥çš„ç¬¬ä¸€ç²’åº¦ä½œä¸ºç»†ç²’åº¦ï¼ˆéå¸¸ç»†ï¼Œå­˜åœ¨è¯­ä¹‰ä¸é€æ˜çš„è¯ç¼€ï¼‰ï¼Œå°†ç¬¬äºŒç²’åº¦å’Œç¬¬ä¸‰ç²’åº¦åˆå¹¶æˆä¸ºç²—ç²’åº¦ï¼‰ï¼Œé’ˆå¯¹ä¸“æœ‰åè¯çš„é—®é¢˜ï¼Œåˆ’å‡ºç²—ç²’åº¦2çº§ï¼ˆè¿™ä¸ªå¯ä»¥è®¨è®ºï¼Œæ˜¯åœ¨åˆ†è¯ä¸­ä¸€ä¸‹å­åˆ’åˆ†å‡ºæ¥ï¼Œè¿˜æ˜¯åœ¨ä¸Šæ¸¸ä»»åŠ¡ä¸­å†å¤„ç†ã€‚åœ¨å‚è€ƒèµ„æ–™çš„ä¸“åˆ©ä¸­ï¼Œä»–ä»¬å¾€å¾€åœ¨åˆ†è¯ä¸­å°±è§£å†³äº†ï¼‰ã€‚ ç†æ¸…å®ä½“å’Œä¸“æœ‰åè¯çš„åŒºåˆ«ç»†ç²’åº¦ å•éŸ³è¯ å•ç‹¬ä¸€ä¸ªè¯­ç´ å³å¯æˆè¯çš„ï¼Œå¦‚â€œç«ã€ä¹¦ã€æ°´â€ è¿ç»µè¯ å¿…é¡»å’Œå…¶ä»–è¯­ç´ ç»“åˆæˆè¯çš„ï¼Œä¸”ç»“åˆçš„è¯­ç´ æ˜¯å›ºå®šçš„ï¼Œå¦‚â€œè‘¡è„â€â€œä¹’ä¹“â€ éŸ³è¯‘è¯ åŒ…æ‹¬äº†å¤–å›½çš„ä¸“åï¼ˆäººåç­‰ï¼‰ æ•°è¯ é‡è¯ æ¯”å¦‚ï¼šæ¡ã€ä¸²ã€å¼  è¿™é‡Œè¦æ³¨æ„ä¸€äº›ä»åè¯å‘å±•è¿‡æ¥çš„é‡è¯ï¼Œæ¯”å¦‚â€œç¢—â€ è¿™é‡ŒåŒ…æ‹¬åº¦é‡ï¼š3/cmï¼Œ7/å¤© å¦å¤–ç»†ç²’åº¦ä¸­ï¼Œæ—¶é—´æ•°å’Œæ—¶é—´å•ä½ä¹Ÿåˆ‡åˆ†å¼€ï¼Œå¦‚ï¼š2018/å¹´ ä¸å«è¡Œæ”¿åŒºåˆ’çš„åœ°å æ¯”å¦‚ï¼šä¸Šæµ·ã€åŒ—äº¬ã€æ­¦æ±‰ ä¸“æœ‰åè¯ï¼šæœºæ„ã€å›¢ä½“ã€ç»„ç»‡ æ˜¯ä¸€ä¸ªå°é—­ç±»ï¼Œæ˜¯ä¸å¯ç±»æ¨çš„ åŒ…å«ä¸Šä¸‹éš¶å±å…³ç³»çš„å›¢ä½“æœºæ„ä¸“æœ‰åè¯ï¼Œåˆ‡åˆ†åˆ°æœ€å°çš„å›¢ä½“æœºæ„ã€‚æ¯”å¦‚â€œä¸­å›½/é“¶è¡Œ/åŒ—äº¬/åˆ†è¡Œâ€ã€‚ ç®€ç§°ç•¥è¯­ æ–¹ä½è¯ è¯­æ°”è¯ å¹è¯ å®è¯­ç´  åŒ…æ‹¬åŒ—å¤§æ–¹æ¡ˆé‡Œçš„å½¢è¯­ç´ ã€åè¯­ç´ ã€åŠ¨è¯­ç´ ã€äººåä¸­çš„å§“æ°ï¼Œæ¯”å¦‚ï¼šé”¦ï¼ˆå½¢è¯­ç´ ï¼‰ è™šè¯­ç´  å‰æ¥æˆåˆ† æ¯”å¦‚â€œé˜¿â€â€œè€â€â€œéâ€ è¿™ç±»é™¤äº†ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å‰ç¼€ï¼Œä¹Ÿè¦è€ƒè™‘ä¸€äº›ç½‘ç»œæµè¡Œè¯­çš„ä¸´æ—¶æ„è¯äº§å‡ºçš„å‰ç¼€ å‰¯è¯­ç´  ä¸»è¦æ˜¯å¦å®šå‰¯è¯ï¼Œæ¯”å¦‚â€œä¸â€â€œå¾ˆâ€ åæ¥æˆåˆ† æ¯”å¦‚ï¼šä»¬ï¼Œå„¿ï¼ˆè¡¨äº²æ˜µçš„ï¼‰ï¼Œå­ï¼Œå¤´ï¼ŒåŒ–ï¼Œè€… æˆ‘è®¤ä¸ºï¼Œè¿˜åº”åŒ…æ‹¬è¡Œæ”¿åŒºåˆ’çš„å•ä½ï¼Œæ¯”å¦‚ï¼šçœã€å¸‚ã€åŒºç­‰ï¼›å’Œè¡¨ç¤ºå°Šç§°çš„â€œè€â€â€œæ€»â€ åŠ©è¯ åŠ©åŠ¨è¯ã€åŠ©æ•°è¯ ä¹ è¯­ åŒ…æ‹¬æˆè¯­ã€å››å­—æ ¼çŸ­è¯­ã€æ­‡åè¯­ ä½†æ˜¯å¦‚æœæ­‡åè¯­æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œè¦æŒ‰ç…§æ ‡ç‚¹ç¬¦å·åˆ’åˆ† æ¯”å¦‚ï¼šâ€œä¸ç®¡ä¸‰ä¸ƒäºŒåä¸€â€â€œç™¾å°ºç«¿å¤´/ï¼Œ/æ›´è¿›ä¸€æ­¥â€ ç²—ç²’åº¦ç®€è¨€ä¹‹ï¼šåˆ‡åˆ°è¯ç»„å±‚ï¼Œä¸”æ³¨æ„éŸ³èŠ‚æ•°ï¼Œå¯¹åŒéŸ³èŠ‚æ”¾å®½ã€‚å°†ç»†ç²’åº¦ä¸­å¯æˆè¯çš„ç»„åˆæˆè¯ï¼ˆæ´¾ç”Ÿè¯ï¼‰ï¼Œå¦å°†å¯ç‹¬ç«‹æˆè¯çš„è¯æ ¹ç»“åˆæˆå¤åˆè¯ã€‚ç²—ç²’åº¦çš„åˆ‡åˆ†ç›®æ ‡æ˜¯ï¼Œä½¿å¾—æ¯ä¸€ä¸ªå®è¯æ€§çš„åˆ‡åˆ†å•ä½éƒ½æ˜¯è¡¨ä¹‰æ˜ç¡®çš„åˆ†è¯å•ä½ï¼Œä¸å­˜åœ¨è¯­ä¹‰ä¸é€æ˜çš„åˆ†è¯å•ä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿä¸èƒ½å¥¢æ±‚å®ä½“è¯†åˆ«ç­‰ä¸Šæ¸¸ä»»åŠ¡åœ¨åˆ†è¯ä»»åŠ¡ä¸­å°±å¾—ä»¥è§£å†³ã€‚ å‰æ¥æˆåˆ†+åè¯ æ¯”å¦‚ï¼šé˜¿ç‰› å‰æ¥æˆåˆ†+æ•° æ¯”å¦‚ï¼šé˜¿å¤§ åè¯+åæ¥æˆåˆ† æ¯”å¦‚ï¼šå­¦ç”Ÿä»¬ã€è€å¸ˆä»¬ã€æ‹³å¤´ã€é«˜æ¸…ç‰ˆ åŠ¨è¯+åæ¥æˆåˆ† æ¯”å¦‚ï¼šåˆ›æ–°åŒ–ï¼ˆå•ç‹¬â€œåˆ›æ–°â€è¿˜æ˜¯åˆ†åˆ°â€åˆ›æ–°â€œï¼‰ å§“æ°+å æ¯”å¦‚ï¼šå¼ ä¼Ÿ æ•°+é‡+ï¼ˆåŠ©æ•°è¯ï¼‰ æ¯”å¦‚ï¼šå››/äººï¼Œäº”ä¸ª/äºº æ—¶é—´ æŒ‰åŒ—å¤§æ–¹æ¡ˆï¼Œä¸è¦åˆå¹¶ æ¯”å¦‚ï¼š1997å¹´/9æœˆ/3æ—¥ï¼Œæ—©/å…«ç‚¹ å¤åˆè¯ åŒéŸ³èŠ‚ã€ä¸‰éŸ³èŠ‚ï¼ˆåˆ‡åˆ†åŸåˆ™è¯¦è§å¯¹åŒ—å¤§æ–¹æ¡ˆçš„è®²è§£ï¼‰ æ³¨æ„ï¼Œä¸è¦å°†è”åˆæ„è¯çš„è¯ç»„ç®—ä½œå¤åˆè¯ã€‚ åœ°å+è¡Œæ”¿åŒºåˆ’ æ¯”å¦‚ï¼šåŒ—äº¬å¸‚ã€ä¸Šæµ·å¸‚ åœ°å+è‡ªç„¶åœ°å½¢ æ¯”å¦‚ï¼šååŒ—å¹³åŸã€å—æ²™ç¾¤å²› ç²—ç²’åº¦ä¸‹çš„åˆ‡åˆ†éš¾ç‚¹1.ä¸“åå’Œå®ä½“çš„åˆ‡åˆ†ä¸“æœ‰åè¯æŒ‡çš„æ˜¯ä¸“æŒ‡æ€§çš„äººåã€åœ°åã€å›¢ä½“ã€æœºæ„ã€ç»„ç»‡ã€æ°‘æ—ã€å•†æ ‡ã€‚ äººåã€åœ°åã€æ°‘æ—ã€å•†æ ‡åŸºæœ¬ä¸Šæ²¡æœ‰å¼‚è®®ï¼Œä½†æ˜¯å“ªäº›å›¢ä½“ã€æœºæ„ã€ç»„ç»‡èƒ½ç®—ä¸“æœ‰åè¯ï¼Œå“ªäº›ä¸èƒ½ç®—æ˜¯ä¸å¤ªæ˜ç¡®çš„ã€‚ å¦å¤–ï¼Œé™¤ä¸Šé¢æŒ‡å‡ºçš„åˆ†ç±»å¤–ï¼Œå…¶ä»–çš„å…·æœ‰ä¸“æŒ‡æ€§çš„å®ä½“ï¼Œä¸èƒ½è¢«å½“åšä¸“æœ‰åè¯æ¥å¤„ç†ã€‚å…·ä½“æ¥è¯´ï¼Œä¸“æœ‰åè¯çš„åˆ‡åˆ†éš¾ç‚¹æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š ï¼ˆ1ï¼‰ ä¸“æœ‰åè¯çš„ä¸“æŒ‡æ€§æ˜¯å¿½ç•¥æ–‡æœ¬è¯­å¢ƒã€‚æ¯”å¦‚â€æ ¡é•¿åŠå…¬å®¤å‘å¸ƒé‡è¦é€šçŸ¥â€œï¼Œå³ä½¿é€šè¿‡å‰æ–‡æˆ‘ä»¬çŸ¥é“è¿™é‡ŒæŒ‡çš„æ˜¯åŒ—å¤§çš„æ ¡é•¿åŠå…¬å®¤ï¼Œæˆ‘ä»¬åªå°†å®ƒä½œä¸ºæ™®é€šåè¯çš„å¤„ç†ï¼Œè€Œä¸æ˜¯ä½œä¸ºä¸€ä¸ªä¸“æŒ‡æ€§çš„æœºæ„åæ¥å¤„ç†ã€‚ ä½†æ˜¯åœ¨å›½é™…æˆ–ä¸­å›½èŒƒå›´å†…çš„çŸ¥åçš„å”¯ä¸€çš„å›¢ä½“ã€æœºæ„ã€ç»„ç»‡ çš„åç§°æˆ‘ä»¬ä¾ç„¶å°†ä¹‹å¤„ç†ä¸ºä¸“åï¼Œæ¯”å¦‚â€œå›½åŠ¡é™¢â€ï¼Œå®ƒå’Œâ€œæ ¡é•¿åŠå…¬å®¤â€çš„åŒºåˆ«åœ¨äºâ€œå›½åŠ¡é™¢â€å…¨å›½åªæœ‰ä¸€ä¸ªï¼Œè€Œâ€œæ ¡é•¿åŠå…¬å®¤â€æœ‰å¾ˆå¤šä¸ªï¼Œå› æ­¤â€œå›½åŠ¡é™¢â€ä½œä¸ºä¸“åä¸åˆ‡åˆ†ï¼Œè€Œâ€œæ ¡é•¿åŠå…¬å®¤â€è¦åˆ‡åˆ†æˆâ€œæ ¡é•¿/åŠå…¬å®¤â€ã€‚ ï¼ˆ2ï¼‰ä¸“æœ‰åè¯çš„ç»„åˆæ€§ã€‚ä¸“æœ‰åè¯æœ‰æ—¶ä¼šå’Œå…¶ä»–åè¯ä¸€èµ·ç»„åˆæˆè¯ã€‚å¯¹äºåˆ†è¯ä»»åŠ¡è€Œè¨€ï¼Œæˆ‘ä»¬åªéœ€è€ƒè™‘å°†ä¸“æœ‰åè¯å’Œè¿™ä¸ªè¯åˆ‡å¼€åè¿™ä¸ªè¯èƒ½å¦å•ç‹¬æˆè¯ï¼Œå¦‚æœä¸èƒ½ï¼Œé‚£ä¹ˆå°±ä¸åˆ‡åˆ†ï¼Œå¦‚æœèƒ½ï¼Œé‚£ä¹ˆå°±åˆ‡åˆ†ã€‚ï¼ˆè¿™é‡Œå’ŒåŒ—å¤§æ–¹æ¡ˆä¸åŒï¼ŒåŒ—å¤§æ–¹æ¡ˆè®¤ä¸ºæ¥å•éŸ³èŠ‚å¯ä»¥åˆ‡åˆ†ï¼Œä¹Ÿå¯ä»¥ä¸åˆ‡åˆ†ã€‚ï¼‰æ¯”å¦‚â€æ»¡äººâ€œï¼Œâ€å“ˆè¨å…‹äººâ€œï¼Œâ€æ˜Œå¹³/åˆ†è¡Œâ€œï¼Œè€Œå¯¹äºä¸€äº›å¤šä¸ªåè¯ç»„åˆæˆä¸“åçš„æƒ…å†µï¼Œæ¯”å¦‚â€œå…¨å›½/æ€»/å·¥ä¼šâ€â€œå…¨å›½/äººæ°‘/ä»£è¡¨/å¤§ä¼šâ€œï¼Œåœ¨ç»†ç²’åº¦å’Œç²—ç²’åº¦ä¸­ï¼Œç”±äºå®ƒä»¬éŸ³èŠ‚æ•°è¾ƒå¤šï¼Œè§†ä¸ºæ™®é€šåè¯è¿›è¡Œåˆ‡åˆ†ã€‚æ˜¯å¦å¯ä»¥è®¾ç½®ä¸€ä¸ªç²—ç²’åº¦2çº§ï¼Œåœ¨ç²—ç²’åº¦2çº§ä¸­ï¼Œä½œä¸ºç»„ç»‡ç±»ä¸“æœ‰åè¯ï¼Œä¸åˆ‡åˆ†ã€‚ ï¼ˆ3ï¼‰ä¸“æœ‰åè¯å±‚æ¬¡æ€§ã€‚è¡¨ç¤ºæœºæ„çš„ä¸“æœ‰åè¯ä¸­æœ‰äº›æ˜¯å‰åç›¸è¿ï¼ŒåŒ…å«ä¸Šä¸‹éš¶å±å…³ç³»çš„ã€‚ä¸‹çº§æœºæ„çš„ä¸“æŒ‡æ€§æœ‰çš„æ˜¯ä»ç”±ä¸Šçº§å›¢ä½“ç»§æ‰¿æ¥çš„ï¼Œæ¯”å¦‚â€œåŒ—äº¬å¤§å­¦è®¡ç®—è¯­è¨€å­¦ç ”ç©¶æ‰€â€æ˜¯ä¸€ä¸ªä¸“æŒ‡æ€§çš„çŸ­è¯­ï¼Œå®ƒä¹‹æ‰€ä»¥æœ‰ä¸“æŒ‡æ€§ï¼Œæ˜¯å› ä¸ºâ€œåŒ—äº¬å¤§å­¦â€è¿™ä¸ªä¸“æœ‰åè¯çš„ä¸“æŒ‡æ€§ï¼Œå¦‚æœæ²¡æœ‰â€œåŒ—äº¬å¤§å­¦â€ï¼Œåˆ™â€œè®¡ç®—è¯­è¨€å­¦ç ”ç©¶æ‰€â€æŒ‰ç…§æ™®é€šåè¯è¯ç»„æ¥åˆ‡åˆ†ï¼ˆå‚ç…§ç¬¬ä¸€ç‚¹ï¼‰ï¼›æœ‰çš„æ˜¯é€šè¿‡å…¶ä»–ä¸“æœ‰åè¯ï¼Œå¦‚åœ°åã€äººåè·å¾—çš„ï¼Œæ¯”å¦‚â€œé²è¿…ç ”ç©¶é™¢â€ï¼Œâ€œåŒ—äº¬åˆ†è¡Œâ€ã€‚åœ¨ç²—ç²’åº¦ä¸­ï¼Œå¯¹äºè·å¾—ä¸“æŒ‡æ€§çš„ä¸“æœ‰åè¯ä¸åˆ‡åˆ†ï¼Œå¦‚â€œé²è¿…ç ”ç©¶é™¢â€ï¼Œâ€œåŒ—äº¬åˆ†è¡Œâ€æ˜¯å¦å¯ä»¥è®¾ç½®ç²—ç²’åº¦2çº§ï¼Œè¡¨ç¤ºä¸Šä¸‹çº§çš„ä¸“æœ‰åè¯å…¨éƒ¨çº³å…¥ï¼Ÿæ¯”å¦‚â€œåŒ—äº¬å¤§å­¦è®¡ç®—è¯­è¨€å­¦ç ”ç©¶æ‰€â€ï¼Œåœ¨ç²—ç²’åº¦2çº§ä¸­å°±ä¸åšåˆ‡åˆ†ã€‚ ï¼ˆ4ï¼‰ç”µè§†èŠ‚ç›®ã€æ–‡è‰ºä½œå“ï¼ˆä¹¦ã€æ–‡æ¡£ã€åè®®ï¼‰æ ‡é¢˜ã€ç”µè§†å‰§ã€æˆ˜äº‰åç­‰ï¼Œä¸ä½œä¸ºä¸“æœ‰åè¯ï¼ŒæŒ‰ç…§æ™®é€šåè¯åˆ’åˆ†ã€‚ä¸¾ä¾‹ï¼š ä¼Šæ‹‰å…‹/æˆ˜äº‰ è¾›äº¥/é©å‘½ å¹³æ´¥/æˆ˜å½¹ å¼€å¿ƒ/è¯å…¸ æ–°é—»/30åˆ† æ–°é—»/æ—©/8ç‚¹ ä¸­å¤®ç”µè§†å°/-/1ï¼ˆå®ƒä»¬åæœŸå¯ä»¥é€šè¿‡ä¹¦åå·å’Œå¼•å·è¯†åˆ«å‡ºæ¥ã€‚ï¼‰ 2.æ”¿æ²»è¯è¯­æ˜¯å¦ç®—ä½œä¹ è¯­ï¼Ÿï¼ˆå¯ä»¥è®¨è®ºï¼‰æ”¿æ²»å£å·å’Œæ”¿æ²»æ€æƒ³ç”±äºåœ¨ä¸€å®šçš„å†å²æ—¶æœŸä¸­é¢‘ç¹ä½¿ç”¨ï¼Œå› æ­¤ï¼Œå¦‚æœåˆ‡åˆ†è¡¨æ„å°±ä¸ä¸€æ ·ã€‚æ¯”å¦‚â€œä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³â€å’Œâ€œä¹ è¿‘å¹³æ–°æ—¶ä»£ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³â€å°±æ˜¯ä¸¤ä¸ªæ¦‚å¿µã€‚ æœ‰ä¸¤ä¸ªè§£å†³æ–¹æ¡ˆï¼Œä¸€ä¸ªæ˜¯å°†éŸ³èŠ‚è¾ƒçŸ­çš„æ”¿æ²»è¯è¯­ç®—ä½œè¯­æ³•è¯å…¸ä¸­çš„è¯ï¼Œå¦‚â€œç§‘æŠ€å¼ºå›½â€â€œç§‘æ•™å…´å›½â€â€œç»¿è‰²ç»æµâ€ï¼Œâ€œç§‘æŠ€åˆ›æ–°â€ç­‰ç­‰ï¼Œç„¶åé‡åˆ°è¿™æ ·çš„è¯ï¼Œç»†ç²’åº¦ã€ç²—ç²’åº¦é‡Œéƒ½ä¸åˆ‡åˆ†ï¼Œè€ŒéŸ³èŠ‚è¾ƒé•¿çš„ï¼Œæ¯”å¦‚â€œä¸­åæ°‘æ—ä¼Ÿå¤§å¤å…´â€å°±ä½œä¸ºæ™®é€šåè¯è¿›è¡Œåˆ‡åˆ†ï¼›ç¬¬äºŒä¸ªè§£å†³æ–¹æ¡ˆæ˜¯å…¨éƒ¨æŒ‰ç…§æ™®é€šåè¯åˆ‡åˆ†ï¼Œåˆ°å…·ä½“çš„ä»»åŠ¡éœ€æ±‚æ—¶å†å¤„ç†ã€‚ä¸è¿‡ï¼Œæˆ‘è§‰å¾—è¿™ä¸¤ä¸ªè§£å†³æ–¹æ¡ˆéƒ½ä¼šå½±å“åˆ†è¯ç²’åº¦æ•´ä½“çš„å¹³è¡¡åº¦ï¼Œå› ä¸ºæ”¿æ²»å£å·æ„è¯æœ‰æ—¶éå¸¸éå¸¸é•¿ã€‚ 3.æŸæŸç†è®ºçš„åç§°ç®—ä½œä¸“åå—ï¼ŸæŸæŸé¢†åŸŸç†è®ºä¸­çš„ä¸“ä¸šæœ¯è¯­ç®—ä½œä¸“åå—ï¼Ÿç†è®ºçš„å‘½ååŒæ ·æ˜¯ä»»æ„æ€§çš„å‘½åè¡Œä¸ºï¼Œå’Œèœåä¸€æ ·ï¼Œå¦‚æœå¯¹â€œxxxç†è®ºâ€ä¸­çš„â€œxxxâ€è¿›è¡Œåˆ‡åˆ†åï¼Œâ€œxxxâ€çš„æ„æ€æœ‰æ‰€æ”¹å˜ï¼Œé‚£ä¹ˆå°±ä¸èƒ½åˆ‡åˆ†ï¼Œå¦‚æœæ²¡æœ‰æ”¹å˜ï¼Œåˆ™å¯ä»¥åˆ‡åˆ†ã€‚æ¯”å¦‚â€œç²¾ç¥åˆ†æ/ç†è®ºâ€ï¼Œå¦‚æœåˆ‡åˆ†æˆâ€œç²¾ç¥/åˆ†æâ€ï¼Œè¿™ä¸ªâ€œç²¾ç¥â€å’Œâ€œä½ ä»Šå¤©ç²¾ç¥ä¸ä½³â€ä¸­çš„â€œç²¾ç¥â€å¹¶ä¸æ˜¯ä¸€ä¸ªæ„æ€ï¼Œå› æ­¤ä¸èƒ½åˆ‡åˆ†ã€‚è€Œâ€œç‰›é¡¿/ç¬¬äºŒ/å®šå¾‹â€åˆ‡åˆ†åæ²¡é—®é¢˜ï¼Œå› ä¸ºè¿™ä¸ªç†è®ºçš„å‘½åæœ¬èº«æ˜¯ç»„åˆè€Œæˆçš„ã€‚ é‚£ä¹ˆå„ä¸ªé¢†åŸŸä¸­çš„ä¸“ä¸šæœ¯è¯­æ˜¯å¦ç®—ä½œä¸“åå‘¢ï¼Ÿæˆ‘è®¤ä¸ºåœ¨é€šç”¨å‹çš„åˆ†è¯ä¸­ï¼ŒåªåŠ å…¥æœ€ä¸ºé‡è¦çš„ä¸€äº›ä¸“ä¸šæœ¯è¯­ï¼›è€Œåœ¨ç‰¹å®šé¢†åŸŸä¸­ï¼Œå†åœ¨è¿™æ–¹é¢è¿›è¡Œæ‹“å±•ã€‚å› æ­¤ï¼Œâ€œç¤¾ä¼šç”Ÿæ´»â€åœ¨ç¤¾ä¼šå­¦ä¸­åº”å½“ç®—ä½œä¸€ä¸ªä¸“ä¸šæœ¯è¯­ï¼Œä½†æ˜¯åœ¨é€šç”¨å‹çš„åˆ†è¯ä¸­è¿˜æ˜¯æŒ‰ç…§æ™®é€šåè¯æ¥è¿›è¡Œåˆ‡åˆ†ï¼Œå³â€œç¤¾ä¼š/ç”Ÿæ´»â€ã€‚ 4.å¹¶åˆ—æˆåˆ†å¦‚ä½•åˆ‡åˆ†ï¼Ÿå¹¶åˆ—æˆåˆ†æŒ‰ç…§é¡¿å·è¿›è¡Œåˆ‡åˆ†ï¼Œæ¯”å¦‚â€œå¹³æ´¥/ã€/è¾½æ²ˆ/æˆ˜å½¹â€ï¼Œâ€å¼ /ã€/æå®¶â€œï¼ˆè¿™é‡Œçš„â€å¼ â€œå¯ä»¥çœ‹åšæ˜¯â€å¼ å®¶â€œçš„ç¼©ç•¥å½¢å¼ï¼‰ã€‚","link":"/2018/08/30/é¢—ç²’åº¦åˆ†è¯è°ƒç ”/"}],"tags":[{"name":"å‘½ä»¤è¡Œ","slug":"å‘½ä»¤è¡Œ","link":"/tags/å‘½ä»¤è¡Œ/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"æ·±åº¦å­¦ä¹ ","slug":"æ·±åº¦å­¦ä¹ ","link":"/tags/æ·±åº¦å­¦ä¹ /"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"keras","slug":"keras","link":"/tags/keras/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"åˆ†è¯","slug":"åˆ†è¯","link":"/tags/åˆ†è¯/"}],"categories":[]}
{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/05/30/hello-world/"},{"title":"终端命令笔记","text":"vi 常用命令1. \b进入 vi 编辑器1sudo vi &lt;path&gt; 2. 修改内容输入i，进入insert模式。按esc，退出模式。 3. 保存，退出 :w 保存文件但不退出vi; :wq 保存文件并退出vi; q: 不保存文件，退出vi :e! 放弃所有修改，从上次保存文件开始再编辑 vi命令大全 Jupyter or conda not found我安装好anaconda后，打算用命令行直接打开jupyter notebook，结果却没有成功，网上一般的解释是要把anaconda配置到环境变量里：在终端中输入： 1sudo vi ~/.bash_profile 打开后在末尾加上： 1234567export PATH='~/anaconda/bin:$PATH'# 这里的path要根据anaconda所在的位置定义source ~/.bash_profile# 表示修改立即生效 但是呢，我试了好几次都没有成功，事实上，是我配置了oh-my-zsh的原因。 因此，正确的解决方法是，打开~/.zshrc，然后在文件最后一行添加： 1export PATH=$PATH:$HOME/anaconda/bin 保存文件后，关闭窗口\b，重新开启窗口时，输入命令conda --v来检测是否成功。 参考链接🔗 zsh not found1exec /bin/zsh 参考资料🔗","link":"/2018/12/22/about-terminal/"},{"title":"TensorFlow小试牛刀","text":"此日志为参照Udacity课程中《Intro to tensorflow》的jupyter notebook所做的分解源码，目的在于理解代码逻辑，熟悉创建流程和套路。其中参考了不少博文链接，非常感谢，全部放在文末，在原文中不再指出。 数据链接：百度云：NoMNIST 密码：fsks P1:预处理数据123456789101112import hashlibimport osimport picklefrom urllib.request import urlretrieveimport numpy as npfrom PIL import Imagefrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.utils import resamplefrom tqdm import tqdmfrom zipfile import ZipFile 解压图片文件1234567891011121314151617181920212223242526272829303132333435363738394041def uncompress_features_labels(file): \"\"\" Uncompress features and labels from a zip file :param file: The zip file to extract the data from \"\"\" features = [] labels = [] with ZipFile(file) as zipf: # Progress Bar filenames_pbar = tqdm(zipf.namelist(), unit='files') # Get features and labels from all files for filename in filenames_pbar: # Check if the file is a directory if not filename.endswith('/'): with zipf.open(filename) as image_file: image = Image.open(image_file) image.load() # Load image data as 1 dimensional array # We're using float32 to save on memory space feature = np.array(image, dtype=np.float32).flatten() # Get the the letter from the filename. This is the letter of the image. label = os.path.split(filename)[1][0] features.append(feature) labels.append(label) return np.array(features), np.array(labels)# Get the features and labels from the zip filestrain_features, train_labels = uncompress_features_labels('notMNIST_train.zip')test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')# Limit the amount of data to work with a docker containerdocker_size_limit = 150000train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)# Set flags for feature engineering. This will prevent you from skipping an important step.is_features_normal = Falseis_labels_encod = False 12100%|█████████████████████████████████████████████████████████████████████| 210001/210001 [00:54&lt;00:00, 3832.78files/s]100%|███████████████████████████████████████████████████████████████████████| 10001/10001 [00:03&lt;00:00, 3207.15files/s] Min-Max ScalingImplement Min-Max scaling in the normalize_grayscale() function to a range of a=0.1 and b=0.9. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9. Since the raw notMNIST image data is in grayscale, the current values range from a min of 0 to a max of 255. Min-Max Scaling:$X’=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}$ 123456789101112def normalize_grayscale(image_data): \"\"\" Normalize the image data with Min-Max scaling to a range of [0.1, 0.9] :param image_data: The image data to be normalized :return: Normalized image data \"\"\" a = 0.1 b = 0.9 max_grayscale = 255 min_grayscale = 0 return a+((image_data-min_grayscale))*(b-a)/(max_grayscale-min_grayscale) 12train_features = normalize_grayscale(train_features)test_features = normalize_grayscale(test_features) 标签二值化LabelBinarizer()是sklearn.preprocession中用来将非数值类标签转换为独热编码向量的函数。 123456789# Create the encoder 创建编码器encoder = LabelBinarizer()# 编码器找到类别并分配 one-hot 向量encoder.fit(train_labels)#最后把目标（lables）转换成独热编码的（one-hot encoded）向量train_labels = encoder.transform(train_labels)test_labels = encoder.transform(test_labels) 转换数据类型，这样后面公式中才可以进行运算。 12train_labels = train_labels.astype(np.float32)test_labels = test_labels.astype(np.float32) 随机划分训练集和测试集常见形式为：X_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0) 参数解释： train_data：所要划分的样本特征集 train_target：所要划分的样本结果 test_size：样本占比，如果是整数的话就是样本的数量 random_state：是随机数的种子。 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。 123456# Get randomized datasets for training and validationtrain_features, valid_features, train_labels, valid_labels = train_test_split( train_features, train_labels, test_size=0.05, random_state=832289) 打包数据方便下次取用序列化的方法为 pickle.dump()，该方法的相关参数如下：pickle.dump(obj, file, protocol=None,*,fix_imports=True) 1234567891011121314151617181920# 新建pickle_file# 参数file必须是以二进制的形式进行操作,即「wb」pickle_file = 'notMNIST.pickle'if not os.path.isfile(pickle_file): print('Saving data to pickle file...') try: with open('notMNIST.pickle', 'wb') as pfile: pickle.dump( { 'train_dataset': train_features, 'train_labels': train_labels, 'valid_dataset': valid_features, 'valid_labels': valid_labels, 'test_dataset': test_features, 'test_labels': test_labels, }, pfile, pickle.HIGHEST_PROTOCOL) except Exception as e: print('Unable to save data to', pickle_file, ':', e) raise P2:从预处理好的pickle中读取数据12345678910111213141516171819202122%matplotlib inline# Load the modulesimport pickleimport mathimport numpy as npimport tensorflow as tffrom tqdm import tqdmimport matplotlib.pyplot as plt# Reload the datapickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: pickle_data = pickle.load(f) train_features = pickle_data['train_dataset'] train_labels = pickle_data['train_labels'] valid_features = pickle_data['valid_dataset'] valid_labels = pickle_data['valid_labels'] test_features = pickle_data['test_dataset'] test_labels = pickle_data['test_labels'] del pickle_data # Free up memory C:\\Users\\10677\\Anaconda3\\envs\\keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 使用TF创建单层神经网络接下来，我们使用TensorFlow创建一个只有一个输入层和输出层的神经网络，激活函数为softmax。在TensorFlow中，数据不是以整数、浮点数或字符串的形式存储的，而是以tensor对象的形式被存储的。 在tensor中传递值有两种方法： 使用tf.constant()，传入变量，但是传入之后就不可变了 如果要使数据可变，结合tf.placeholder()和tf.feed_dict来输入 1234567891011121314151617181920212223242526272829303132333435363738394041# All the pixels in the image (28 * 28 = 784)features_count = 784# All the labels (\"A,B...J\")labels_count = 10features = tf.placeholder(tf.float32)labels = tf.placeholder(tf.float32)# Set the weights and biases tensors# tf.truncated_normal:生成正态分布的随机值# weights已经随机化，biases就不必随机，简化为0即可weights = tf.Variable(tf.truncated_normal((features_count,labels_count)))biases = tf.Variable(tf.zeros(labels_count))# Feed dicts for training, validation, and test sessiontrain_feed_dict = {features: train_features, labels: train_labels}valid_feed_dict = {features: valid_features, labels: valid_labels}test_feed_dict = {features: test_features, labels: test_labels}# Linear Function WX + blogits = tf.matmul(features, weights) + biasesprediction = tf.nn.softmax(logits)# Cross entropycross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)# Training lossloss = tf.reduce_mean(cross_entropy)# Create an operation that initializes all variablesinit = tf.global_variables_initializer()# Test Caseswith tf.Session() as session: session.run(init) session.run(loss, feed_dict=train_feed_dict) session.run(loss, feed_dict=valid_feed_dict) session.run(loss, feed_dict=test_feed_dict) biases_data = session.run(biases) 12is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32)) P3:训练神经网络1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Change if you have memory restrictionsbatch_size = 128# Find the best parameters for each configurationepochs = 4learning_rate = 0.2# Gradient Descent# 使用梯度下降进行训练optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # The accuracy measured against the validation setvalidation_accuracy = 0.0# Measurements use for graphing loss and accuracylog_batch_step = 50batches = []loss_batch = []train_acc_batch = []valid_acc_batch = []with tf.Session() as session: session.run(init) batch_count = int(math.ceil(len(train_features)/batch_size)) for epoch_i in range(epochs): # Progress bar batches_pbar = tqdm(range(batch_count), desc='Epoch {:&gt;2}/{}'.format(epoch_i+1, epochs), unit='batches') # The training cycle for batch_i in batches_pbar: # Get a batch of training features and labels batch_start = batch_i*batch_size batch_features = train_features[batch_start:batch_start + batch_size] batch_labels = train_labels[batch_start:batch_start + batch_size] # Run optimizer and get loss _, l = session.run( [optimizer, loss], feed_dict={features: batch_features, labels: batch_labels}) # Log every 50 batches if not batch_i % log_batch_step: # Calculate Training and Validation accuracy training_accuracy = session.run(accuracy, feed_dict=train_feed_dict) validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict) # Log batches previous_batch = batches[-1] if batches else 0 batches.append(log_batch_step + previous_batch) loss_batch.append(l) train_acc_batch.append(training_accuracy) valid_acc_batch.append(validation_accuracy) # Check accuracy against Validation data validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)loss_plot = plt.subplot(211)loss_plot.set_title('Loss')loss_plot.plot(batches, loss_batch, 'g')loss_plot.set_xlim([batches[0], batches[-1]])acc_plot = plt.subplot(212)acc_plot.set_title('Accuracy')acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')acc_plot.set_ylim([0, 1.0])acc_plot.set_xlim([batches[0], batches[-1]])acc_plot.legend(loc=4)plt.tight_layout()plt.show()print('Validation accuracy at {}'.format(validation_accuracy)) Epoch 1/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:11&lt;00:00, 101.27batches/s] Epoch 2/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:10&lt;00:00, 101.99batches/s] Epoch 3/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:10&lt;00:00, 101.38batches/s] Epoch 4/4: 100%|█████████████████████████████████████████████████████████████| 1114/1114 [00:12&lt;00:00, 92.55batches/s] Validation accuracy at 0.7662666440010071 P4:检测12345678910111213141516171819202122232425262728test_accuracy = 0.0with tf.Session() as session: session.run(init) batch_count = int(math.ceil(len(train_features)/batch_size)) for epoch_i in range(epochs): # Progress bar batches_pbar = tqdm(range(batch_count), desc='Epoch {:&gt;2}/{}'.format(epoch_i+1, epochs), unit='batches') # The training cycle for batch_i in batches_pbar: # Get a batch of training features and labels batch_start = batch_i*batch_size batch_features = train_features[batch_start:batch_start + batch_size] batch_labels = train_labels[batch_start:batch_start + batch_size] # Run optimizer _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels}) # Check accuracy against Test data test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)assert test_accuracy &gt;= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)print('Nice Job! Test Accuracy is {}'.format(test_accuracy)) Epoch 1/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 588.57batches/s] Epoch 2/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 634.64batches/s] Epoch 3/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 633.74batches/s] Epoch 4/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 638.60batches/s] Nice Job! Test Accuracy is 0.8468999862670898 参考链接： python tqdm模块分析 Sklearn-train_test_split随机划分训练集和测试集 numpy_ndarray.flatten sklearn.LabelBinarizer","link":"/2018/09/03/tf小试牛刀/"},{"title":"Slot Filling with SimpleRNN","text":"什么是Slot Filling？Slot Filling是自然语言理解中的一个基本问题，是对语言含义的简单化处理，它的思想类似于语言学中框架主义的一派，先设定好特定的语言类型槽，再将输入的单词一一填入槽内，而获取言语含义的时候即是根据语义槽的含义进行提取和检索。我们这里的任务就是将表示定购航班（ATIS数据集）这一言语行为的一系列语句填入各种类型的语义槽中。 为什么使用SimpleRNN?Slot Filling属于RNN应用中一对一的应用，通过训练模型，每个词都能被填到合适的槽中。RNN和一般的神经网络的不同在于，在RNN中，我们在时间t的输出不仅取决于当前的输入和权重，还取决于之前的输入，而对于其他神经网络模型，每个时刻的输入和输出都是独立而随机的，没有相关性。放到我们要处理语义理解的问题上看，语言作为一种基于时间的线性输出，显然会受到前词的影响，因此我们选取RNN模型来进行解决这个问题。这里选取SimpleRNN,是因为这个RNN比较简单，能达到熟悉框架的练习效果，之后可以选取其他有效的RNN模型，如LSTMS进行优化。 构建思路一览： 载入数据，使用的是chsasank修改的mesnilgr的load.py。 定义模型。采取Keras中的序列模型搭建，首先使用一个100维的word embedding层将输入的单词转化为高维空间中的一个向量（在这个空间中，语义和语法位置越近的单词的距离越小），然后我们构建一个dropout层防止过拟合，设置SimpleRNN层，设置TimeDistributed层以完成基于时间的反向传播。最后我们将这些层组织在一起，并确定optimizer和loss function。我们选取的optimizer是rmsprop,这样在训练后期依然能找到较有项，而选取categorical_crossentropy作为损失函数，则是因为处理的问题性质适合于此。 训练模型。出于对计算资源的考虑，我们一般使用minibtach的方法批量对模型进行训练。但是我们这里的数据是一句句话，如果按照一个固定的batch_size将其分裂，可能增加了不必要的联系（因为上下两句话是独立的），因此我们将一句话作为一个batch去进行训练、验证以及预测，并手动算出一个epoch的平均误差。 评估和预测模型。我们通过观察验证误差和预测F1精度来对模型进行评估。预测F1精度使用的是signsmile编写的conlleval.py。 保存模型。 123456789101112import numpy as npimport picklefrom keras.models import Sequentialfrom keras.layers.embeddings import Embeddingfrom keras.layers.recurrent import SimpleRNNfrom keras.layers.core import Dense,Dropoutfrom keras.utils import to_categoricalfrom keras.layers.wrappers import TimeDistributedfrom matplotlib import pyplot as pltimport data.loadfrom metrics.accuracy import evaluate Using TensorFlow backend. Load Data123456train_set,valid_set,dicts = data.load.atisfull()# print(train_set[:1])# dicts = {'label2idx':{},'words2idx':{},'table2idx':{}}w2idx,labels2idx = dicts['words2idx'],dicts['labels2idx']train_x,_,train_label = train_setval_x,_,val_label = valid_set 12idx2w = {w2idx[i]:i for i in w2idx}idx2lab = {labels2idx[i]:i for i in labels2idx} 12n_classes = len(idx2lab)n_vocab = len(idx2w) 123456789101112131415words_train = [[idx2w[i] for i in w[:]] for w in train_x]labels_train = [[idx2lab[i] for i in w[:]] for w in train_label]words_val = [[idx2w[i] for i in w[:]] for w in val_x]# labels_val = [[idx2lab[i] for i in w[:]] for w in val_label]labels_val =[]for w in val_label: for i in w[:]: labels_val.append(idx2lab[i])print('Real Sentence : {}'.format(words_train[0]))print('Encoded Form : {}'.format(train_x[0]))print('='*40)print('Real Label : {}'.format(labels_train[0]))print('Encoded Form : {}'.format(train_label[0])) Real Sentence : [&apos;i&apos;, &apos;want&apos;, &apos;to&apos;, &apos;fly&apos;, &apos;from&apos;, &apos;boston&apos;, &apos;at&apos;, &apos;DIGITDIGITDIGIT&apos;, &apos;am&apos;, &apos;and&apos;, &apos;arrive&apos;, &apos;in&apos;, &apos;denver&apos;, &apos;at&apos;, &apos;DIGITDIGITDIGITDIGIT&apos;, &apos;in&apos;, &apos;the&apos;, &apos;morning&apos;] Encoded Form : [232 542 502 196 208 77 62 10 35 40 58 234 137 62 11 234 481 321] ======================================== Real Label : [&apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;B-fromloc.city_name&apos;, &apos;O&apos;, &apos;B-depart_time.time&apos;, &apos;I-depart_time.time&apos;, &apos;O&apos;, &apos;O&apos;, &apos;O&apos;, &apos;B-toloc.city_name&apos;, &apos;O&apos;, &apos;B-arrive_time.time&apos;, &apos;O&apos;, &apos;O&apos;, &apos;B-arrive_time.period_of_day&apos;] Encoded Form : [126 126 126 126 126 48 126 35 99 126 126 126 78 126 14 126 126 12] Define and Compile the model1234567model = Sequential()model.add(Embedding(n_vocab,100))model.add(Dropout(0.25))model.add(SimpleRNN(100,return_sequences=True))model.add(TimeDistributed(Dense(n_classes,activation='softmax')))model.compile(optimizer = 'rmsprop',loss = 'categorical_crossentropy')model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, None, 100) 57200 _________________________________________________________________ dropout_1 (Dropout) (None, None, 100) 0 _________________________________________________________________ simple_rnn_1 (SimpleRNN) (None, None, 100) 20100 _________________________________________________________________ time_distributed_1 (TimeDist (None, None, 127) 12827 ================================================================= Total params: 90,127 Trainable params: 90,127 Non-trainable params: 0 _________________________________________________________________ Train the model1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def train_the_model(n_epochs,train_x,train_label,val_x,val_label): epoch,train_avgloss,val_avgloss,f1s = [],[],[],[] for i in range(1,n_epochs+1): epoch.append(i) ## training train_avg_loss =0 for n_batch,sent in enumerate(train_x): label = train_label[n_batch] # label to one-hot label = to_categorical(label,num_classes=n_classes)[np.newaxis,:] sent = sent[np.newaxis,:] loss = model.train_on_batch(sent,label) train_avg_loss += loss train_avg_loss = train_avg_loss/n_batch train_avgloss.append(train_avg_loss) ## evaluate&amp;predict val_pred_label,pred_label_val,val_avg_loss = [],[],0 for n_batch,sent in enumerate(val_x): label = val_label[n_batch] label = to_categorical(label,num_classes=n_classes)[np.newaxis,:] sent = sent[np.newaxis,:] loss = model.test_on_batch(sent,label) val_avg_loss += loss pred = model.predict_on_batch(sent) pred = np.argmax(pred,-1)[0] val_pred_label.append(pred) val_avg_loss = val_avg_loss/n_batch val_avgloss.append(val_avg_loss) for w in val_pred_label: for k in w[:]: pred_label_val.append(idx2lab[k]) prec, rec, f1 = evaluate(labels_val,pred_label_val, verbose=False) print('Training epoch {}\\t train_avg_loss = {} \\t val_avg_loss = {}'.format(i,train_avg_loss,val_avg_loss)) print('precision: {:.2f}% \\t recall: {:.2f}% \\t f1 :{:.2f}%'.format(prec,rec,f1)) print('-'*60) f1s.append(f1) # return epoch,pred_label_train,train_avgloss,pred_label_val,val_avgloss return epoch,f1s,val_avgloss,train_avgloss 1epoch,f1s,val_avgloss,train_avgloss = train_the_model(40,train_x,train_label,val_x,val_label) 输出：1234567891011121314 Training epoch 1 train_avg_loss = 0.5546463992293973 val_avg_loss = 0.4345020865901363 precision: 84.79% recall: 80.79% f1 :82.74% ------------------------------------------------------------ Training epoch 2 train_avg_loss = 0.2575569036037627 val_avg_loss = 0.36228470020366654 precision: 86.64% recall: 83.86% f1 :85.22% ------------------------------------------------------------ Training epoch 3 train_avg_loss = 0.2238766908014994 val_avg_loss = 0.33974187403771694 precision: 88.03% recall: 85.55% f1 :86.77% ------------------------------------------------------------…… ------------------------------------------------------------ Training epoch 40 train_avg_loss = 0.09190682124901069 val_avg_loss = 0.2697056618613356 precision: 92.51% recall: 91.47% f1 :91.99% ------------------------------------------------------------ 可视化观察验证误差，选取合适的epoch。 123456%matplotlib inlineplt.xlabel=('epoch')plt.ylabel=('loss')plt.plot(epoch,train_avgloss,'b')plt.plot(epoch,val_avgloss,'r',label=('validation error'))plt.show() 1print('最大f1值为 {:.2f}%'.format(max(f1s))) 最大f1值为 92.56% 保存模型1model.save('slot_filling_with_simpleRNN.h5') 结果分析使用SimpleRNN最终得到的F1值为92.56%，和师兄的95.47%相比确实还相差很多。这主要是和我们模型的选取有关，SimpleRNN只能将前词的影响带入到模型中，但是语言中后词对前词也会有一定的影响，因此可以通过选择更加复杂的模型或者增加能够捕捉到后词信息的层来进行优化。 参考资料 Keras Tutorial - Spoken Language Understanding pytorch-slot-filling liu946 AtisSlotLabeling 【Keras情感分类】训练过程中出现的问题汇总 keras-SimpleRNN 机器学习中过拟合的解决办法","link":"/2018/09/10/slot-filling/"},{"title":"北大分词方案解读及颗粒度分词方案","text":"一、调研资料 北大现代汉语语料库基本加工规范 计算所汉语词性标注集 几个开源分词系统所使用标注集的来源 海量中文智能分词接口手册 阿里多粒度分词专利 腾讯多粒度分词专利 百度多粒度分词专利 KTDictSeg 分词组件1.3版本 部分算法讨论 – 分词粒度 二、调研目的分词单位不同于语言学中的“词”，不同的算法下的分词结果千差万别，有的分出的是语言学意义上的词，而有的分出的是语言学意义上的“短语”（或者说“词组”）因此，我们希望寻找一个可理解的统一的粒度标准，而这个粒度标准能够实现对不同分词任务的不同层次的分词。为证实多颗粒度的分词标注确实能提高特定的分词任务的准确率，我们进行了这样的前期调研。通过搜集资料，我们以北大方案为蓝本，以一定的语言学知识为基础，对分词颗粒进行不同粒度的划分。首先对北大分词方案进行解读，然后再阐释我对分词粒度初步的构建想法。 注：颗粒度方案只考虑分词问题，不考虑词性标注。 三、北大分词方案讲解1. 分词单位的概念界定分词单位，“指信息处理中使用的、具有确定的语义和语法功能的基本单位”，该概念明确了其使用的特定环境——“信息处理任务”，以及其语义和语法功能明确的特点。 基于这样的概念划分，北大方案认定的分词单位里不仅包括了词，还“包括了一部分结合紧密、使用稳定的词组”，并且“在某些特殊情况孤立的语素或非语素字”。 事实上，我们撇开北大方案来看词这个整体，根据朱德熙先生的划分，可以分为可穷尽的虚词类和不可穷尽的实词类。虚词类，举例来说，包括连词、语气词、介词等，这类词可以在语法词典中被枚举出来，因此在进行分词时难度较小。因此，分词的困难常常出现在实词的切分上。 结合北大方案的划分，我认为对实词序列进行划分时，一般可以遵照以下原则： （1）依据语法词典来划分，如果语法词典中进行规定，那么就不做划分。语言是约定俗成的产物，当某个词语组合被广泛而稳定地使用时，那么社会团体便会接受这样的一个“新词”，因此这样的一个词语组合也可以被视作是一个分词单位。而判断社会团体是否已经接受这一语言现象很显性的一大标志便是词典收录了该词条。那么问题就转变为，什么样的词典可以成为可供划分的语法词典。 （2）考虑切分序列的音节组合。汉语在发展过程中经历了一个从单音节向双音节的发展过程。虽然现代汉语以双音节为主要的成词单位，但是古代汉语中的一些单音节词依然残存在现代汉语中，并且在一些特殊语体中还广泛地存在着。因此，对于那些单音节成词的单位在标注时要格外注意标记出来，而处理多音节序列时，则要尽量保证分词结果以双音节为一个单位。 （3）考虑到词义与语素结合义。我们所认定的分词单位，它的词义是凝合而成的，而不是两个语素的意义简单的相加。因此，如果一个切分单位的语义是其切分单位意义的简单相加，那么就要对其进行切分。而判定是否是词义简单的相加的方法主要有“的”插入法和替换法两种，这在后面具体的讲解中会进行阐释。 （4）要考虑到切分的经济性。北大方案是切分和标注同时进行，为了保证标注符号使用的经济性，方案要求，要保证切分出来的单位尽量少的是无法独立成词的语素。因此，对于一个切分序列，如果我们切分后多出了无法独立成词的语素，比如说前接成分、后接成分等，我们尽可能地不去切分它。 2.分词实际情况中的应用接下来，我们将对分词方案的第四章、第五章结合我们总结出来的规则进行精简式的说明。 （1）人名 对于人名的切分，方案给出的切分标准是姓和名切分开。而对于其他称呼是否切分，可以用语义规则来解释。第二条规则：姓名后的职务、职称或称呼要分开。第四条规则：带明显排行的亲属称谓要切分开。这两条规则是因为组成的切分序列的意思即是各组成成分的组合义，因此要切分。而第三条规则：对人的简称、尊称若为两个字，则合为一个切分单位。不仅是因为这些切分序列的含义不是其组成成分的组合义，至少有表示尊敬的社会含义，还是因为如果切分，会多出无法独立成词的语素，因此把这些双音节作为一个切分单位。而对于外国人名和笔名、著名人名，我们不做切分，一是因为这种命名是随意的，切分下来的意义不大；二是因为著名人名是在语法词典中就规定了的内容。 （2）地名 大部分地名都是在语法词典中事先规定了的，除此以外的切分原则主要是和音节有关，如果地名后接的是单音节语素，则不切分；如果接的是双音节或多音节语素，则要进行切分。 （3）团体、机构、组织的专有名称 对于团体、机构、组织的专有名称，如果它们被语法词典收录，那么肯定不切分，如果没有，则要进行切分。（如果找不到这样合适的词典，一个PLAN B的建议：按照普通词组切分，再上游任务中再识别出来） （4）除人名、国名、地名、团体、机构、组织以外的其他专名 首先，我们还是要考虑其是否被语法词典收录。然后要考虑其后接语素的音节，如果是单音节的，如“人”“族”这样的，不切分，如果是多音节的，则要进行切分。 （5）数词与数量词组 数词与数量词组的规定是另外的。详见方案。 （6）时间词 时间词中登录在语法词典中的，比如历史朝代的名称，特殊的年份“甲午年”等，不做切分，其他的要按照“年、月、日、时、分、秒”的层次进行切分。 （7）单音节代词“本”、“每”、“各”、“诸 若后接成分是单音节名词，则不做切分，若是双音节或多音节，则要切分开。 （8）区别词 首先，我们要明确何为区别词，区别词指的是成对的，有分类性质的一类词，它们只能够做定语，不能做谓语，所以又称为非谓形容词。 举例来说，区别词包括：男、女、雌、雄、单、双、复、金、银、西式、中式、古代、近代、现代、当代、阴性、阳性、军用、民用、国有、私有、小型、中型、大型、微型、有期、无期、彩色、黑白、急性、慢性、小号、中号、大号、野生、家养、正式、非正式、人造（从动词过来的）、天然、冒牌、正牌、正版、盗版、下等、中等、上等、初级、中级、高级、中式、欧式等等。 对于含有区别词的序列，我们的切分原则也是同样按照音节来进行，如果区别词后接一个单音节名词，则不切分，若接的是多音节名词，则要切分。 （9）述补结构 简单来说，述补结构指的是描述一个动词发生的情貌或结果，即对动词所代表的事件进行的补充。对于双音节的述补结构我们的切分原则是，如果进行切分后，会有无法独立成词的语素存在，则不切分，反之，则切分。 述补结构中还有一类常见的多音节的“得”字补语，对于这类述补结构，我们可以将“得”字去掉，若去掉后依然能成词，则要将其切分；若不能成词，则“得”字补语整体作为一个分词单位，内部不做切分。 （10）、（11）、（12）、（13）略 （14）语素和非语素字的处理 对于离合词的离析形式，要进行切分。所谓离合词，指的是可以在组合的两个语素中插入其他成分的词，比如“吃饭”，它的离析形式有，“吃了饭”“吃了一个饭”等。 对于表示方位的双音节词，若切分出无法独立成词的语素，则不切分，否则则要进行切分。 （15）文本中非汉字的字符串 略 （16）重叠 重叠是汉语独特的语言现象之一。北大方案中对这类词的切分看似复杂，实质上是切分到能够独立使用的单位，并且要避免切分出不能单独成词的语素。 比如，“甜甜的蜂蜜”，由于“甜甜”不能单独成词，因此要切分到“甜甜的”。 而“试试看”由于“看”这里表示动作的尝试，作为这个意义并不能单独运用，因此不切分。 （17）附加成分 附加成分实质上指的是构词中的前缀和后缀。汉语构词法中有一类是依据词缀加词根进行的派生构词。对于这一类切分序列，除非其接入成分太多，会对其进行切分，否则不切分。比如“老师们”就不做切分，“苦苦追求而不得者”中的“者”由于统摄的成分太多，所以要单独切分开。 （18）复合词构词 在切分复合词的问题上，北大方案是存在讨论的余地的。由于复合词本身和短语之间的界限较为模糊，即使在语言学意义的界定上也会存在分歧，因此对于复合词类型的切分序列是否切分，实质上很难回答。北大方案给出的解决办法是，首先如果切分后会有无法独立成词的成分，那么就不切分；另外要判断这个复合词的意义是否只是组成成分的简单相加，如果是，那么就切分，如果不是，那就说明组成该词的两个成分之间意义是有相互渗透的联结的，就不能切分。但是如何判断复合词意义是否是组合成分的相加呢？ 这里的方法主要有两个，一个是加“的”法。这个方法主要针对的是定中结构的复合词，即一个语素修饰另一个语素。比如“白花”，和“白的花”意义一致，那么就要切分。 第二个方法是替换法，将复合词“AB”的A语素拿出来进行组词，再将B语素拿出来进行组词，若单独组词后其词义都是一样的，那么就说明复合词AB的词义是A语素义和B语素义的相加，因此要切分；若有A语素或B语素有和其他组词情况中语义不同的，那么就不切分复合词AB。 但是这两个方法并不能解决所有的复合词判断问题，因此到底是将问题简化还是对规则进一步细致，是值得思考的。 颗粒度方案（调整版）调整内容： 将原来的第一粒度作为细粒度（非常细，存在语义不透明的词缀），将第二粒度和第三粒度合并成为粗粒度），针对专有名词的问题，划出粗粒度2级（这个可以讨论，是在分词中一下子划分出来，还是在上游任务中再处理。在参考资料的专利中，他们往往在分词中就解决了）。 理清实体和专有名词的区别细粒度 单音词 单独一个语素即可成词的，如“火、书、水” 连绵词 必须和其他语素结合成词的，且结合的语素是固定的，如“葡萄”“乒乓” 音译词 包括了外国的专名（人名等） 数词 量词 比如：条、串、张 这里要注意一些从名词发展过来的量词，比如“碗” 这里包括度量：3/cm，7/天 另外细粒度中，时间数和时间单位也切分开，如：2018/年 不含行政区划的地名 比如：上海、北京、武汉 专有名词：机构、团体、组织 是一个封闭类，是不可类推的 包含上下隶属关系的团体机构专有名词，切分到最小的团体机构。比如“中国/银行/北京/分行”。 简称略语 方位词 语气词 叹词 实语素 包括北大方案里的形语素、名语素、动语素、人名中的姓氏，比如：锦（形语素） 虚语素 前接成分 比如“阿”“老”“非” 这类除了传统意义上的前缀，也要考虑一些网络流行语的临时构词产出的前缀 副语素 主要是否定副词，比如“不”“很” 后接成分 比如：们，儿（表亲昵的），子，头，化，者 我认为，还应包括行政区划的单位，比如：省、市、区等；和表示尊称的“老”“总” 助词 助动词、助数词 习语 包括成语、四字格短语、歇后语 但是如果歇后语有标点符号，要按照标点符号划分 比如：“不管三七二十一”“百尺竿头/，/更进一步” 粗粒度简言之：切到词组层，且注意音节数，对双音节放宽。将细粒度中可成词的组合成词（派生词），另将可独立成词的词根结合成复合词。粗粒度的切分目标是，使得每一个实词性的切分单位都是表义明确的分词单位，不存在语义不透明的分词单位。因此，我们也不能奢求实体识别等上游任务在分词任务中就得以解决。 前接成分+名词 比如：阿牛 前接成分+数 比如：阿大 名词+后接成分 比如：学生们、老师们、拳头、高清版 动词+后接成分 比如：创新化（单独“创新”还是分到”创新“） 姓氏+名 比如：张伟 数+量+（助数词） 比如：四/人，五个/人 时间 按北大方案，不要合并 比如：1997年/9月/3日，早/八点 复合词 双音节、三音节（切分原则详见对北大方案的讲解） 注意，不要将联合构词的词组算作复合词。 地名+行政区划 比如：北京市、上海市 地名+自然地形 比如：华北平原、南沙群岛 粗粒度下的切分难点1.专名和实体的切分专有名词指的是专指性的人名、地名、团体、机构、组织、民族、商标。 人名、地名、民族、商标基本上没有异议，但是哪些团体、机构、组织能算专有名词，哪些不能算是不太明确的。 另外，除上面指出的分类外，其他的具有专指性的实体，不能被当做专有名词来处理。具体来说，专有名词的切分难点有以下几点： （1） 专有名词的专指性是忽略文本语境。比如”校长办公室发布重要通知“，即使通过前文我们知道这里指的是北大的校长办公室，我们只将它作为普通名词的处理，而不是作为一个专指性的机构名来处理。 但是在国际或中国范围内的知名的唯一的团体、机构、组织 的名称我们依然将之处理为专名，比如“国务院”，它和“校长办公室”的区别在于“国务院”全国只有一个，而“校长办公室”有很多个，因此“国务院”作为专名不切分，而“校长办公室”要切分成“校长/办公室”。 （2）专有名词的组合性。专有名词有时会和其他名词一起组合成词。对于分词任务而言，我们只需考虑将专有名词和这个词切开后这个词能否单独成词，如果不能，那么就不切分，如果能，那么就切分。（这里和北大方案不同，北大方案认为接单音节可以切分，也可以不切分。）比如”满人“，”哈萨克人“，”昌平/分行“，而对于一些多个名词组合成专名的情况，比如“全国/总/工会”“全国/人民/代表/大会“，在细粒度和粗粒度中，由于它们音节数较多，视为普通名词进行切分。是否可以设置一个粗粒度2级，在粗粒度2级中，作为组织类专有名词，不切分。 （3）专有名词层次性。表示机构的专有名词中有些是前后相连，包含上下隶属关系的。下级机构的专指性有的是从由上级团体继承来的，比如“北京大学计算语言学研究所”是一个专指性的短语，它之所以有专指性，是因为“北京大学”这个专有名词的专指性，如果没有“北京大学”，则“计算语言学研究所”按照普通名词词组来切分（参照第一点）；有的是通过其他专有名词，如地名、人名获得的，比如“鲁迅研究院”，“北京分行”。在粗粒度中，对于获得专指性的专有名词不切分，如“鲁迅研究院”，“北京分行”是否可以设置粗粒度2级，表示上下级的专有名词全部纳入？比如“北京大学计算语言学研究所”，在粗粒度2级中就不做切分。 （4）电视节目、文艺作品（书、文档、协议）标题、电视剧、战争名等，不作为专有名词，按照普通名词划分。举例： 伊拉克/战争 辛亥/革命 平津/战役 开心/词典 新闻/30分 新闻/早/8点 中央电视台/-/1（它们后期可以通过书名号和引号识别出来。） 2.政治话语是否算作习语？（可以讨论）政治口号和政治思想由于在一定的历史时期中频繁使用，因此，如果切分表意就不一样。比如“中国特色社会主义思想”和“习近平新时代中国特色社会主义思想”就是两个概念。 有两个解决方案，一个是将音节较短的政治话语算作语法词典中的词，如“科技强国”“科教兴国”“绿色经济”，“科技创新”等等，然后遇到这样的词，细粒度、粗粒度里都不切分，而音节较长的，比如“中华民族伟大复兴”就作为普通名词进行切分；第二个解决方案是全部按照普通名词切分，到具体的任务需求时再处理。不过，我觉得这两个解决方案都会影响分词粒度整体的平衡度，因为政治口号构词有时非常非常长。 3.某某理论的名称算作专名吗？某某领域理论中的专业术语算作专名吗？理论的命名同样是任意性的命名行为，和菜名一样，如果对“xxx理论”中的“xxx”进行切分后，“xxx”的意思有所改变，那么就不能切分，如果没有改变，则可以切分。比如“精神分析/理论”，如果切分成“精神/分析”，这个“精神”和“你今天精神不佳”中的“精神”并不是一个意思，因此不能切分。而“牛顿/第二/定律”切分后没问题，因为这个理论的命名本身是组合而成的。 那么各个领域中的专业术语是否算作专名呢？我认为在通用型的分词中，只加入最为重要的一些专业术语；而在特定领域中，再在这方面进行拓展。因此，“社会生活”在社会学中应当算作一个专业术语，但是在通用型的分词中还是按照普通名词来进行切分，即“社会/生活”。 4.并列成分如何切分？并列成分按照顿号进行切分，比如“平津/、/辽沈/战役”，”张/、/李家“（这里的”张“可以看做是”张家“的缩略形式）。","link":"/2018/08/30/颗粒度分词调研/"}],"tags":[{"name":"命令行","slug":"命令行","link":"/tags/命令行/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"深度学习","slug":"深度学习","link":"/tags/深度学习/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"keras","slug":"keras","link":"/tags/keras/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"分词","slug":"分词","link":"/tags/分词/"}],"categories":[]}
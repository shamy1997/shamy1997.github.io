<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>好乐无荒</title>
  
  <subtitle>好乐无荒，良士休休</subtitle>
  <link href="http://shamy1997.github.io/atom.xml" rel="self"/>
  
  <link href="http://shamy1997.github.io/"/>
  <updated>2021-04-20T01:19:50.071Z</updated>
  <id>http://shamy1997.github.io/</id>
  
  <author>
    <name>Yuqiu Ji</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>linux中快速统计单词词频.md</title>
    <link href="http://shamy1997.github.io/passages/linux%E4%B8%8B%E6%9F%A5%E6%89%BE%E5%8D%95%E8%AF%8D/"/>
    <id>http://shamy1997.github.io/passages/linux%E4%B8%8B%E6%9F%A5%E6%89%BE%E5%8D%95%E8%AF%8D/</id>
    <published>2021-04-20T01:11:15.000Z</published>
    <updated>2021-04-20T01:19:50.071Z</updated>
    
    <content type="html"><![CDATA[<ol><li>使用vim统计</li></ol><p>用vim打开目标文件，在命令模式下，输入</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%s/word//gn</span><br></pre></td></tr></table></figure><ol><li>使用grep：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -o objStr  filename|wc -l</span><br></pre></td></tr></table></figure><p>统计多个字符串出现的次数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -o ‘objStr1\|objStr2<span class="string">'  filename|wc -l</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;使用vim统计&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;用vim打开目标文件，在命令模式下，输入&lt;/p&gt;
&lt;figure class=&quot;highlight vim&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;</summary>
      
    
    
    
    <category term="一日一技" scheme="http://shamy1997.github.io/categories/一日一技/"/>
    
    
    <category term="linux" scheme="http://shamy1997.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>文本相似度计算</title>
    <link href="http://shamy1997.github.io/passages/text-similarity-md/"/>
    <id>http://shamy1997.github.io/passages/text-similarity-md/</id>
    <published>2020-03-14T07:56:01.000Z</published>
    <updated>2021-03-21T08:42:07.660Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【翻译】文本相似度计算：如何估计两个文本之间相似程度？"><a href="#【翻译】文本相似度计算：如何估计两个文本之间相似程度？" class="headerlink" title="【翻译】文本相似度计算：如何估计两个文本之间相似程度？"></a>【翻译】文本相似度计算：如何估计两个文本之间相似程度？</h1><a id="more"></a><p>文本相似的计算场景有很多：</p><ul><li>搜索引擎：我们需要知道文本和搜索语句之间的关联程度；比如问答软件需要通过文本相似度计算给出数据库中依存的与用户输入文本最相近的条目。</li><li>法律事务：我们需要通过文本相似度计算检索出与新立法条最为接近的法律条文，确保条目之间不会有冲突。</li><li>咨询行业：AI 系统需要根据客户多种多样的问询给出一个标准回答。在这个系统中，AI 系统需要通过文本相似度计算辨别出具有不同表现形式但相同语义的问句，比如“我的快递怎么了”和“我的包裹怎么了”，用户应该得到的是相同的回答。</li></ul><p>那么什么是文本相似度计算呢？</p><!---more---><p>文本相似度，指的是两个文本片段在表层（词汇相似度）和含义（语义相似度）上两个方面上的相近程度。</p><ul><li>在表层上：如果你只考虑词汇级别的相似度，那么那些 4 个单词里有 3 个单词重合的句子都会被认为是相似的。这种方法不会考虑文本语境对词汇的词义带来的影响，也不会考虑整个句子的语义。</li><li>在含义上：我们不仅要关注词汇上的重复数目，还要关注上下文去更充分地捕捉词义。为了考虑语义相似度，我们要注重于篇章、段落层面上的语义挖掘。因为即使两句话词汇上完全一致，也可能有完全不一样的意思。</li></ul><p>我们知道，任何一个句子都有深层结构：</p><blockquote><p><strong>mouse</strong> <em>is the object of</em> <strong>ate</strong> <em>in the first case and</em> <strong>food</strong> <em>is the object of</em> <strong>ate</strong> <em>in the second case</em></p></blockquote><p>由于词序常常会导致语义的希望，我们希望我们的 sentence embedding 能对这种变化敏感。而幸运的是，我们的词向量经过多年的进化，已经能够区别<code>record the play</code>  和 <code>play the record</code> 的区别了。</p><h2 id="那么，我们的制胜秘诀是？"><a href="#那么，我们的制胜秘诀是？" class="headerlink" title="那么，我们的制胜秘诀是？"></a>那么，我们的制胜秘诀是？</h2><p>我们的大方向是，将文本表示成特征向量，然后比较文本之间的特征向量的距离来确定文本之间的相似度。我们有许多方式去计算文本的语义特征和深层结构。有监督的方法可以帮助 sentence embedding 更直接地学习到一个句子的语义。</p><p>下面，我们将比较计算语义相似度的最流行的集中做法，并比较他们的表现。</p><div class="table-container"><table><thead><tr><th>方法</th><th>效果</th></tr></thead><tbody><tr><td>Jaccard Similarity</td><td>😢😢😢</td></tr><tr><td>Different embedding + K-means</td><td>😢😢</td></tr><tr><td>Different embedding + Cosine Similarity</td><td>😢</td></tr><tr><td>Word2Vec + Smooth Inverse Frequency + Cosine Similarity</td><td>😊</td></tr><tr><td>Different embeddings + LSI + Cosine Similarity</td><td>😢</td></tr><tr><td>Different embeddings + LDA + Jensen-Shannon distance</td><td>😊</td></tr><tr><td>Different embeddings + Word Mover Distance</td><td>😊😊</td></tr><tr><td>Different embeddings + Variantional Auto Encoder(VAE)</td><td>😊😊</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td></tr></tbody></table></div>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;【翻译】文本相似度计算：如何估计两个文本之间相似程度？&quot;&gt;&lt;a href=&quot;#【翻译】文本相似度计算：如何估计两个文本之间相似程度？&quot; class=&quot;headerlink&quot; title=&quot;【翻译】文本相似度计算：如何估计两个文本之间相似程度？&quot;&gt;&lt;/a&gt;【翻译】文本相似度计算：如何估计两个文本之间相似程度？&lt;/h1&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="http://shamy1997.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读 | Deep Semantic Role Labeling:What Works and What’s Next</title>
    <link href="http://shamy1997.github.io/passages/srl-paper-reading-md/"/>
    <id>http://shamy1997.github.io/passages/srl-paper-reading-md/</id>
    <published>2019-04-10T07:10:50.000Z</published>
    <updated>2020-02-22T03:30:32.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是SRL"><a href="#什么是SRL" class="headerlink" title="什么是SRL"></a>什么是SRL</h2><p>Semantic Role Labeling 任务指的是围绕着谓词标记一句话的论元信息，识别出what，who，whom，when，where等信息。这是一项标记句子事件的浅层语义处理，不涉及句子的句法分析。比如对于“他昨天把书交给了张三”和“昨天书被他交给了张三”这两句话，它们在句法上不一样，但是在语义角色标注上是一样的。</p><a id="more"></a><p>语义角色标注是自然语言处理的底层任务，通过这项任务，我们可以直接获取到一句话事件性的信息，如果能够处理好，将对自动问答、自动文摘等任务产生直接而有力的帮助。</p><h2 id="本文的模型"><a href="#本文的模型" class="headerlink" title="本文的模型"></a>本文的模型</h2><p>Lu的模型之所以能够比原有的系统有那么大的提升，她认为主要原因是两方面，一方面是使用了优化过的BiLSTM模型，另一方面是对输出进行了优化编码。</p><p><img width="400" src="http://ww1.sinaimg.cn/large/62751203ly1g1xhlrv9d4j20sa0mutca.jpg"></p><h3 id="优化过的BiLSTM模型"><a href="#优化过的BiLSTM模型" class="headerlink" title="优化过的BiLSTM模型"></a>优化过的BiLSTM模型</h3><h4 id="Input、Output-amp-Function"><a href="#Input、Output-amp-Function" class="headerlink" title="Input、Output &amp; Function"></a>Input、Output &amp; Function</h4><p><img width="400" src="http://ww1.sinaimg.cn/large/62751203ly1g1xihhtogej213k0he10j.jpg"></p><ul><li><p>训练输入$(w,v)$。</p><ul><li>$w$代表词向量，本文使用的是GLoVe embedding，然后$v$代表是否是predicate（谓词），若是，则为1，否为0，两个都是100 dim。</li></ul></li><li><p>输出是y(BIO-tagger)</p></li><li>Scoring Function：<ul><li>$f(\boldsymbol{w}, \boldsymbol{y})=\sum_{t=1}^{n} \log p\left(y_{t} | \boldsymbol{w}\right)-\sum_{c \in \mathcal{C}} c\left(\boldsymbol{w}, y_{1 : t}\right)$</li><li>可能性减去惩罚值。因为输出的结果有一些限制，这些后面会讲。</li></ul></li><li>为使目标函数最大进行前向反馈和反向反馈进行训练。</li></ul><h4 id="BiLSTM"><a href="#BiLSTM" class="headerlink" title="BiLSTM"></a>BiLSTM</h4><p>BiLSTM 的内部构造就是简单的LSTM只不过叠加了两层，即一个单元会收到前词信息也会收到后词信息。</p><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{i}_{l, t} &=\sigma\left(\mathbf{W}_{\mathrm{i}}^{l}\left[\boldsymbol{h}_{l, t+\delta_{l}}, \boldsymbol{x}_{l, t}\right]+\boldsymbol{b}_{\mathrm{i}}^{l}\right) \\ \boldsymbol{o}_{l, t} &=\sigma\left(\mathbf{W}_{\mathrm{o}}^{l}\left[\boldsymbol{h}_{l, t+\delta_{l}}, \boldsymbol{x}_{l, t}\right]+\boldsymbol{b}_{\mathrm{o}}^{l}\right) \\ \boldsymbol{f}_{l, t} &=\sigma\left(\mathbf{W}_{\mathrm{f}}^{l}\left[\boldsymbol{h}_{l, t+\delta_{l}}, \boldsymbol{x}_{l, t}\right]+\boldsymbol{b}_{\mathrm{f}}^{l}+1\right) \\ \tilde{\boldsymbol{c}}_{l, t} &=\tanh \left(\mathbf{W}_{\mathrm{c}}^{l}\left[\boldsymbol{h}_{l, t+\delta_{l}}, \boldsymbol{x}_{l, t}\right]+\boldsymbol{b}_{\mathrm{c}}^{l}\right) \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{c}_{l, t} &=\boldsymbol{i}_{l, t} \circ \tilde{\boldsymbol{c}}_{l, t}+\boldsymbol{f}_{l, t} \circ \boldsymbol{c}_{t+\delta_{l}} \\ \boldsymbol{h}_{l, t} &=\boldsymbol{o}_{l, t} \circ \tanh \left(\boldsymbol{c}_{l, t}\right) \end{aligned}</script><script type="math/tex; mode=display">\boldsymbol{x}_{l, t}=\left\{\begin{array}{ll}{\left[\mathbf{W}_{\mathrm{emb}}\left(w_{t}\right), \mathbf{W}_{\mathrm{mask}}(t=v)\right]} & {l=1} \\ {\boldsymbol{h}_{l-1, t}} & {l>1}\end{array}\right.</script><script type="math/tex; mode=display">\delta_{l}=\left\{\begin{array}{ll}{1} & {\text { if } l \text { is even }} \\ {-1} & {\text { otherwise }}\end{array}\right.</script><p><img width="400" src="http://ww1.sinaimg.cn/large/62751203ly1g1xjba5x79j20ok0n0jw0.jpg"></p><h4 id="Recurrent-dropout"><a href="#Recurrent-dropout" class="headerlink" title="Recurrent dropout"></a>Recurrent dropout</h4><p>为了防止过拟合，我们会使用dropout的方法。过去的dropout我们大多使用随机生成，但是在这样复杂的网络中，如果采取之前的做法会让模型训练的噪声越来越大，为此，我们使用Recurrent dropout，这种dropout每层都是一样的（shared），因此可以减少噪声，达到防止过拟合的效果。</p><script type="math/tex; mode=display">\begin{aligned} \widetilde{\boldsymbol{h}}_{l, t} &=\boldsymbol{r}_{l, t} \circ \boldsymbol{h}_{l, t}^{\prime}+\left(1-\boldsymbol{r}_{l, t}\right) \circ \mathbf{W}_{\mathrm{h}}^{l} \boldsymbol{x}_{l, t} \\ \boldsymbol{h}_{l, t} &=\boldsymbol{z}_{l} \circ \widetilde{\boldsymbol{h}}_{l, t} \end{aligned}</script><p><img width="400" src="http://ww1.sinaimg.cn/large/62751203ly1g1xjeuagzzj20lm0nw43c.jpg"></p><h3 id="Constrained-A-decoding"><a href="#Constrained-A-decoding" class="headerlink" title="Constrained A* decoding"></a>Constrained A* decoding</h3><p>经过softmax层之后，我们会得到一个概率分布，但是并非选择概率最高的那个tag就是我们所要的tag，因为前词后词的tag选择并非独立，而是会相互影响的，换句话说，我们最后选择tag时会收到一些限制。</p><p>作者主要讲了有三种限制：</p><ul><li>第一是BIO标签体系的限制，比如I-tag不能在B-前面；</li><li>第二是语义角色上的限制，比如核心的语义角色AG0-AG5，在只有一个谓词的情况下，每个最多出现1次；</li><li>第三是句法上的限制，比如句法上不同在一个父节点中的两个论元不能被标记为B-X，I-X（X指有同样的语义角色）。</li></ul><p>针对这一问题，作者给出了一个惩罚函数来控制最后的分数，她希望选出在考虑了这些限制之后概率最大的结果。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>比以前的模型的F1提高了10%。并通过实验证明了：</p><ul><li>Deep-BiLSTM 可以很好地解决语义角色标注中长距离依存的问题；</li><li>训练时对权重进行随机正交分解能够使训练更快开始；</li><li>句法信息对语义角色标注是有用的，未来可以考虑在惩罚函数中优化，我觉得就是能将之前特征工程中所总结的一些条件规划到这个模型里来。</li></ul><h2 id="后续学习"><a href="#后续学习" class="headerlink" title="后续学习"></a>后续学习</h2><ul><li>这个算法模型已经被整合到AllenNLP中，可以学习下如何在本地使用；</li><li>如何迁移到中文任务中？</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://www.aclweb.org/anthology/papers/P/P17/P17-1044/" target="_blank" rel="noopener">论文</a></li><li><a href="https://www.youtube.com/watch?v=aptipHMTmmk" target="_blank" rel="noopener">Lu关于这个模型的talk</a>(油管)</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;什么是SRL&quot;&gt;&lt;a href=&quot;#什么是SRL&quot; class=&quot;headerlink&quot; title=&quot;什么是SRL&quot;&gt;&lt;/a&gt;什么是SRL&lt;/h2&gt;&lt;p&gt;Semantic Role Labeling 任务指的是围绕着谓词标记一句话的论元信息，识别出what，who，whom，when，where等信息。这是一项标记句子事件的浅层语义处理，不涉及句子的句法分析。比如对于“他昨天把书交给了张三”和“昨天书被他交给了张三”这两句话，它们在句法上不一样，但是在语义角色标注上是一样的。&lt;/p&gt;</summary>
    
    
    
    <category term="SRL" scheme="http://shamy1997.github.io/categories/SRL/"/>
    
    
    <category term="SRL" scheme="http://shamy1997.github.io/tags/SRL/"/>
    
    <category term="paper-reading" scheme="http://shamy1997.github.io/tags/paper-reading/"/>
    
    <category term="NLP" scheme="http://shamy1997.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>python面向对象笔记</title>
    <link href="http://shamy1997.github.io/passages/py-oop/"/>
    <id>http://shamy1997.github.io/passages/py-oop/</id>
    <published>2019-03-26T00:36:40.000Z</published>
    <updated>2020-02-22T03:33:52.236Z</updated>
    
    <content type="html"><![CDATA[<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>定义了<code>__init__</code>后就要传入其定义的所有参数。<code>__init__</code>中所定义的属性直接与实例想联。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name,score)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.score = score</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_score</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.score&gt;<span class="number">90</span>:</span><br><span class="line">            print(<span class="string">'A'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'B'</span>)</span><br></pre></td></tr></table></figure><h2 id="访问限制"><a href="#访问限制" class="headerlink" title="访问限制"></a>访问限制</h2><ul><li>以双划线<code>__</code>开头，<code>__</code>结尾的变量是特殊变量，特殊变量可以直接访问。</li><li>以一个下划线<code>_</code>开头的变量是保护变量，只允许其自身和子类使用，不能用<code>form xx import *</code>。</li><li>以两个下划线<code>__</code>开头的变量是private变量，虽然可以访问，但是按照惯例，应该视为私有变量，只允许这个类访问，不应该访问。</li><li>如果要访问和更改内部私有变量，可以在类中设置相应的<code>set_item</code>，<code>change_item</code>，<code>get_item</code>方法来获得。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name,score)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.score = score</span><br><span class="line">        self.grade = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_score</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.score &gt; <span class="number">90</span>:</span><br><span class="line">            self.grade = <span class="string">'A'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.grade = <span class="string">'-A'</span></span><br><span class="line">        <span class="keyword">return</span> self.grade</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_grade</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._score()</span><br></pre></td></tr></table></figure><h2 id="实例属性和类属性"><a href="#实例属性和类属性" class="headerlink" title="实例属性和类属性"></a>实例属性和类属性</h2><ul><li>实例属性属于各个实例所有，互不干扰；</li><li>类属性属于类所有，所有实例共享一个属性；</li><li>不要对实例属性和类属性使用相同的名字，否则将产生难以发现的错误!实例属性的值会覆盖类属性的值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加类属性</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span><span class="params">(object)</span>:</span></span><br><span class="line">    name = <span class="string">'animal'</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="继承和多态"><a href="#继承和多态" class="headerlink" title="继承和多态"></a>继承和多态</h2><ul><li>继承就是说，我创建一个新的类别的时候，object处填写已有的一个类别，新类别是子类别，旧类别是父类别。</li><li>子类别会继承旧类别的所有的属性和方法；</li><li>子类别可以更改父类别的方法（会覆盖父类别的方法）；</li><li>子类别可以添加新的方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'Animal is running...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Animal)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'Dog is running...'</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eat</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'Dog eats bones.'</span>)</span><br></pre></td></tr></table></figure><h2 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h2><p>简单来说，装饰器就是把另外一个函数当成参数传入到当前函数中并返回一个函数，这个函数会取代被装饰的函数。</p><h3 id="装饰器的运行过程"><a href="#装饰器的运行过程" class="headerlink" title="装饰器的运行过程"></a>装饰器的运行过程</h3><p>先将被修饰函数放入修饰器中的函数，然后正常运行修饰器函数，当调用被修饰函数时，执行被修饰函数。如果需要传入的被修饰函数需要传参，那么在调用的函数中也要设置参数位置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># It’s not black magic, you just have to let the wrapper </span></span><br><span class="line"><span class="comment"># pass the argument:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_decorator_passing_arguments</span><span class="params">(function_to_decorate)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_wrapper_accepting_arguments</span><span class="params">(arg1, arg2)</span>:</span></span><br><span class="line">        print(<span class="string">"I got args! Look: &#123;0&#125;, &#123;1&#125;"</span>.format(arg1, arg2))</span><br><span class="line">        function_to_decorate(arg1, arg2)</span><br><span class="line">    <span class="keyword">return</span> a_wrapper_accepting_arguments</span><br><span class="line"></span><br><span class="line"><span class="comment"># Since when you are calling the function returned by the decorator, you are</span></span><br><span class="line"><span class="comment"># calling the wrapper, passing arguments to the wrapper will let it pass them to </span></span><br><span class="line"><span class="comment"># the decorated function</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@a_decorator_passing_arguments</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_full_name</span><span class="params">(first_name, last_name)</span>:</span></span><br><span class="line">    print(<span class="string">"My name is &#123;0&#125; &#123;1&#125;"</span>.format(first_name, last_name))</span><br><span class="line"></span><br><span class="line">print_full_name(<span class="string">"Peter"</span>, <span class="string">"Venkman"</span>)</span><br><span class="line"><span class="comment"># outputs:</span></span><br><span class="line"><span class="comment">#I got args! Look: Peter Venkman</span></span><br><span class="line"><span class="comment">#My name is Peter Venkman</span></span><br></pre></td></tr></table></figure></p><p>如果是在类中使用呢？方法和函数的唯一区别只在于第一个参数是<code>self</code>。因此，我们只需要更改一下这个地方即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">method_friendly_decorator</span><span class="params">(method_to_decorate)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(self, lie)</span>:</span></span><br><span class="line">        lie = lie - <span class="number">3</span> <span class="comment"># very friendly, decrease age even more :-)</span></span><br><span class="line">        <span class="keyword">return</span> method_to_decorate(self, lie)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lucy</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.age = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @method_friendly_decorator</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sayYourAge</span><span class="params">(self, lie)</span>:</span></span><br><span class="line">        print(<span class="string">"I am &#123;0&#125;, what did you think?"</span>.format(self.age + lie))</span><br><span class="line"></span><br><span class="line">l = Lucy()</span><br><span class="line">l.sayYourAge(<span class="number">-3</span>)</span><br><span class="line"><span class="comment">#outputs: I am 26, what did you think?</span></span><br></pre></td></tr></table></figure><p>装饰器函数可以不带参数，也可以带参数。当装饰器函数带参数时，其实充当的是装饰器函数的构造器的功能，其携带的参数都可访问，但是不能被传入到「真正」的装饰器函数中，否则会起冲突！在定义装饰器时传入参数，我们可以更好地控制装饰器的行为。但是在构建简单的装饰器时，使用不带参数的即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">methord_with_args</span><span class="params">(arg1)</span>:</span></span><br><span class="line">    print(<span class="string">'here is my arg &#123;&#125;'</span>.format(arg1))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_fn</span><span class="params">(fn)</span>:</span></span><br><span class="line">        print(<span class="string">'run get(fn)'</span>)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(arg2,arg3,arg4)</span>:</span></span><br><span class="line">            print(<span class="string">'here is my decorator args: &#123;0&#125;,&#123;1&#125;,&#123;2&#125;'</span>.format(arg2,arg3,arg4))</span><br><span class="line">            fn(arg1,arg3)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> get_fn</span><br><span class="line"></span><br><span class="line"><span class="meta">@methord_with_args('hi')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">what</span><span class="params">(arg2,arg3)</span>:</span></span><br><span class="line">    print(<span class="string">'what'</span>,arg2,arg3)</span><br><span class="line"></span><br><span class="line">what(<span class="string">'2'</span>,<span class="string">'3'</span>,<span class="string">'4'</span>)</span><br><span class="line"><span class="comment"># what 函数传入三个参数而非两个，是因为它已经被装饰后的 wrapper 函数所取代了！</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## output:</span></span><br><span class="line"><span class="comment"># here is my arg hi</span></span><br><span class="line"><span class="comment"># run get(fn)</span></span><br><span class="line"><span class="comment"># here is my decorator args: 2,3,4</span></span><br><span class="line"><span class="comment"># what hi 3</span></span><br></pre></td></tr></table></figure><h3 id="常见的内置装饰器函数"><a href="#常见的内置装饰器函数" class="headerlink" title="常见的内置装饰器函数"></a>常见的内置装饰器函数</h3><h4 id="类方法-classmethod"><a href="#类方法-classmethod" class="headerlink" title="类方法@classmethod"></a>类方法<code>@classmethod</code></h4><p>在类中定义方法时使用<code>@classmethod</code>装饰器可以不创建实例就被使用。被类方法装饰器装饰的函数虽然<strong>不用传入<code>self</code>参数，但是仍需要传入一个<code>cls</code>参数来表示其自身</strong>。这个<code>cls</code>和<code>self</code>一样是不能传变量的。</p><h4 id="静态方法-staticmethod"><a href="#静态方法-staticmethod" class="headerlink" title="静态方法@staticmethod"></a>静态方法<code>@staticmethod</code></h4><p>静态方法可以不创建实例来调用类中的一个方法。静态方法不需要传入参数，但也可以传入参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Myclass</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'instance method called'</span>,self</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">staticmethod</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'staticmethod called'</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classmethod</span><span class="params">(cls,a)</span>:</span></span><br><span class="line">        a += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'classmethod called'</span>,a</span><br><span class="line"></span><br><span class="line">print(Myclass.staticmethod())</span><br><span class="line">print(Myclass.classmethod(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">## output</span></span><br><span class="line"><span class="comment"># staticmethod called</span></span><br><span class="line"><span class="comment"># ('classmethod called', 2)</span></span><br></pre></td></tr></table></figure><h4 id="property"><a href="#property" class="headerlink" title="@property"></a><code>@property</code></h4><h5 id="用途1：与私有变量配合使用，来定义只读属性。"><a href="#用途1：与私有变量配合使用，来定义只读属性。" class="headerlink" title="用途1：与私有变量配合使用，来定义只读属性。"></a>用途1：与私有变量配合使用，来定义<strong>只读属性</strong>。</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self._age = <span class="number">20</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">age</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._age</span><br><span class="line"></span><br><span class="line">a = Person(<span class="string">'张三'</span>)</span><br><span class="line">a.age</span><br><span class="line"><span class="comment"># 20</span></span><br><span class="line"></span><br><span class="line">a.age = <span class="number">30</span></span><br><span class="line"><span class="comment"># 会报错：AttributeError: can't set attribute</span></span><br></pre></td></tr></table></figure><h5 id="用途2：对类中的属性进行检查"><a href="#用途2：对类中的属性进行检查" class="headerlink" title="用途2：对类中的属性进行检查"></a>用途2：对类中的属性进行检查</h5><p>原来，我们要对类中属性的设置进行检查，会这样写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_score</span><span class="params">(self)</span>:</span></span><br><span class="line">         <span class="keyword">return</span> self._score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_score</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(value, int):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must be an integer!'</span>)</span><br><span class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span> <span class="keyword">or</span> value &gt; <span class="number">100</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must between 0 ~ 100!'</span>)</span><br><span class="line">        self._score = value</span><br></pre></td></tr></table></figure></p><p>然后调用的时候会这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = Student()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.set_score(<span class="number">60</span>) <span class="comment"># ok!</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.get_score()</span><br><span class="line"><span class="number">60</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.set_score(<span class="number">9999</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  ...</span><br><span class="line">ValueError: score must between <span class="number">0</span> ~ <span class="number">100</span>!</span><br></pre></td></tr></table></figure></p><p>但这样用两个方法太麻烦了，有了<code>@property</code>这个装饰器，就会方便很多。我们把一个<code>getter</code>方法变成属性，只需要加上<code>@property</code>就可以了，此时，<code>@property</code>本身又创建了另一个装饰器<code>@score.setter</code>，负责把一个<code>setter</code>方法变成属性赋值，于是，我们就拥有一个可控的属性操作:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._score</span><br><span class="line"></span><br><span class="line"><span class="meta">    @score.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(value, int):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must be an integer!'</span>)</span><br><span class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span> <span class="keyword">or</span> value &gt; <span class="number">100</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'score must between 0 ~ 100!'</span>)</span><br><span class="line">        self._score = value</span><br><span class="line">        </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = Student()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.score = <span class="number">60</span> <span class="comment"># OK，实际转化为s.set_score(60)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.score <span class="comment"># OK，实际转化为s.get_score()</span></span><br><span class="line"><span class="number">60</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.score = <span class="number">9999</span></span><br><span class="line">&gt;&gt;&gt;Traceback (most recent call last):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> ...</span><br><span class="line">&gt;&gt;&gt;ValueError: score must between <span class="number">0</span> ~ <span class="number">100</span>!</span><br></pre></td></tr></table></figure><p>参考资料：</p><ol><li><p><a href="https://stackoverflow.com/questions/739654/how-to-make-a-chain-of-function-decorators/1594484#1594484" target="_blank" rel="noopener">stackflow高赞讲解装饰器</a></p></li><li><p><a href="https://www.runoob.com/python/python-object.html" target="_blank" rel="noopener">菜鸟教程</a></p></li><li><p><a href="https://www.cnblogs.com/wangyueyouyi/p/8508967.html" target="_blank" rel="noopener">python装饰器的原理</a></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;初始化&quot;&gt;&lt;a href=&quot;#初始化&quot; class=&quot;headerlink&quot; title=&quot;初始化&quot;&gt;&lt;/a&gt;初始化&lt;/h2&gt;&lt;p&gt;定义了&lt;code&gt;__init__&lt;/code&gt;后就要传入其定义的所有参数。&lt;code&gt;__init__&lt;/code&gt;中所定义的属性直接与实例想联。&lt;/p&gt;</summary>
    
    
    
    <category term="代码精进" scheme="http://shamy1997.github.io/categories/代码精进/"/>
    
    
    <category term="python" scheme="http://shamy1997.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>北大分词方案解读及颗粒度分词方案</title>
    <link href="http://shamy1997.github.io/passages/%E9%A2%97%E7%B2%92%E5%BA%A6%E5%88%86%E8%AF%8D%E8%B0%83%E7%A0%94/"/>
    <id>http://shamy1997.github.io/passages/%E9%A2%97%E7%B2%92%E5%BA%A6%E5%88%86%E8%AF%8D%E8%B0%83%E7%A0%94/</id>
    <published>2019-01-30T06:23:49.000Z</published>
    <updated>2020-02-22T03:33:28.034Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>分词单位不同于语言学中的“词”，不同的算法下的分词结果千差万别，有的分出的是语言学意义上的词，而有的分出的是语言学意义上的“短语”（或者说“词组”）因此，我们希望寻找一个可理解的统一的粒度标准，而这个粒度标准能够实现对不同分词任务的不同层次的分词。</p></blockquote><a id="more"></a><h1 id="一、调研资料"><a href="#一、调研资料" class="headerlink" title="一、调研资料"></a>一、调研资料</h1><ol><li><a href="https://www.jianguoyun.com/p/DXc5BJwQhaz8Bhio1m4" target="_blank" rel="noopener">北大现代汉语语料库基本加工规范</a></li><li><a href="https://www.jianguoyun.com/p/DQfbcd4Qhaz8BhjF1m4" target="_blank" rel="noopener">计算所汉语词性标注集</a></li><li><a href="http://www.hankcs.com/nlp/corpus/several-revenue-segmentation-system-used-set-of-source-tagging.html" target="_blank" rel="noopener">几个开源分词系统所使用标注集的来源</a></li><li><a href="https://www.jianguoyun.com/p/DT74b1kQhaz8Bhje1m4" target="_blank" rel="noopener">海量中文智能分词接口手册</a></li><li><a href="https://patents.google.com/patent/CN102479191B" target="_blank" rel="noopener">阿里多粒度分词专利</a></li><li><a href="https://patents.google.com/patent/CN101246472A/zh" target="_blank" rel="noopener">腾讯多粒度分词专利</a></li><li><a href="https://patents.google.com/patent/CN103324626A/zh" target="_blank" rel="noopener">百度多粒度分词专利</a></li><li><a href="http://www.cnblogs.com/eaglet/archive/2008/05/27/1208423.html" target="_blank" rel="noopener">KTDictSeg 分词组件1.3版本 部分算法讨论 — 分词粒度</a></li></ol><h1 id="二、调研目的"><a href="#二、调研目的" class="headerlink" title="二、调研目的"></a>二、调研目的</h1><p>分词单位不同于语言学中的“词”，不同的算法下的分词结果千差万别，有的分出的是语言学意义上的词，而有的分出的是语言学意义上的“短语”（或者说“词组”）因此，我们希望寻找一个可理解的统一的粒度标准，而这个粒度标准能够实现对不同分词任务的不同层次的分词。为证实多颗粒度的分词标注确实能提高特定的分词任务的准确率，我们进行了这样的前期调研。<br>通过搜集资料，我们以北大方案为蓝本，以一定的语言学知识为基础，对分词颗粒进行不同粒度的划分。<br>首先对北大分词方案进行解读，然后再阐释我对分词粒度初步的构建想法。</p><p>注：颗粒度方案只考虑分词问题，不考虑词性标注。</p><h1 id="三、北大分词方案讲解"><a href="#三、北大分词方案讲解" class="headerlink" title="三、北大分词方案讲解"></a>三、北大分词方案讲解</h1><h2 id="1-分词单位的概念界定"><a href="#1-分词单位的概念界定" class="headerlink" title="1. 分词单位的概念界定"></a>1. 分词单位的概念界定</h2><p><code>分词单位</code>，“指信息处理中使用的、具有确定的语义和语法功能的基本单位”，该概念明确了其使用的特定环境——“信息处理任务”，以及其语义和语法功能明确的特点。</p><p>基于这样的概念划分，北大方案认定的分词单位里不仅包括了词，还“包括了一部分结合紧密、使用稳定的词组”，并且“在某些特殊情况孤立的语素或非语素字”。</p><p>事实上，我们撇开北大方案来看词这个整体，根据朱德熙先生的划分，可以分为可穷尽的虚词类和不可穷尽的实词类。虚词类，举例来说，包括连词、语气词、介词等，这类词可以在语法词典中被枚举出来，因此在进行分词时难度较小。因此，分词的困难常常出现在实词的切分上。</p><p>结合北大方案的划分，我认为对实词序列进行划分时，一般可以遵照以下原则：</p><p>（1）依据语法词典来划分，如果语法词典中进行规定，那么就不做划分。语言是约定俗成的产物，当某个词语组合被广泛而稳定地使用时，那么社会团体便会接受这样的一个“新词”，因此这样的一个词语组合也可以被视作是一个分词单位。而判断社会团体是否已经接受这一语言现象很显性的一大标志便是词典收录了该词条。那么问题就转变为，什么样的词典可以成为可供划分的语法词典。</p><p>（2）考虑切分序列的音节组合。汉语在发展过程中经历了一个从单音节向双音节的发展过程。虽然现代汉语以双音节为主要的成词单位，但是古代汉语中的一些单音节词依然残存在现代汉语中，并且在一些特殊语体中还广泛地存在着。因此，对于那些单音节成词的单位在标注时要格外注意标记出来，而处理多音节序列时，则要尽量保证分词结果以双音节为一个单位。</p><p>（3）考虑到词义与语素结合义。我们所认定的分词单位，它的词义是凝合而成的，而不是两个语素的意义简单的相加。因此，如果一个切分单位的语义是其切分单位意义的简单相加，那么就要对其进行切分。而判定是否是词义简单的相加的方法主要有“的”插入法和替换法两种，这在后面具体的讲解中会进行阐释。</p><p>（4）要考虑到切分的经济性。北大方案是切分和标注同时进行，为了保证标注符号使用的经济性，方案要求，要保证切分出来的单位尽量少的是无法独立成词的语素。因此，对于一个切分序列，如果我们切分后多出了无法独立成词的语素，比如说前接成分、后接成分等，我们尽可能地不去切分它。</p><h2 id="2-分词实际情况中的应用"><a href="#2-分词实际情况中的应用" class="headerlink" title="2.分词实际情况中的应用"></a>2.分词实际情况中的应用</h2><p>接下来，我们将对分词方案的第四章、第五章结合我们总结出来的规则进行精简式的说明。</p><p>（1）人名</p><p>对于人名的切分，方案给出的切分标准是姓和名切分开。而对于其他称呼是否切分，可以用语义规则来解释。第二条规则：姓名后的职务、职称或称呼要分开。第四条规则：带明显排行的亲属称谓要切分开。这两条规则是因为组成的切分序列的意思即是各组成成分的组合义，因此要切分。而第三条规则：对人的简称、尊称若为两个字，则合为一个切分单位。不仅是因为这些切分序列的含义不是其组成成分的组合义，至少有表示尊敬的社会含义，还是因为如果切分，会多出无法独立成词的语素，因此把这些双音节作为一个切分单位。而对于外国人名和笔名、著名人名，我们不做切分，一是因为这种命名是随意的，切分下来的意义不大；二是因为著名人名是在语法词典中就规定了的内容。</p><p>（2）地名</p><p>大部分地名都是在语法词典中事先规定了的，除此以外的切分原则主要是和音节有关，如果地名后接的是单音节语素，则不切分；如果接的是双音节或多音节语素，则要进行切分。</p><p>（3）团体、机构、组织的专有名称</p><p>对于团体、机构、组织的专有名称，如果它们被语法词典收录，那么肯定不切分，如果没有，则要进行切分。（如果找不到这样合适的词典，一个PLAN B的建议：按照普通词组切分，再上游任务中再识别出来）</p><p>（4）除人名、国名、地名、团体、机构、组织以外的其他专名</p><p>首先，我们还是要考虑其是否被语法词典收录。然后要考虑其后接语素的音节，如果是单音节的，如“人”“族”这样的，不切分，如果是多音节的，则要进行切分。</p><p>（5）数词与数量词组</p><p>数词与数量词组的规定是另外的。详见方案。</p><p>（6）时间词</p><p>时间词中登录在语法词典中的，比如历史朝代的名称，特殊的年份“甲午年”等，不做切分，其他的要按照“年、月、日、时、分、秒”的层次进行切分。</p><p>（7）单音节代词“本”、“每”、“各”、“诸</p><p>若后接成分是单音节名词，则不做切分，若是双音节或多音节，则要切分开。</p><p>（8）区别词</p><p>首先，我们要明确何为区别词，区别词指的是成对的，有分类性质的一类词，它们只能够做定语，不能做谓语，所以又称为非谓形容词。</p><p>举例来说，区别词包括：男、女、雌、雄、单、双、复、金、银、西式、中式、古代、近代、现代、当代、阴性、阳性、军用、民用、国有、私有、小型、中型、大型、微型、有期、无期、彩色、黑白、急性、慢性、小号、中号、大号、野生、家养、正式、非正式、人造（从动词过来的）、天然、冒牌、正牌、正版、盗版、下等、中等、上等、初级、中级、高级、中式、欧式等等。</p><p>对于含有区别词的序列，我们的切分原则也是同样按照音节来进行，如果区别词后接一个单音节名词，则不切分，若接的是多音节名词，则要切分。</p><p>（9）述补结构</p><p>简单来说，述补结构指的是描述一个动词发生的情貌或结果，即对动词所代表的事件进行的补充。对于双音节的述补结构我们的切分原则是，如果进行切分后，会有无法独立成词的语素存在，则不切分，反之，则切分。</p><p>述补结构中还有一类常见的多音节的“得”字补语，对于这类述补结构，我们可以将“得”字去掉，若去掉后依然能成词，则要将其切分；若不能成词，则“得”字补语整体作为一个分词单位，内部不做切分。</p><p>（10）、（11）、（12）、（13）略</p><p>（14）语素和非语素字的处理</p><p>对于离合词的离析形式，要进行切分。所谓离合词，指的是可以在组合的两个语素中插入其他成分的词，比如“吃饭”，它的离析形式有，“吃了饭”“吃了一个饭”等。</p><p>对于表示方位的双音节词，若切分出无法独立成词的语素，则不切分，否则则要进行切分。</p><p>（15）文本中非汉字的字符串  略</p><p>（16）重叠</p><p> 重叠是汉语独特的语言现象之一。北大方案中对这类词的切分看似复杂，实质上是切分到能够独立使用的单位，并且要避免切分出不能单独成词的语素。</p><p>比如，“甜甜的蜂蜜”，由于“甜甜”不能单独成词，因此要切分到“甜甜的”。</p><p>而“试试看”由于“看”这里表示动作的尝试，作为这个意义并不能单独运用，因此不切分。</p><p>（17）附加成分</p><p>附加成分实质上指的是构词中的前缀和后缀。汉语构词法中有一类是依据词缀加词根进行的派生构词。对于这一类切分序列，除非其接入成分太多，会对其进行切分，否则不切分。比如“老师们”就不做切分，“苦苦追求而不得者”中的“者”由于统摄的成分太多，所以要单独切分开。</p><p>（18）复合词构词</p><p>在切分复合词的问题上，北大方案是存在讨论的余地的。由于复合词本身和短语之间的界限较为模糊，即使在语言学意义的界定上也会存在分歧，因此对于复合词类型的切分序列是否切分，实质上很难回答。北大方案给出的解决办法是，首先如果切分后会有无法独立成词的成分，那么就不切分；另外要判断这个复合词的意义是否只是组成成分的简单相加，如果是，那么就切分，如果不是，那就说明组成该词的两个成分之间意义是有相互渗透的联结的，就不能切分。但是如何判断复合词意义是否是组合成分的相加呢？</p><p>这里的方法主要有两个，一个是加“的”法。这个方法主要针对的是定中结构的复合词，即一个语素修饰另一个语素。比如“白花”，和“白的花”意义一致，那么就要切分。</p><p>第二个方法是替换法，将复合词“AB”的A语素拿出来进行组词，再将B语素拿出来进行组词，若单独组词后其词义都是一样的，那么就说明复合词AB的词义是A语素义和B语素义的相加，因此要切分；若有A语素或B语素有和其他组词情况中语义不同的，那么就不切分复合词AB。</p><p>但是这两个方法并不能解决所有的复合词判断问题，因此到底是将问题简化还是对规则进一步细致，是值得思考的。</p><h2 id="颗粒度方案（调整版）"><a href="#颗粒度方案（调整版）" class="headerlink" title="颗粒度方案（调整版）"></a>颗粒度方案（调整版）</h2><p>调整内容：</p><ul><li>将原来的第一粒度作为细粒度（非常细，存在语义不透明的词缀），将第二粒度和第三粒度合并成为粗粒度），针对专有名词的问题，划出粗粒度2级（这个可以讨论，是在分词中一下子划分出来，还是在上游任务中再处理。在参考资料的专利中，他们往往在分词中就解决了）。</li><li>理清实体和专有名词的区别<h3 id="细粒度"><a href="#细粒度" class="headerlink" title="细粒度"></a>细粒度</h3></li><li>单音词<ul><li>单独一个语素即可成词的，如“火、书、水”</li></ul></li><li>连绵词<ul><li>必须和其他语素结合成词的，且结合的语素是固定的，如“葡萄”“乒乓”</li></ul></li><li>音译词<ul><li>包括了外国的专名（人名等）</li></ul></li><li>数词</li><li>量词<ul><li>比如：条、串、张</li><li>这里要注意一些从名词发展过来的量词，比如“碗”</li><li>这里包括度量：3/cm，7/天</li><li>另外细粒度中，时间数和时间单位也切分开，如：2018/年</li></ul></li><li>不含行政区划的地名<ul><li>比如：上海、北京、武汉</li></ul></li><li>专有名词：机构、团体、组织<ul><li>是一个封闭类，是不可类推的</li><li>包含上下隶属关系的团体机构专有名词，切分到最小的团体机构。比如“中国/银行/北京/分行”。</li></ul></li><li>简称略语</li><li>方位词</li><li>语气词</li><li>叹词</li><li>实语素<ul><li>包括北大方案里的形语素、名语素、动语素、人名中的姓氏，比如：锦（形语素）</li></ul></li><li>虚语素<ul><li>前接成分<ul><li>比如“阿”“老”“非”</li><li>这类除了传统意义上的前缀，也要考虑一些网络流行语的临时构词产出的前缀</li></ul></li><li>副语素<ul><li>主要是否定副词，比如“不”“很”</li></ul></li><li>后接成分<ul><li>比如：们，儿（表亲昵的），子，头，化，者</li><li>我认为，还应包括行政区划的单位，比如：省、市、区等；和表示尊称的“老”“总”</li></ul></li><li>助词<ul><li>助动词、助数词</li></ul></li></ul></li><li>习语<ul><li>包括成语、四字格短语、歇后语</li><li>但是如果歇后语有标点符号，要按照标点符号划分</li><li>比如：“不管三七二十一”“百尺竿头/，/更进一步”</li></ul></li></ul><h3 id="粗粒度"><a href="#粗粒度" class="headerlink" title="粗粒度"></a>粗粒度</h3><p>简言之：切到词组层，且注意音节数，对双音节放宽。将细粒度中可成词的组合成词（派生词），另将可独立成词的词根结合成复合词。<br>粗粒度的切分目标是，使得每一个实词性的切分单位都是表义明确的分词单位，不存在语义不透明的分词单位。因此，我们也不能奢求实体识别等上游任务在分词任务中就得以解决。</p><ul><li>前接成分+名词<ul><li>比如：阿牛</li></ul></li><li>前接成分+数<ul><li>比如：阿大</li></ul></li><li>名词+后接成分<ul><li>比如：学生们、老师们、拳头、高清版</li></ul></li><li>动词+后接成分<ul><li>比如：创新化（单独“创新”还是分到”创新“）</li></ul></li><li>姓氏+名<ul><li>比如：张伟</li></ul></li><li>数+量+（助数词）<ul><li>比如：四/人，五个/人</li></ul></li><li>时间<ul><li>按北大方案，不要合并</li><li>比如：1997年/9月/3日，早/八点</li></ul></li><li>复合词<ul><li>双音节、三音节（切分原则详见对北大方案的讲解）</li><li>注意，不要将联合构词的词组算作复合词。</li></ul></li><li>地名+行政区划<ul><li>比如：北京市、上海市</li></ul></li><li>地名+自然地形<ul><li>比如：华北平原、南沙群岛</li></ul></li></ul><h2 id="粗粒度下的切分难点"><a href="#粗粒度下的切分难点" class="headerlink" title="粗粒度下的切分难点"></a>粗粒度下的切分难点</h2><h3 id="1-专名和实体的切分"><a href="#1-专名和实体的切分" class="headerlink" title="1.专名和实体的切分"></a>1.专名和实体的切分</h3><p>专有名词指的是专指性的人名、地名、团体、机构、组织、民族、商标。</p><p>人名、地名、民族、商标基本上没有异议，但是哪些团体、机构、组织能算专有名词，哪些不能算是不太明确的。</p><p>另外，除上面指出的分类外，其他的具有专指性的实体，不能被当做专有名词来处理。具体来说，专有名词的切分难点有以下几点：</p><p>（1） 专有名词的专指性是忽略文本语境。比如”校长办公室发布重要通知“，即使通过前文我们知道这里指的是北大的校长办公室，我们只将它作为普通名词的处理，而不是作为一个专指性的机构名来处理。 <strong>但是在国际或中国范围内的知名的唯一的团体、机构、组织 的名称我们依然将之处理为专名</strong>，比如“国务院”，它和“校长办公室”的区别在于“国务院”全国只有一个，而“校长办公室”有很多个，因此“国务院”作为专名不切分，而“校长办公室”要切分成“校长/办公室”。</p><p> （2）专有名词的组合性。专有名词有时会和其他名词一起组合成词。对于分词任务而言，我们只需考虑将专有名词和这个词切开后这个词能否单独成词，如果不能，那么就不切分，如果能，那么就切分。（这里和北大方案不同，北大方案认为接单音节可以切分，也可以不切分。）比如”满人“，”哈萨克人“，”昌平/分行“，而对于一些多个名词组合成专名的情况，比如“全国/总/工会”“全国/人民/代表/大会“，在细粒度和粗粒度中，由于它们音节数较多，视为普通名词进行切分。是否可以设置一个<strong>粗粒度2级，在粗粒度2级中，作为组织类专有名词，不切分</strong>。</p><p>（3）专有名词层次性。表示机构的专有名词中有些是前后相连，包含上下隶属关系的。<strong>下级机构的专指性</strong>有的是从由上级团体继承来的，比如“北京大学计算语言学研究所”是一个专指性的短语，它之所以有专指性，是因为“北京大学”这个专有名词的专指性，如果没有“北京大学”，则“计算语言学研究所”按照普通名词词组来切分（参照第一点）；有的是通过其他专有名词，如地名、人名获得的，比如“鲁迅研究院”，“北京分行”。在粗粒度中，对于获得专指性的专有名词不切分，如“鲁迅研究院”，“北京分行”<strong>是否可以设置粗粒度2级，表示上下级的专有名词全部纳入？</strong>比如“北京大学计算语言学研究所”，在粗粒度2级中就不做切分。</p><p>（4）电视节目、文艺作品（书、文档、协议）标题、电视剧、战争名等，不作为专有名词，按照普通名词划分。举例：</p><ul><li>伊拉克/战争</li><li>辛亥/革命</li><li>平津/战役</li><li>开心/词典</li><li>新闻/30分</li><li>新闻/早/8点</li><li>中央电视台/-/1<br>（它们后期可以通过书名号和引号识别出来。）</li></ul><h3 id="2-政治话语是否算作习语？（可以讨论）"><a href="#2-政治话语是否算作习语？（可以讨论）" class="headerlink" title="2.政治话语是否算作习语？（可以讨论）"></a>2.政治话语是否算作习语？（可以讨论）</h3><p>政治口号和政治思想由于在一定的历史时期中频繁使用，因此，如果切分表意就不一样。比如“中国特色社会主义思想”和“习近平新时代中国特色社会主义思想”就是两个概念。</p><p>有两个解决方案，一个是将音节较短的政治话语算作语法词典中的词，如“科技强国”“科教兴国”“绿色经济”，“科技创新”等等，然后遇到这样的词，细粒度、粗粒度里都不切分，而音节较长的，比如“中华民族伟大复兴”就作为普通名词进行切分；第二个解决方案是全部按照普通名词切分，到具体的任务需求时再处理。不过，我觉得这两个解决方案都会影响分词粒度整体的平衡度，因为政治口号构词有时非常非常长。</p><h3 id="3-某某理论的名称算作专名吗？某某领域理论中的专业术语算作专名吗？"><a href="#3-某某理论的名称算作专名吗？某某领域理论中的专业术语算作专名吗？" class="headerlink" title="3.某某理论的名称算作专名吗？某某领域理论中的专业术语算作专名吗？"></a>3.某某理论的名称算作专名吗？某某领域理论中的专业术语算作专名吗？</h3><p>理论的命名同样是任意性的命名行为，和菜名一样，如果对“xxx理论”中的“xxx”进行切分后，“xxx”的意思有所改变，那么就不能切分，如果没有改变，则可以切分。比如“精神分析/理论”，如果切分成“精神/分析”，这个“精神”和“你今天精神不佳”中的“精神”并不是一个意思，因此不能切分。而“牛顿/第二/定律”切分后没问题，因为这个理论的命名本身是组合而成的。</p><p>那么各个领域中的专业术语是否算作专名呢？我认为在通用型的分词中，只加入最为重要的一些专业术语；而在特定领域中，再在这方面进行拓展。因此，“社会生活”在社会学中应当算作一个专业术语，但是在通用型的分词中还是按照普通名词来进行切分，即“社会/生活”。</p><h3 id="4-并列成分如何切分？"><a href="#4-并列成分如何切分？" class="headerlink" title="4.并列成分如何切分？"></a>4.并列成分如何切分？</h3><p>并列成分按照顿号进行切分，比如“平津/、/辽沈/战役”，”张/、/李家“（这里的”张“可以看做是”张家“的缩略形式）。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;分词单位不同于语言学中的“词”，不同的算法下的分词结果千差万别，有的分出的是语言学意义上的词，而有的分出的是语言学意义上的“短语”（或者说“词组”）因此，我们希望寻找一个可理解的统一的粒度标准，而这个粒度标准能够实现对不同分词任务的不同层次的分词。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="http://shamy1997.github.io/tags/NLP/"/>
    
    <category term="分词" scheme="http://shamy1997.github.io/tags/分词/"/>
    
  </entry>
  
  <entry>
    <title>金融句法标注实践</title>
    <link href="http://shamy1997.github.io/passages/biaozhu-md/"/>
    <id>http://shamy1997.github.io/passages/biaozhu-md/</id>
    <published>2019-01-22T06:09:38.000Z</published>
    <updated>2020-02-22T06:21:22.385Z</updated>
    
    <content type="html"><![CDATA[<p>在上周的标注过程中，我们针对2000条金融新闻语料进行了标注。</p><a id="more"></a><p>起先，我们希望针对金融语料进行一些领域特定性的修改，但是由于语料的语体一致，利用新闻语料训练出来的自动标注系统的正确率还是不错的，并没有特别需要因为是金融领域而与原设定的依存方案产生冲突的地方。</p><p>但是，这次的标注依然具有一定的难度，主要体现在下面几个方面：</p><p>第一，短语内部层次的依存标注问题。金融语料中，公司名和一些其他有长定语修饰的专有名词非常多；另外，缩写词参与的短语依存标注也容易被忽视，需要标注者先对缩写词进行正确地分词，而后再进行依存标注。</p><p>第二，是一词多义、词类活用现象造成的干扰。</p><p>首先是一词多义的现象，即一个词它又两种词性，两种词性之间的语义或有关联或无关联。拿“公告”来举例，在没有其他成分充当核心谓词的情况下，它是动词当谓语，而在有其他动词时，比如“某公司公告称，……”就要考虑这里的“公告”是动词和“称”并列，还是作为一个名词呢？在实际的处理中，我们将之处理为动词与“称”并列，这是因为虽然“公司公告”中可以认为“公司”是“公告”的定语，但是作为名词的“公告”却并不能作为“称”的执行者，换言之，“称”所具有的“+人”的属性与“公告”“-人”的语义产生了冲突。但在另一种情况中，“公司发布公告称”，“公告”明显是作为名词充当“发布”的宾语了。</p><p>再就是词类活用的现象，这主要出现在长定语的判断中，就是一个一个动词活用作形容词来修饰名词（这边暂时没想到相关的例子），词类活用与一词多义的区别在于，“动词活用作形容词”的现象是临时性的，因而该动词依然是动词，所以我们无需改变其词性标签，而在标注其依存关系时按实际标注即可。</p><p>第三，是宾语和补语的混淆，这一点主要是对动词的及物性有所犹豫。汉语动词的按及物性，可以分为及物、不及物、两者皆可这三类。如果是及物动词，那么必须要带宾语，如果是不及物动词后面跟了名词必然不能当宾语，而是充当补语，如果可以是及物动词也可以是不及物动词，那么要分清其什么语义情景中是及物动词，什么语义情景中是不及物动词，而后在检查那么名词是否符合其语义，再判断是宾语还是补语。例如“较前年上涨10%”，这里“10%”和“上涨”就是补语的关系，因为“上涨”是不及物的。</p><p>此外，感觉上句法依存确实会损失一些信息，比如单句单句间的联系全部以并列结构处理，又比如由于规则限制，句子的核心谓语定在了信息量并不多的“称”“报道”“认为”这类词上，而根据句子的信息包装，新信息基本都在句子后半部分，如果我能对这项任务的上游操作有具体的了解，我想我也许对方案的调整会有更大的操作空间吧。</p><h2 id="附详细答疑总结："><a href="#附详细答疑总结：" class="headerlink" title="附详细答疑总结："></a>附详细答疑总结：</h2><h3 id="是-是否-形容词"><a href="#是-是否-形容词" class="headerlink" title="是/是否+形容词"></a>是/是否+形容词</h3><p>这是系表结构，系表结构在框架内似乎只能处理成动宾关系。</p><p>原因是否与……有关</p><p>原因&lt;-是否，SBV，是否-&gt;有关，VOB，与&lt;-有关，ADV</p><h3 id="A和-与B相比"><a href="#A和-与B相比" class="headerlink" title="A和/与B相比"></a>A和/与B相比</h3><p>A与B相比，净利润下降7%。</p><p>A&lt;-相比，SBV ； 与-&gt;B, POB; 与&lt;-相比，ADV；“A与B相比”做后句的状语。</p><h3 id="A至B"><a href="#A至B" class="headerlink" title="A至B"></a>A至B</h3><p>A&lt;-至，SBV，至-&gt;B,POB</p><h3 id="无-V-N"><a href="#无-V-N" class="headerlink" title="无+V+N"></a>无+V+N</h3><p>无销售条件股份</p><p>无-&gt;条件，VOB，销售&lt;-条件，ATT，无&lt;-股份，ATT</p><h3 id="主语是个从句"><a href="#主语是个从句" class="headerlink" title="主语是个从句"></a>主语是个从句</h3><p>[刚泰集团持有刚泰控股股份] 共计1.96亿元。</p><p>这里应该是“刚泰集团持有刚泰控股股份”这样的小句做主语，然后“共计”是谓语，“1.96亿股”是宾语。</p><h3 id="冒号"><a href="#冒号" class="headerlink" title="冒号"></a>冒号</h3><p>本次互保金额：合计总额不超过20亿元人命币。</p><p>假设没有冒号，按照主谓谓语的标法，本次互保金额&lt;-合计总额，ATT。但是这里有了冒号，“本次互保金额”必须和前面的相连接</p><h3 id="两个动词"><a href="#两个动词" class="headerlink" title="两个动词"></a>两个动词</h3><p>两个动词V1和V2同时出现，可以分词以下集中情况：</p><h4 id="V1-V2"><a href="#V1-V2" class="headerlink" title="V1+V2"></a>V1+V2</h4><p>我们唱歌跳舞。</p><p>V1-&gt;V2，COO</p><h4 id="V1-N1-V2"><a href="#V1-N1-V2" class="headerlink" title="V1+N1+V2"></a>V1+N1+V2</h4><p>带他走</p><p>这是一个兼语句，带-&gt;他，VOB，带-&gt;走，VOB</p><h4 id="V1-V2-N2"><a href="#V1-V2-N2" class="headerlink" title="V1+V2+N2"></a>V1+V2+N2</h4><p>动词按照是否需要宾语可以分成三类，一类是必须要带宾语，一类是一定不能带宾语，一类是既可以带宾语也可以不带宾语。</p><p>如果V1一定要带宾语，那么V2+N2是V1的宾语从句，其他情况，V1-&gt;V2，COO。</p><p>(例子待补充)</p><h4 id="V1-N1-V2-N2"><a href="#V1-N1-V2-N2" class="headerlink" title="V1+N1+V2+N2"></a>V1+N1+V2+N2</h4><ul><li>若N1既是V1的宾语又是V2的宾语，则是N1是兼语，V1-&gt;N1，VOB，V1-&gt;V2，VOB，V2-&gt;N2，VOB。</li><li>若N1和N2分别是V1、V2的宾语，则是两个并列的动宾短语，V1-&gt;V2,COO</li></ul><h3 id="注意-观察-了解-……到"><a href="#注意-观察-了解-……到" class="headerlink" title="注意/观察/了解/……到"></a>注意/观察/了解/……到</h3><p>注意-&gt;到  CMP</p><h3 id="提供全额连带责任"><a href="#提供全额连带责任" class="headerlink" title="提供全额连带责任"></a>提供全额连带责任</h3><h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc55gumid3j30fu0ge755.jpg" alt="图片"></h2><h3 id="给"><a href="#给" class="headerlink" title="给"></a>给</h3><ul><li>动词</li></ul><p>实意的“给予”意。小明给我一块钱。给-&gt;钱，VOB</p><ul><li>介词</li></ul><p>表示动作发出的方向。他昨天把这本书卖给小张了。给-&gt;小张，卖-&gt;给，CMP</p><h3 id="累计-共计-合计"><a href="#累计-共计-合计" class="headerlink" title="累计/共计/合计"></a>累计/共计/合计</h3><ul><li>放在名词前面的时候，作为形容词来修饰名词</li></ul><p>累计金额为……   累计&lt;-金额，ATT</p><ul><li>放在名词后面时，作动词，若后面还有动词，与之并列</li></ul><p>金额累计约为……  金额&lt;-累计，SBV   累计-&gt;约，COO   </p><ul><li>放在动词前，做状语修饰动词</li></ul><p>亨利达公司共计生产1000只皮包。 共计&lt;-生产，SDV</p><h3 id="对外担保"><a href="#对外担保" class="headerlink" title="对外担保"></a>对外担保</h3><p>违规对外担保   </p><p>违规&lt;-担保 ADV  对外&lt;-担保 ADV</p><h3 id="xxx公告-11月1日消息"><a href="#xxx公告-11月1日消息" class="headerlink" title="xxx公告/11月1日消息"></a>xxx公告/11月1日消息</h3><p>A公司公告，明天公司发红包。</p><p>11月1日消息，明天公司发红包。</p><p>A公司&lt;-公告，SBV   公告-&gt;发，COO</p><p>11月1日&lt;-消息，SBV  消息-&gt;发，COO</p><p>区分</p><p>A公司公告称，明天公司发红包。</p><p>A公司-&gt;公告，SBV 公告-&gt;称，COO  称-&gt;发，VOB</p><p>A公司发布公告称，明天公司发红包。</p><p>A公司-&gt;发布，SBV   发布-&gt;公告，VOB， 发布-&gt;称，COO</p><h3 id="公司（002）……"><a href="#公司（002）……" class="headerlink" title="公司（002）……"></a>公司（002）……</h3><p>002&lt;-公司，COO</p><h3 id="拟……V1"><a href="#拟……V1" class="headerlink" title="拟……V1"></a>拟……V1</h3><p>公司拟发行第二批债券</p><p>拟-&gt;发行,ADV</p><h3 id="为"><a href="#为" class="headerlink" title="为"></a>为</h3><ul><li>为表示“是”的意思的时候，是动词</li></ul><p>今年的营销总额为1亿元。root-&gt;为,HED </p><ul><li>表示目的的，是介词</li></ul><p>为迎接圣诞，学校…… 为-&gt;迎接，POB</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在上周的标注过程中，我们针对2000条金融新闻语料进行了标注。&lt;/p&gt;</summary>
    
    
    
    
    <category term="labeling" scheme="http://shamy1997.github.io/tags/labeling/"/>
    
  </entry>
  
  <entry>
    <title>终端命令笔记</title>
    <link href="http://shamy1997.github.io/passages/about-terminal/"/>
    <id>http://shamy1997.github.io/passages/about-terminal/</id>
    <published>2018-12-22T12:37:49.000Z</published>
    <updated>2020-02-22T03:31:03.607Z</updated>
    
    <content type="html"><![CDATA[<p>常用记录</p><a id="more"></a><h1 id="vi-常用命令"><a href="#vi-常用命令" class="headerlink" title="vi 常用命令"></a>vi 常用命令</h1><h2 id="1-进入-vi-编辑器"><a href="#1-进入-vi-编辑器" class="headerlink" title="1. 进入 vi 编辑器"></a>1. 进入 vi 编辑器</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vi</span> <span class="symbol">&lt;path&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-修改内容"><a href="#2-修改内容" class="headerlink" title="2. 修改内容"></a>2. 修改内容</h2><p>输入<code>i</code>，进入<code>insert</code>模式。<br>按<code>esc</code>，退出模式。</p><h2 id="3-保存，退出"><a href="#3-保存，退出" class="headerlink" title="3. 保存，退出"></a>3. 保存，退出</h2><ul><li><code>:w</code> 保存文件但不退出vi;</li><li><code>:wq</code> 保存文件并退出vi;</li><li><code>q:</code> 不保存文件，退出vi</li><li><code>:e!</code> 放弃所有修改，从上次保存文件开始再编辑</li></ul><p><a href="https://www.cnblogs.com/mondol/p/vi-examples.html" target="_blank" rel="noopener">vi命令大全</a> </p><h1 id="Jupyter-or-conda-not-found"><a href="#Jupyter-or-conda-not-found" class="headerlink" title="Jupyter or conda not found"></a>Jupyter or conda not found</h1><p>我安装好anaconda后，打算用命令行直接打开jupyter notebook，结果却没有成功，网上一般的解释是要把anaconda配置到环境变量里：<br>在终端中输入：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi ~<span class="string">/.bash_profile</span></span><br></pre></td></tr></table></figure><p>打开后在末尾加上：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="string">'~/anaconda/bin:$PATH'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的path要根据anaconda所在的位置定义</span></span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 表示修改立即生效</span></span><br></pre></td></tr></table></figure><p>但是呢，我试了好几次都没有成功，事实上，是我配置了<code>oh-my-zsh</code>的原因。</p><p>因此，正确的解决方法是，打开<code>~/.zshrc</code>，然后在文件最后一行添加：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="variable">$PATH</span>:$HOME/anaconda/bin</span><br></pre></td></tr></table></figure><p>保存文件后，关闭窗口，重新开启窗口时，输入命令<code>conda --v</code>来检测是否成功。</p><p><a href="https://stackoverflow.com/questions/18675907/how-to-run-conda" target="_blank" rel="noopener">参考链接🔗</a> </p><h1 id="zsh-not-found"><a href="#zsh-not-found" class="headerlink" title="zsh not found"></a>zsh not found</h1><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exec <span class="regexp">/bin/</span>zsh</span><br></pre></td></tr></table></figure><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH=<span class="regexp">/bin:/u</span>sr<span class="regexp">/bin:/u</span>sr<span class="regexp">/local/</span>bin:<span class="variable">$&#123;PATH&#125;</span></span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure><p><a href="https://www.jiloc.com/43492.html" target="_blank" rel="noopener">参考资料🔗</a> </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;常用记录&lt;/p&gt;</summary>
    
    
    
    
    <category term="命令行" scheme="http://shamy1997.github.io/tags/命令行/"/>
    
  </entry>
  
  <entry>
    <title>理解Word Embedding（1）：从Count Vector到word2vec</title>
    <link href="http://shamy1997.github.io/passages/%E7%90%86%E8%A7%A3Word%20Embedding%EF%BC%881%EF%BC%89%EF%BC%9A%E4%BB%8ECount%20Vector%E5%88%B0word2vec/"/>
    <id>http://shamy1997.github.io/passages/%E7%90%86%E8%A7%A3Word%20Embedding%EF%BC%881%EF%BC%89%EF%BC%9A%E4%BB%8ECount%20Vector%E5%88%B0word2vec/</id>
    <published>2018-12-18T02:27:04.000Z</published>
    <updated>2020-02-22T03:33:39.025Z</updated>
    
    <content type="html"><![CDATA[<p>参考博客链接: 🔗 <a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">这个</a></p><h2 id="1-什么是Word-Embedding"><a href="#1-什么是Word-Embedding" class="headerlink" title="1. 什么是Word Embedding"></a>1. 什么是Word Embedding</h2><p>在机器学习和深度学习的任务中，我们都无法直接处理字符串或平文本，所以需要通过一种编码方式将其处理为数值，Word Embedding 就是这样将文本处理成数值的一类方法。</p><a id="more"></a><h2 id="2-不同的Word-Embedding-类型"><a href="#2-不同的Word-Embedding-类型" class="headerlink" title="2. 不同的Word Embedding 类型"></a>2. 不同的Word Embedding 类型</h2><h3 id="2-1-基于频率的Word-Embedding"><a href="#2-1-基于频率的Word-Embedding" class="headerlink" title="2.1. 基于频率的Word Embedding"></a>2.1. 基于频率的Word Embedding</h3><h4 id="2-1-1-Count-Vectors"><a href="#2-1-1-Count-Vectors" class="headerlink" title="2.1.1 Count Vectors"></a>2.1.1 Count Vectors</h4><p>假设一个一个语料库C有D个文本片段{d1,d2,d3,…dD} 以及N个从语料库C中提取的token。这N个token将会形成我们的词典，这样我们设定的Count Vector 大小便是 DxN。在矩阵M中，每行都包含着每个文本片段的token出现的频率。</p><p>让我们通过一个简单的例子来理解。</p><p>D1: He is  lazy boy. She is also lazy.</p><p>D2: Neeraj is a lazy person.</p><p>假设我们的语料库就仅有这两句话组成，那么我们的词典即为[‘He’,’She’,’lazy’,’boy’,’Neeraj’,’person’]。这里，D=2，N=6。</p><p>那么我们2x6的矩阵将被表示为：</p><div class="table-container"><table><thead><tr><th></th><th>He</th><th>She</th><th>lazy</th><th>boy</th><th>Neeraj</th><th>person</th></tr></thead><tbody><tr><td>D1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>0</td><td>0</td></tr><tr><td>D2</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td></tr></tbody></table></div><p>这样，每一纵列便可被认为是每个单词的词向量。比如，lazy的词向量就是[2,1]，其他单词的词向量以此类推。在上图这个矩阵中，行对应着语料库中的一个个文本片段，列对应着词典中的一个个token。我们要像这样阅读这个矩阵。D2 包含了’lazy’：一次，’Neeraj’：一次以及’person’：一次。</p><p>然而，在准备上面这个矩阵M时，可能有一些变体。这些变体的变化之处在于：</p><ul><li>准备词典的方式</li></ul><p>你可以会疑惑为何准备词典时我们也要加以变动？事实上，在实际应用中，我们的语料库可能包含着成百上千个文本片段。那么我们就需要从这成百上千的文本片段中提取出独特的token，那么这势必会导致我们所得出的例如上图的矩阵非常稀疏，且计算时非常低效。因此，一个可选的解决方法是，我们将基于频率选取比方前10000个单词来作为我们的词典，然后再基于这个词典来构建我们的矩阵。</p><ul><li>计算单词频次的方式</li></ul><p>在计数时，其实我们有两种选择，一种是计算频率，即一个单词在这个文本中的次数，一种是计算是否出现，即一个单词如果在这个文中出现则为1，否则为0。但是一般情况下，我们还是倾向于使用前者。</p><p>下图是矩阵M的示意图，方便你理解：</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hk5tjp1j30g20afwfu.jpg" alt="image-20190702163004826"></p><h4 id="2-1-2-TF-IDF-vectorization"><a href="#2-1-2-TF-IDF-vectorization" class="headerlink" title="2.1.2 TF-IDF vectorization"></a>2.1.2 TF-IDF vectorization</h4><p>TF-IDF vectorization 是另一种基于频率的表示方式，但是它与 Count Vector不同在于它所考虑的不仅仅是一个单词在单个文本片段中的出现频次，而是考虑它在整个语料中的出现频率。所以，这背后有何合理性呢？让我们试着理解这一点。</p><p>比较常见的单词，例如”is”,”the”,”a”等和那些对于文本片段更为重要的片段相比往往出现得更为频繁。比如”the”这种单词在各个文本片段中都有出现，而”Harry Potter”可能只出现在《哈利波特》这部小说有关的文本片段里，但是对于这些片段来说，”Harry Potter”显然比”the”更重要，因为它把这些文本和其他文本区别开。于是我们希望降低这些较为常见的单词的权重并且更加重视那些文本片段中独特的单词。</p><p>TF-IDF就可以做到上面这一点。那么TD-IDF是如何工作的呢？</p><p>如下图所示，假设我们有这样一个表格，第一列是文本中的token，第二列是出现的频次。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73ht7n9doj30fz05j74n.jpg" alt="image-20190702163027515"></p><p>首先，我们先来定义一下TF-IDF相关的一些术语：</p><ul><li>TF：文本中term T出现的次数/文本的term总数</li></ul><p>因此，TF(This,Doucument1)=1/8,TF(This,Document2)=1/5。</p><p>TF表示了这个单词对这个文本的贡献程度，比如说和文本更为相关的单词，它的TF值会比较大，因为它会更高频地出现在文本中。</p><ul><li>DF：log(语料库中的文本总数/语料库中含有term T的文本数)</li></ul><p>因此，IDF(this)=log(2/2)=0。</p><p>理论上来说，如果一个单词在语料库所有的单词中都出现了，那么可能这个单词对于某个或某些特定的文本并不重要，即是我们所说的那类比较常见的单词。但是如果一个单词只出现语料库的一个子集的文本中出现，那么这个单词对于那些文本具有一定的相关性。比如IDF(Messi)=log(2/1)=0.301。</p><p>由此可见，对于文本片段1而言，TF-IDF方法狠狠地处罚了”this”但是却给予”Messi”更高的权重。因此这个方法能够帮助我们更好的理解”Messi”是文本片段1的一个重要单词。</p><h4 id="2-1-3-具有固定上下文窗口的共现矩阵"><a href="#2-1-3-具有固定上下文窗口的共现矩阵" class="headerlink" title="2.1.3 具有固定上下文窗口的共现矩阵"></a>2.1.3 具有固定上下文窗口的共现矩阵</h4><p>指导思想：相似的单词往往一起出现并且具有相似的文本环境。比如”Apple is a fruit”,”Mango is a fruit”，苹果和芒果倾向于有一个相似的上下午，如”fruit”。</p><p>在我深入一个共现矩阵是如何构建的细节之前，我们有必要先理清两个概念。</p><ul><li>Co-occurence：对于一个给定的语料库，一对单词的共现，比方说w1和w2的共现，就是在一个上下文窗口中它们共同出现的次数。</li><li>Context Window：上下文窗口的参数由数字和方向设定。我们举个例子来帮助理解。</li></ul><div class="table-container"><table><thead><tr><th><em>Quick</em></th><th><em>brown</em></th><th><u>fox</u></th><th><em>jump</em></th><th><em>over</em></th><th>the</th><th>lazy</th><th>dog</th></tr></thead><tbody><tr><td>Quick</td><td>brown</td><td><em>fox</em></td><td><em>jump</em></td><td><u>Over</u></td><td><em>The</em></td><td><em>lazy</em></td><td>dog</td></tr></tbody></table></div><p>这个表格第一行的斜体是fox的长度为2的上下文窗口，第二行的斜体是over的长度为2的上下文窗口。</p><p>现在，我们编一个用来计算共现矩阵的语料库。</p><p>语料库：He is not lazy. He is intelligent. He is smart.</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hkdhmrqj30f606tt8w.jpg" alt="image-20190628161726114"></p><p>让我们通过看上面两个红、蓝着色的例子来理解共现矩阵。</p><p>红色格子所表示的，是”He”和”is”在长度为2的上下文窗口中出现的次数，红色格子中的数字为4，我们可以通过下面的表格可视化这个计数过程。（即出现无需紧挨着，只要都在窗口中，即使顺序颠倒，都是可以的）。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hkhhazfj30ku05m74j.jpg" alt="image-20190628162117450"></p><p>蓝色格子所表示的，是”lazy”和”intelligent”在长度为2的上下文窗口中出现的次数，其中数字为0，表示它们从不曾同时出现在一个上下文窗口中。</p><h5 id="共现矩阵的变体"><a href="#共现矩阵的变体" class="headerlink" title="共现矩阵的变体"></a>共现矩阵的变体</h5><p>假设语料库中有V个独特的单词，那么我们的词汇长度即为V。下面给出了共现矩阵的两种不同变体：</p><ul><li>大小为VxV的共现矩阵。但是由于这样的矩阵太大而难于计算，所以实际中这类建模并不被看好。</li><li>大小为VxN的共现矩阵。N表示去除掉停用词等不相关词汇后的V的一个自己。但是这类建模也依然很大且难于计算。</li></ul><p>这里要强调一点，共现矩阵并非是我们广泛使用的词向量。相反，这类共现矩阵常常与诸如PCA，SVD这样的技术组合使用在因素分解上。而这些因素分解的组合构成了词向量表示。</p><p>说得更明白些，你在下面VxV的共现矩阵上使用了PCA，你会得到V个主元素。如此，你可以从这V个元素中选出k个。因此，你讲得到一个Vxk的新剧证。这样，虽然一个单词是由k维表示的而不是V维表示的，但是它依然能捕捉到几乎同样的意义。</p><p>因此，PCA背后的操作实际上就是讲共现矩阵拆解为三个矩阵，U，S和V。</p><h4 id="共现矩阵的优点"><a href="#共现矩阵的优点" class="headerlink" title="共现矩阵的优点"></a>共现矩阵的优点</h4><ol><li>它蕴含了单词之间的语义联系。比如”男人”和”女人”会比”男人”和”苹果”更近。</li><li>它的核心在于使用SVD来创造出比现存方法更准确的词向量表示。</li><li>它使用因子分解，因子分解是一个良定义问题并且可以被有效解决。</li><li>它只要被计算一次，之后任何时候都可以被使用。在这个意义上，它比其他方法更快。</li></ol><h4 id="共现矩阵的缺点"><a href="#共现矩阵的缺点" class="headerlink" title="共现矩阵的缺点"></a>共现矩阵的缺点</h4><ol><li>它需要巨大的内存去存储。</li></ol><h2 id="2-2-基于预测的word-embedding"><a href="#2-2-基于预测的word-embedding" class="headerlink" title="2.2 基于预测的word embedding"></a>2.2 基于预测的word embedding</h2><p>前面所说的基于频率的word embedding有各种各样的局限。而后Mitolov等人将word2vec介绍到nlp的各个领域中，使得基于预测的word embedding走上历史舞台。这些方法是基于预测的，也就是说它们会给出各个单词的概率。它们在词汇相似度的任务上表现得非常好，甚至能达到King -man +woman = Queen这样的神奇效果。下面，我们就来看看word2vec具体是如何得出词向量的。</p><p>word2vec并不是一个单独的算法，它是由CBOW和Skip-gram模型组合而成的。这两个模型都是由词到词的浅层神经网络组成的。它们所学习的权重将成为单词的向量表示。接下来，我们就分别介绍这两种模型。</p><h3 id="2-2-1-CBOW-连续词袋模型"><a href="#2-2-1-CBOW-连续词袋模型" class="headerlink" title="2.2.1 CBOW 连续词袋模型"></a>2.2.1 CBOW 连续词袋模型</h3><p>CBOW是基于语境（文本上下文）来预测出一个单词的概率。这个语境可能是一个单词或者一组词。但为了介绍的简单，我们这边以一个单词作为语境并一次来预测一个目标词汇。</p><p>假设我们有一个语料库C=”Hey, this is sample corpus using only one context word.”。并且我们已经定义了上下文窗口大小为1。这个语料库将会被转换成下图所示的训练集合。输入如下图所示。下图中右边的矩阵包含了左边的输入的独热编码。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hkl49p1j31880deafz.jpg" alt="image-20190701085559787"></p><p>比如说is的目标的输入为[0001000000]。</p><p>上图所显示的矩阵将会被送入一个由三层组成的浅层神经网络：一个输入层，一个隐藏层和一个输出层。输出层会使用softmax函数，softmax函数是一个用于分类预测的常用函数，这个函数得出的各个类别的数值总和为1。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hkoy5zej30bi06paal.jpg" alt="image-20190701141942061"></p><p>流程是这样的：</p><ol><li>输入层和目标层都是由1xV的独热编码组成，这里V=10。</li><li>这里有两套权重，一套是输入层和隐藏层之间的权重，一套是隐藏层和输出层之间的权重。input-hidden 层矩阵大小为VxN, hidden-output 那层的矩阵大小为NXV ：这里，N指的是我们选择用来表达单词的维度。它是任意的，并且是神经网络的一个超参数。并且，N是也是隐藏层的节点数，这里，我们设N=4。</li><li>任意一层之间都不存在激活函数。</li><li>输入与 input-hidden层权重的乘积被称为 hidden activation。</li><li>Hidden input 与hidden-output层权重相乘，得到输出。</li><li>输出层和目标之间的误差将会被用来进行反向传播，以调整weights。</li><li>在隐藏层和输出层之间权重将会成为词向量。</li></ol><p>上面的流程是针对于上下文窗口为1的，下图显示了上下文窗口大于1的情况。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hksmslgj308o08hq3e.jpg" alt="image-20190701144531128"></p><p>下图是为了更好理解这个结构的矩阵图例。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hkwsehxj30oz04laah.jpg" alt="image-20190702163209378"></p><p>如上图所示，我们要使用3个 context word去预测目标单词的概率。输入层是3个[1xV]的向量，而输出是一个[1xV]。剩下的构造和1-context的CBOW是一样的。</p><p>但是在隐藏层中，3-context word的模型不再是简单复制输入，而是要进行一个平均。我们可以通过上面的图来理解这一点，如果我们有3个context word, 那么我们将会有3个 初步的hidden activation 然后最后平均得到最终的 hidden activiation。</p><p>那么CBOW和一般的MLP之间有何不同呢？</p><ul><li>CBOW的目标函数和MLP不同，CBOW是目标函数负的最大似然。</li><li>误差梯度不一样，因为MLP以sigmoid作为激活函数，而CBOW一般是线性激活。</li></ul><p>CBOW的优点：</p><ul><li>基于概率的，更符合实际；</li><li>占用内存小。</li></ul><p>CBOW的缺点：</p><ul><li>CBOW是利用单词的语境来表示单词的，但是对于多义词而言，比如苹果既是水果也是一家公司，但是由于CBOW将这个文本都考虑进去，所以苹果被表示在水果和公司之间了。</li><li>从头训练一个CBOW若没有很好优化的话将训练很久。</li></ul><h3 id="2-2-2-Skip-Gram-model"><a href="#2-2-2-Skip-Gram-model" class="headerlink" title="2.2.2 Skip-Gram model"></a>2.2.2 Skip-Gram model</h3><p>Skip-gram 的类型与CBOW一样，但是它的结构是反过来的，它是基于给定单词去预测单词的上下文。我们依然用讲解CBOW时使用的语料。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hl3etyzj30aa09emxu.jpg" alt="image-20190702163240931"></p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g73hl6oauhj309g06t750.jpg" alt="image-20190701152814848"></p><p>Skip-gram的输入和1-context的CBOW 模型很类似。并且隐藏层的activitiaon也是一样的。不同的仅仅是目标变量。因为我们已经在单词两边都定义了一个长度为1的上下文窗口，所以我们会有两个独热编码的目标变量和两个相应的输出。</p><p>而这两个目标变量的误差将会被分别计算，然后将两个误差加起来进行反向传播。</p><h4 id="Skip-Gram-模型的优点"><a href="#Skip-Gram-模型的优点" class="headerlink" title="Skip-Gram 模型的优点"></a>Skip-Gram 模型的优点</h4><ul><li>可以抓住一个单词的多个义项。</li><li>使用负采样的Skip-Gram模型会比其他方法更高效。</li></ul><h4 id="Skip-Gram模型的缺点"><a href="#Skip-Gram模型的缺点" class="headerlink" title="Skip-Gram模型的缺点"></a>Skip-Gram模型的缺点</h4><ul><li>依赖语料库的大小</li><li>采样是对统计数据的低效利用</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;参考博客链接: 🔗 &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这个&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-什么是Word-Embedding&quot;&gt;&lt;a href=&quot;#1-什么是Word-Embedding&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是Word Embedding&quot;&gt;&lt;/a&gt;1. 什么是Word Embedding&lt;/h2&gt;&lt;p&gt;在机器学习和深度学习的任务中，我们都无法直接处理字符串或平文本，所以需要通过一种编码方式将其处理为数值，Word Embedding 就是这样将文本处理成数值的一类方法。&lt;/p&gt;</summary>
    
    
    
    <category term="NLP" scheme="http://shamy1997.github.io/categories/NLP/"/>
    
    
    <category term="Word Embedding" scheme="http://shamy1997.github.io/tags/Word-Embedding/"/>
    
  </entry>
  
  <entry>
    <title>Slot Filling with SimpleRNN</title>
    <link href="http://shamy1997.github.io/passages/slot-filling/"/>
    <id>http://shamy1997.github.io/passages/slot-filling/</id>
    <published>2018-12-10T02:20:20.000Z</published>
    <updated>2020-02-22T03:33:58.223Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是Slot-Filling？"><a href="#什么是Slot-Filling？" class="headerlink" title="什么是Slot Filling？"></a>什么是Slot Filling？</h1><p>Slot Filling是自然语言理解中的一个基本问题，是对语言含义的简单化处理，它的思想类似于语言学中框架主义的一派，先设定好特定的语言类型槽，再将输入的单词一一填入槽内，而获取言语含义的时候即是根据语义槽的含义进行提取和检索。我们这里的任务就是将表示定购航班（ATIS数据集）这一言语行为的一系列语句填入各种类型的语义槽中。</p><a id="more"></a><h1 id="为什么使用SimpleRNN"><a href="#为什么使用SimpleRNN" class="headerlink" title="为什么使用SimpleRNN?"></a>为什么使用SimpleRNN?</h1><p>Slot Filling属于RNN应用中一对一的应用，通过训练模型，每个词都能被填到合适的槽中。<br>RNN和一般的神经网络的不同在于，在RNN中，我们在时间t的输出不仅取决于当前的输入和权重，还取决于之前的输入，而对于其他神经网络模型，每个时刻的输入和输出都是独立而随机的，没有相关性。放到我们要处理语义理解的问题上看，语言作为一种基于时间的线性输出，显然会受到前词的影响，因此我们选取RNN模型来进行解决这个问题。<br>这里选取SimpleRNN,是因为这个RNN比较简单，能达到熟悉框架的练习效果，之后可以选取其他有效的RNN模型，如LSTMS进行优化。</p><h1 id="构建思路一览："><a href="#构建思路一览：" class="headerlink" title="构建思路一览："></a>构建思路一览：</h1><ul><li>载入数据，使用的是<a href="https://github.com/chsasank/ATIS.keras" target="_blank" rel="noopener">chsasank</a>修改的<a href="https://github.com/mesnilgr/is13" target="_blank" rel="noopener">mesnilgr</a>的load.py。</li><li>定义模型。采取Keras中的序列模型搭建，首先使用一个100维的word embedding层将输入的单词转化为高维空间中的一个向量（在这个空间中，语义和语法位置越近的单词的距离越小），然后我们构建一个dropout层防止过拟合，设置SimpleRNN层，设置TimeDistributed层以完成基于时间的反向传播。最后我们将这些层组织在一起，并确定optimizer和loss function。我们选取的optimizer是rmsprop,这样在训练后期依然能找到较有项，而选取categorical_crossentropy作为损失函数，则是因为处理的问题性质适合于此。</li><li>训练模型。出于对计算资源的考虑，我们一般使用minibtach的方法批量对模型进行训练。但是我们这里的数据是一句句话，如果按照一个固定的batch_size将其分裂，可能增加了不必要的联系（因为上下两句话是独立的），因此我们将一句话作为一个batch去进行训练、验证以及预测，并手动算出一个epoch的平均误差。</li><li>评估和预测模型。我们通过观察验证误差和预测F1精度来对模型进行评估。预测F1精度使用的是<a href="https://github.com/sighsmile/conlleval" target="_blank" rel="noopener">signsmile</a>编写的conlleval.py。</li><li>保存模型。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.layers.recurrent <span class="keyword">import</span> SimpleRNN</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense,Dropout</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> keras.layers.wrappers <span class="keyword">import</span> TimeDistributed</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> data.load</span><br><span class="line"><span class="keyword">from</span> metrics.accuracy <span class="keyword">import</span> evaluate</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><h1 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_set,valid_set,dicts = data.load.atisfull()</span><br><span class="line"><span class="comment"># print(train_set[:1])</span></span><br><span class="line"><span class="comment"># dicts = &#123;'label2idx':&#123;&#125;,'words2idx':&#123;&#125;,'table2idx':&#123;&#125;&#125;</span></span><br><span class="line">w2idx,labels2idx = dicts[<span class="string">'words2idx'</span>],dicts[<span class="string">'labels2idx'</span>]</span><br><span class="line">train_x,_,train_label = train_set</span><br><span class="line">val_x,_,val_label = valid_set</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">idx2w = &#123;w2idx[i]:i <span class="keyword">for</span> i <span class="keyword">in</span> w2idx&#125;</span><br><span class="line">idx2lab = &#123;labels2idx[i]:i <span class="keyword">for</span> i <span class="keyword">in</span> labels2idx&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_classes = len(idx2lab)</span><br><span class="line">n_vocab = len(idx2w)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">words_train = [[idx2w[i] <span class="keyword">for</span> i <span class="keyword">in</span> w[:]] <span class="keyword">for</span> w <span class="keyword">in</span> train_x]</span><br><span class="line">labels_train = [[idx2lab[i] <span class="keyword">for</span> i <span class="keyword">in</span> w[:]] <span class="keyword">for</span> w <span class="keyword">in</span> train_label]</span><br><span class="line"></span><br><span class="line">words_val = [[idx2w[i] <span class="keyword">for</span> i <span class="keyword">in</span> w[:]] <span class="keyword">for</span> w <span class="keyword">in</span> val_x]</span><br><span class="line"><span class="comment"># labels_val = [[idx2lab[i] for i in w[:]] for w in val_label]</span></span><br><span class="line">labels_val =[]</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> val_label:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> w[:]:</span><br><span class="line">        labels_val.append(idx2lab[i])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Real Sentence : &#123;&#125;'</span>.format(words_train[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'Encoded Form : &#123;&#125;'</span>.format(train_x[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Real Label : &#123;&#125;'</span>.format(labels_train[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'Encoded Form : &#123;&#125;'</span>.format(train_label[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>Real Sentence : [&#39;i&#39;, &#39;want&#39;, &#39;to&#39;, &#39;fly&#39;, &#39;from&#39;, &#39;boston&#39;, &#39;at&#39;, &#39;DIGITDIGITDIGIT&#39;, &#39;am&#39;, &#39;and&#39;, &#39;arrive&#39;, &#39;in&#39;, &#39;denver&#39;, &#39;at&#39;, &#39;DIGITDIGITDIGITDIGIT&#39;, &#39;in&#39;, &#39;the&#39;, &#39;morning&#39;]Encoded Form : [232 542 502 196 208  77  62  10  35  40  58 234 137  62  11 234 481 321]========================================Real Label : [&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-fromloc.city_name&#39;, &#39;O&#39;, &#39;B-depart_time.time&#39;, &#39;I-depart_time.time&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-toloc.city_name&#39;, &#39;O&#39;, &#39;B-arrive_time.time&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-arrive_time.period_of_day&#39;]Encoded Form : [126 126 126 126 126  48 126  35  99 126 126 126  78 126  14 126 126  12]</code></pre><h1 id="Define-and-Compile-the-model"><a href="#Define-and-Compile-the-model" class="headerlink" title="Define and Compile the model"></a>Define and Compile the model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Embedding(n_vocab,<span class="number">100</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line">model.add(SimpleRNN(<span class="number">100</span>,return_sequences=<span class="keyword">True</span>))</span><br><span class="line">model.add(TimeDistributed(Dense(n_classes,activation=<span class="string">'softmax'</span>)))</span><br><span class="line">model.compile(optimizer = <span class="string">'rmsprop'</span>,loss = <span class="string">'categorical_crossentropy'</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================embedding_1 (Embedding)      (None, None, 100)         57200     _________________________________________________________________dropout_1 (Dropout)          (None, None, 100)         0         _________________________________________________________________simple_rnn_1 (SimpleRNN)     (None, None, 100)         20100     _________________________________________________________________time_distributed_1 (TimeDist (None, None, 127)         12827     =================================================================Total params: 90,127Trainable params: 90,127Non-trainable params: 0_________________________________________________________________</code></pre><h1 id="Train-the-model"><a href="#Train-the-model" class="headerlink" title="Train the model"></a>Train the model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_the_model</span><span class="params">(n_epochs,train_x,train_label,val_x,val_label)</span>:</span></span><br><span class="line">    epoch,train_avgloss,val_avgloss,f1s = [],[],[],[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n_epochs+<span class="number">1</span>):</span><br><span class="line">        epoch.append(i)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## training</span></span><br><span class="line">        train_avg_loss =<span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> n_batch,sent <span class="keyword">in</span> enumerate(train_x):</span><br><span class="line">            label = train_label[n_batch]</span><br><span class="line">            <span class="comment"># label to one-hot</span></span><br><span class="line">            label = to_categorical(label,num_classes=n_classes)[np.newaxis,:]</span><br><span class="line">            sent = sent[np.newaxis,:]</span><br><span class="line">            loss = model.train_on_batch(sent,label)</span><br><span class="line">            train_avg_loss += loss</span><br><span class="line">            </span><br><span class="line">        train_avg_loss = train_avg_loss/n_batch</span><br><span class="line">        train_avgloss.append(train_avg_loss)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## evaluate&amp;predict</span></span><br><span class="line">        val_pred_label,pred_label_val,val_avg_loss  = [],[],<span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> n_batch,sent <span class="keyword">in</span> enumerate(val_x):</span><br><span class="line">            label = val_label[n_batch]</span><br><span class="line">            label = to_categorical(label,num_classes=n_classes)[np.newaxis,:]</span><br><span class="line">            sent = sent[np.newaxis,:]</span><br><span class="line">            loss = model.test_on_batch(sent,label)</span><br><span class="line">            val_avg_loss += loss</span><br><span class="line">            </span><br><span class="line">            pred = model.predict_on_batch(sent)</span><br><span class="line">            pred = np.argmax(pred,<span class="number">-1</span>)[<span class="number">0</span>]</span><br><span class="line">            val_pred_label.append(pred)</span><br><span class="line">            </span><br><span class="line">        val_avg_loss = val_avg_loss/n_batch</span><br><span class="line">        val_avgloss.append(val_avg_loss)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> val_pred_label:</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> w[:]:</span><br><span class="line">                pred_label_val.append(idx2lab[k])</span><br><span class="line">            </span><br><span class="line">        prec, rec, f1 = evaluate(labels_val,pred_label_val, verbose=<span class="keyword">False</span>)</span><br><span class="line">        print(<span class="string">'Training epoch &#123;&#125;\t train_avg_loss = &#123;&#125; \t val_avg_loss = &#123;&#125;'</span>.format(i,train_avg_loss,val_avg_loss))</span><br><span class="line">        print(<span class="string">'precision: &#123;:.2f&#125;% \t recall: &#123;:.2f&#125;% \t f1 :&#123;:.2f&#125;%'</span>.format(prec,rec,f1))</span><br><span class="line">        print(<span class="string">'-'</span>*<span class="number">60</span>)</span><br><span class="line">        f1s.append(f1)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment">#     return epoch,pred_label_train,train_avgloss,pred_label_val,val_avgloss</span></span><br><span class="line">    <span class="keyword">return</span> epoch,f1s,val_avgloss,train_avgloss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">epoch,f1s,val_avgloss,train_avgloss = train_the_model(<span class="number">40</span>,train_x,train_label,val_x,val_label)</span><br></pre></td></tr></table></figure><p><strong>输出：</strong><br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  Training epoch <span class="number">1</span> train_avg_loss = <span class="number">0.5546463992293973</span>  val_avg_loss = <span class="number">0.4345020865901363</span></span><br><span class="line">  precision: <span class="number">84.79</span>%  recall: <span class="number">80.79</span>%  f1 :<span class="number">82.74</span>%</span><br><span class="line">  ------------------------------------------------------------</span><br><span class="line">  Training epoch <span class="number">2</span> train_avg_loss = <span class="number">0.2575569036037627</span>  val_avg_loss = <span class="number">0.36228470020366654</span></span><br><span class="line">  precision: <span class="number">86.64</span>%  recall: <span class="number">83.86</span>%  f1 :<span class="number">85.22</span>%</span><br><span class="line">  ------------------------------------------------------------</span><br><span class="line">  Training epoch <span class="number">3</span> train_avg_loss = <span class="number">0.2238766908014994</span>  val_avg_loss = <span class="number">0.33974187403771694</span></span><br><span class="line">  precision: <span class="number">88.03</span>%  recall: <span class="number">85.55</span>%  f1 :<span class="number">86.77</span>%</span><br><span class="line">  ------------------------------------------------------------</span><br><span class="line">……</span><br><span class="line">     ------------------------------------------------------------</span><br><span class="line">  Training epoch <span class="number">40</span> train_avg_loss = <span class="number">0.09190682124901069</span>  val_avg_loss = <span class="number">0.2697056618613356</span></span><br><span class="line">  precision: <span class="number">92.51</span>%  recall: <span class="number">91.47</span>%  f1 :<span class="number">91.99</span>%</span><br><span class="line">  ------------------------------------------------------------</span><br></pre></td></tr></table></figure></p><h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><p>观察验证误差，选取合适的epoch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">plt.xlabel=(<span class="string">'epoch'</span>)</span><br><span class="line">plt.ylabel=(<span class="string">'loss'</span>)</span><br><span class="line">plt.plot(epoch,train_avgloss,<span class="string">'b'</span>)</span><br><span class="line">plt.plot(epoch,val_avgloss,<span class="string">'r'</span>,label=(<span class="string">'validation error'</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ohj9e0ect.bkt.clouddn.com/blog/180911/5A7mAF88bL.png?imageslim" alt="mark"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'最大f1值为 &#123;:.2f&#125;%'</span>.format(max(f1s)))</span><br></pre></td></tr></table></figure><pre><code>最大f1值为 92.56%</code></pre><h1 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">'slot_filling_with_simpleRNN.h5'</span>)</span><br></pre></td></tr></table></figure><h1 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h1><p>使用SimpleRNN最终得到的F1值为92.56%，和师兄的95.47%相比确实还相差很多。这主要是和我们模型的选取有关，SimpleRNN只能将前词的影响带入到模型中，但是语言中后词对前词也会有一定的影响，因此可以通过选择更加复杂的模型或者增加能够捕捉到后词信息的层来进行优化。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://chsasank.github.io/spoken-language-understanding.html" target="_blank" rel="noopener">Keras Tutorial - Spoken Language Understanding</a></li><li><a href="https://github.com/czs0x55aa/pytorch-slot-filling/blob/master/evaluate.py" target="_blank" rel="noopener">pytorch-slot-filling</a></li><li><a href="https://github.com/liu946/AtisSlotLabeling" target="_blank" rel="noopener">liu946 AtisSlotLabeling</a></li><li><a href="https://blog.csdn.net/winteeena/article/details/78997053" target="_blank" rel="noopener">【Keras情感分类】训练过程中出现的问题汇总</a></li><li><a href="https://keras.io/zh/layers/recurrent/" target="_blank" rel="noopener">keras-SimpleRNN</a></li><li><a href="https://www.cnblogs.com/dapeng-bupt/p/7606111.html" target="_blank" rel="noopener">机器学习中过拟合的解决办法</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;什么是Slot-Filling？&quot;&gt;&lt;a href=&quot;#什么是Slot-Filling？&quot; class=&quot;headerlink&quot; title=&quot;什么是Slot Filling？&quot;&gt;&lt;/a&gt;什么是Slot Filling？&lt;/h1&gt;&lt;p&gt;Slot Filling是自然语言理解中的一个基本问题，是对语言含义的简单化处理，它的思想类似于语言学中框架主义的一派，先设定好特定的语言类型槽，再将输入的单词一一填入槽内，而获取言语含义的时候即是根据语义槽的含义进行提取和检索。我们这里的任务就是将表示定购航班（ATIS数据集）这一言语行为的一系列语句填入各种类型的语义槽中。&lt;/p&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="http://shamy1997.github.io/tags/NLP/"/>
    
    <category term="keras" scheme="http://shamy1997.github.io/tags/keras/"/>
    
    <category term="RNN" scheme="http://shamy1997.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow小试牛刀</title>
    <link href="http://shamy1997.github.io/passages/tf%E5%B0%8F%E8%AF%95%E7%89%9B%E5%88%80/"/>
    <id>http://shamy1997.github.io/passages/tf%E5%B0%8F%E8%AF%95%E7%89%9B%E5%88%80/</id>
    <published>2018-10-02T16:02:32.000Z</published>
    <updated>2020-02-22T03:35:03.592Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>此日志为参照Udacity课程中《Intro to tensorflow》的jupyter notebook所做的分解源码，目的在于理解代码逻辑，熟悉创建流程和套路。其中参考了不少博文链接，非常感谢，全部放在文末，在原文中不再指出。</p></blockquote><a id="more"></a><p>数据链接：百度云：<a href="https://pan.baidu.com/s/1xEB_B8QPzSjuLpPXgnAhJg" target="_blank" rel="noopener">NoMNIST</a>  密码：fsks</p><h1 id="P1-预处理数据"><a href="#P1-预处理数据" class="headerlink" title="P1:预处理数据"></a>P1:预处理数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlretrieve</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> resample</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br></pre></td></tr></table></figure><h2 id="解压图片文件"><a href="#解压图片文件" class="headerlink" title="解压图片文件"></a>解压图片文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uncompress_features_labels</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uncompress features and labels from a zip file</span></span><br><span class="line"><span class="string">    :param file: The zip file to extract the data from</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    features = []</span><br><span class="line">    labels = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ZipFile(file) <span class="keyword">as</span> zipf:</span><br><span class="line">        <span class="comment"># Progress Bar</span></span><br><span class="line">        filenames_pbar = tqdm(zipf.namelist(), unit=<span class="string">'files'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get features and labels from all files</span></span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames_pbar:</span><br><span class="line">            <span class="comment"># Check if the file is a directory</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> filename.endswith(<span class="string">'/'</span>):</span><br><span class="line">                <span class="keyword">with</span> zipf.open(filename) <span class="keyword">as</span> image_file:</span><br><span class="line">                    image = Image.open(image_file)</span><br><span class="line">                    image.load()</span><br><span class="line">                    <span class="comment"># Load image data as 1 dimensional array</span></span><br><span class="line">                    <span class="comment"># We're using float32 to save on memory space</span></span><br><span class="line">                    feature = np.array(image, dtype=np.float32).flatten()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Get the the letter from the filename.  This is the letter of the image.</span></span><br><span class="line">                label = os.path.split(filename)[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">                features.append(feature)</span><br><span class="line">                labels.append(label)</span><br><span class="line">    <span class="keyword">return</span> np.array(features), np.array(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the features and labels from the zip files</span></span><br><span class="line">train_features, train_labels = uncompress_features_labels(<span class="string">'notMNIST_train.zip'</span>)</span><br><span class="line">test_features, test_labels = uncompress_features_labels(<span class="string">'notMNIST_test.zip'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Limit the amount of data to work with a docker container</span></span><br><span class="line">docker_size_limit = <span class="number">150000</span></span><br><span class="line">train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set flags for feature engineering.  This will prevent you from skipping an important step.</span></span><br><span class="line">is_features_normal = <span class="keyword">False</span></span><br><span class="line">is_labels_encod = <span class="keyword">False</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">100</span>%|█████████████████████████████████████████████████████████████████████| <span class="number">210001</span>/<span class="number">210001</span> [<span class="number">00</span>:<span class="number">54</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">3832.78f</span>iles/s]</span><br><span class="line"><span class="number">100</span>%|███████████████████████████████████████████████████████████████████████| <span class="number">10001</span>/<span class="number">10001</span> [<span class="number">00</span>:<span class="number">03</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">3207.15f</span>iles/s]</span><br></pre></td></tr></table></figure><h2 id="Min-Max-Scaling"><a href="#Min-Max-Scaling" class="headerlink" title="Min-Max Scaling"></a>Min-Max Scaling</h2><p>Implement Min-Max scaling in the <code>normalize_grayscale()</code> function to a range of <code>a=0.1</code> and <code>b=0.9</code>. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9.</p><p>Since the raw notMNIST image data is in <a href="https://en.wikipedia.org/wiki/Grayscale" target="_blank" rel="noopener">grayscale</a>, the current values range from a min of 0 to a max of 255.</p><p>Min-Max Scaling:$X’=a+{\frac {\left(X-X_{\min }\right)\left(b-a\right)}{X_{\max }-X_{\min }}}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_grayscale</span><span class="params">(image_data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]</span></span><br><span class="line"><span class="string">    :param image_data: The image data to be normalized</span></span><br><span class="line"><span class="string">    :return: Normalized image data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    a = <span class="number">0.1</span></span><br><span class="line">    b = <span class="number">0.9</span></span><br><span class="line">    max_grayscale = <span class="number">255</span></span><br><span class="line">    min_grayscale = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> a+((image_data-min_grayscale))*(b-a)/(max_grayscale-min_grayscale)</span><br></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">train_features</span> = normalize_grayscale(train_features)</span><br><span class="line"><span class="attr">test_features</span> = normalize_grayscale(test_features)</span><br></pre></td></tr></table></figure><h2 id="标签二值化"><a href="#标签二值化" class="headerlink" title="标签二值化"></a>标签二值化</h2><p><code>LabelBinarizer()</code>是sklearn.preprocession中用来将非数值类标签转换为独热编码向量的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the encoder 创建编码器</span></span><br><span class="line">encoder = LabelBinarizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码器找到类别并分配 one-hot 向量</span></span><br><span class="line">encoder.fit(train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment">#最后把目标（lables）转换成独热编码的（one-hot encoded）向量</span></span><br><span class="line">train_labels = encoder.transform(train_labels)</span><br><span class="line">test_labels = encoder.transform(test_labels)</span><br></pre></td></tr></table></figure><p>转换数据类型，这样后面公式中才可以进行运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_labels = train_labels.astype(np.float32)</span><br><span class="line">test_labels = test_labels.astype(np.float32)</span><br></pre></td></tr></table></figure><h2 id="随机划分训练集和测试集"><a href="#随机划分训练集和测试集" class="headerlink" title="随机划分训练集和测试集"></a>随机划分训练集和测试集</h2><p>常见形式为：<br><code>X_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)</code></p><p><strong>参数解释：</strong></p><ul><li>train_data：所要划分的样本特征集</li><li>train_target：所要划分的样本结果</li><li>test_size：样本占比，如果是整数的话就是样本的数量</li><li>random_state：是随机数的种子。</li></ul><p>随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get randomized datasets for training and validation</span></span><br><span class="line">train_features, valid_features, train_labels, valid_labels = train_test_split(</span><br><span class="line">    train_features,</span><br><span class="line">    train_labels,</span><br><span class="line">    test_size=<span class="number">0.05</span>,</span><br><span class="line">    random_state=<span class="number">832289</span>)</span><br></pre></td></tr></table></figure><h2 id="打包数据方便下次取用"><a href="#打包数据方便下次取用" class="headerlink" title="打包数据方便下次取用"></a>打包数据方便下次取用</h2><p>序列化的方法为 pickle.dump()，该方法的相关参数如下：<br><code>pickle.dump(obj, file, protocol=None,*,fix_imports=True)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建pickle_file</span></span><br><span class="line"><span class="comment"># 参数file必须是以二进制的形式进行操作,即「wb」</span></span><br><span class="line">pickle_file = <span class="string">'notMNIST.pickle'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(pickle_file):</span><br><span class="line">    print(<span class="string">'Saving data to pickle file...'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'notMNIST.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> pfile:</span><br><span class="line">            pickle.dump(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">'train_dataset'</span>: train_features,</span><br><span class="line">                    <span class="string">'train_labels'</span>: train_labels,</span><br><span class="line">                    <span class="string">'valid_dataset'</span>: valid_features,</span><br><span class="line">                    <span class="string">'valid_labels'</span>: valid_labels,</span><br><span class="line">                    <span class="string">'test_dataset'</span>: test_features,</span><br><span class="line">                    <span class="string">'test_labels'</span>: test_labels,</span><br><span class="line">                &#125;,</span><br><span class="line">                pfile, pickle.HIGHEST_PROTOCOL)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'Unable to save data to'</span>, pickle_file, <span class="string">':'</span>, e)</span><br><span class="line">        <span class="keyword">raise</span></span><br></pre></td></tr></table></figure><h1 id="P2-从预处理好的pickle中读取数据"><a href="#P2-从预处理好的pickle中读取数据" class="headerlink" title="P2:从预处理好的pickle中读取数据"></a>P2:从预处理好的pickle中读取数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the modules</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reload the data</span></span><br><span class="line">pickle_file = <span class="string">'notMNIST.pickle'</span></span><br><span class="line"><span class="keyword">with</span> open(pickle_file, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  pickle_data = pickle.load(f)</span><br><span class="line">  train_features = pickle_data[<span class="string">'train_dataset'</span>]</span><br><span class="line">  train_labels = pickle_data[<span class="string">'train_labels'</span>]</span><br><span class="line">  valid_features = pickle_data[<span class="string">'valid_dataset'</span>]</span><br><span class="line">  valid_labels = pickle_data[<span class="string">'valid_labels'</span>]</span><br><span class="line">  test_features = pickle_data[<span class="string">'test_dataset'</span>]</span><br><span class="line">  test_labels = pickle_data[<span class="string">'test_labels'</span>]</span><br><span class="line">  <span class="keyword">del</span> pickle_data  <span class="comment"># Free up memory</span></span><br></pre></td></tr></table></figure><pre><code>C:\Users\10677\Anaconda3\envs\keras\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.  from ._conv import register_converters as _register_converters</code></pre><h1 id="使用TF创建单层神经网络"><a href="#使用TF创建单层神经网络" class="headerlink" title="使用TF创建单层神经网络"></a>使用TF创建单层神经网络</h1><p>接下来，我们使用<code>TensorFlow</code>创建一个只有一个输入层和输出层的神经网络，激活函数为<code>softmax</code>。<br>在<code>TensorFlow</code>中，数据不是以整数、浮点数或字符串的形式存储的，而是以<code>tensor</code>对象的形式被存储的。</p><p>在<code>tensor</code>中传递值有两种方法：</p><ul><li>使用<code>tf.constant()</code>，传入变量，但是传入之后就不可变了</li><li>如果要使数据可变，结合<code>tf.placeholder()</code>和<code>tf.feed_dict</code>来输入</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># All the pixels in the image (28 * 28 = 784)</span></span><br><span class="line">features_count = <span class="number">784</span></span><br><span class="line"><span class="comment"># All the labels ("A,B...J")</span></span><br><span class="line">labels_count = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">features = tf.placeholder(tf.float32)</span><br><span class="line">labels = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the weights and biases tensors</span></span><br><span class="line"><span class="comment"># tf.truncated_normal:生成正态分布的随机值</span></span><br><span class="line"><span class="comment"># weights已经随机化，biases就不必随机，简化为0即可</span></span><br><span class="line"></span><br><span class="line">weights = tf.Variable(tf.truncated_normal((features_count,labels_count)))</span><br><span class="line">biases = tf.Variable(tf.zeros(labels_count))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feed dicts for training, validation, and test session</span></span><br><span class="line">train_feed_dict = &#123;features: train_features, labels: train_labels&#125;</span><br><span class="line">valid_feed_dict = &#123;features: valid_features, labels: valid_labels&#125;</span><br><span class="line">test_feed_dict = &#123;features: test_features, labels: test_labels&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Linear Function WX + b</span></span><br><span class="line">logits = tf.matmul(features, weights) + biases</span><br><span class="line"></span><br><span class="line">prediction = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cross entropy</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loss</span></span><br><span class="line">loss = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an operation that initializes all variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Cases</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    session.run(init)</span><br><span class="line">    session.run(loss, feed_dict=train_feed_dict)</span><br><span class="line">    session.run(loss, feed_dict=valid_feed_dict)</span><br><span class="line">    session.run(loss, feed_dict=test_feed_dict)</span><br><span class="line">    biases_data = session.run(biases)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">is_correct_prediction = tf.equal(tf.argmax(prediction, <span class="number">1</span>), tf.argmax(labels, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure><h1 id="P3-训练神经网络"><a href="#P3-训练神经网络" class="headerlink" title="P3:训练神经网络"></a>P3:训练神经网络</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Change if you have memory restrictions</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the best parameters for each configuration</span></span><br><span class="line">epochs =  <span class="number">4</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient Descent</span></span><br><span class="line"><span class="comment"># 使用梯度下降进行训练</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    </span><br><span class="line"></span><br><span class="line"><span class="comment"># The accuracy measured against the validation set</span></span><br><span class="line">validation_accuracy = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Measurements use for graphing loss and accuracy</span></span><br><span class="line">log_batch_step = <span class="number">50</span></span><br><span class="line">batches = []</span><br><span class="line">loss_batch = []</span><br><span class="line">train_acc_batch = []</span><br><span class="line">valid_acc_batch = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    session.run(init)</span><br><span class="line">    batch_count = int(math.ceil(len(train_features)/batch_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch_i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Progress bar</span></span><br><span class="line">        batches_pbar = tqdm(range(batch_count), desc=<span class="string">'Epoch &#123;:&gt;2&#125;/&#123;&#125;'</span>.format(epoch_i+<span class="number">1</span>, epochs), unit=<span class="string">'batches'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The training cycle</span></span><br><span class="line">        <span class="keyword">for</span> batch_i <span class="keyword">in</span> batches_pbar:</span><br><span class="line">            <span class="comment"># Get a batch of training features and labels</span></span><br><span class="line">            batch_start = batch_i*batch_size</span><br><span class="line">            batch_features = train_features[batch_start:batch_start + batch_size]</span><br><span class="line">            batch_labels = train_labels[batch_start:batch_start + batch_size]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Run optimizer and get loss</span></span><br><span class="line">            _, l = session.run(</span><br><span class="line">                [optimizer, loss],</span><br><span class="line">                feed_dict=&#123;features: batch_features, labels: batch_labels&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Log every 50 batches</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> batch_i % log_batch_step:</span><br><span class="line">                <span class="comment"># Calculate Training and Validation accuracy</span></span><br><span class="line">                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)</span><br><span class="line">                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Log batches</span></span><br><span class="line">                previous_batch = batches[<span class="number">-1</span>] <span class="keyword">if</span> batches <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                batches.append(log_batch_step + previous_batch)</span><br><span class="line">                loss_batch.append(l)</span><br><span class="line">                train_acc_batch.append(training_accuracy)</span><br><span class="line">                valid_acc_batch.append(validation_accuracy)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check accuracy against Validation data</span></span><br><span class="line">        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)</span><br><span class="line"></span><br><span class="line">loss_plot = plt.subplot(<span class="number">211</span>)</span><br><span class="line">loss_plot.set_title(<span class="string">'Loss'</span>)</span><br><span class="line">loss_plot.plot(batches, loss_batch, <span class="string">'g'</span>)</span><br><span class="line">loss_plot.set_xlim([batches[<span class="number">0</span>], batches[<span class="number">-1</span>]])</span><br><span class="line">acc_plot = plt.subplot(<span class="number">212</span>)</span><br><span class="line">acc_plot.set_title(<span class="string">'Accuracy'</span>)</span><br><span class="line">acc_plot.plot(batches, train_acc_batch, <span class="string">'r'</span>, label=<span class="string">'Training Accuracy'</span>)</span><br><span class="line">acc_plot.plot(batches, valid_acc_batch, <span class="string">'x'</span>, label=<span class="string">'Validation Accuracy'</span>)</span><br><span class="line">acc_plot.set_ylim([<span class="number">0</span>, <span class="number">1.0</span>])</span><br><span class="line">acc_plot.set_xlim([batches[<span class="number">0</span>], batches[<span class="number">-1</span>]])</span><br><span class="line">acc_plot.legend(loc=<span class="number">4</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Validation accuracy at &#123;&#125;'</span>.format(validation_accuracy))</span><br></pre></td></tr></table></figure><pre><code>Epoch  1/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:11&lt;00:00, 101.27batches/s]Epoch  2/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:10&lt;00:00, 101.99batches/s]Epoch  3/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:10&lt;00:00, 101.38batches/s]Epoch  4/4: 100%|█████████████████████████████████████████████████████████████| 1114/1114 [00:12&lt;00:00, 92.55batches/s]</code></pre><p><img src="http://ohj9e0ect.bkt.clouddn.com/blog/180903/dAh5BG2iKD.png?imageslim" alt="mark"></p><pre><code>Validation accuracy at 0.7662666440010071</code></pre><h1 id="P4-检测"><a href="#P4-检测" class="headerlink" title="P4:检测"></a>P4:检测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">test_accuracy = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    </span><br><span class="line">    session.run(init)</span><br><span class="line">    batch_count = int(math.ceil(len(train_features)/batch_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch_i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Progress bar</span></span><br><span class="line">        batches_pbar = tqdm(range(batch_count), desc=<span class="string">'Epoch &#123;:&gt;2&#125;/&#123;&#125;'</span>.format(epoch_i+<span class="number">1</span>, epochs), unit=<span class="string">'batches'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The training cycle</span></span><br><span class="line">        <span class="keyword">for</span> batch_i <span class="keyword">in</span> batches_pbar:</span><br><span class="line">            <span class="comment"># Get a batch of training features and labels</span></span><br><span class="line">            batch_start = batch_i*batch_size</span><br><span class="line">            batch_features = train_features[batch_start:batch_start + batch_size]</span><br><span class="line">            batch_labels = train_labels[batch_start:batch_start + batch_size]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Run optimizer</span></span><br><span class="line">            _ = session.run(optimizer, feed_dict=&#123;features: batch_features, labels: batch_labels&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check accuracy against Test data</span></span><br><span class="line">        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> test_accuracy &gt;= <span class="number">0.80</span>, <span class="string">'Test accuracy at &#123;&#125;, should be equal to or greater than 0.80'</span>.format(test_accuracy)</span><br><span class="line">print(<span class="string">'Nice Job! Test Accuracy is &#123;&#125;'</span>.format(test_accuracy))</span><br></pre></td></tr></table></figure><pre><code>Epoch  1/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 588.57batches/s]Epoch  2/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 634.64batches/s]Epoch  3/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 633.74batches/s]Epoch  4/4: 100%|████████████████████████████████████████████████████████████| 1114/1114 [00:01&lt;00:00, 638.60batches/s]Nice Job! Test Accuracy is 0.8468999862670898</code></pre><h2 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h2><ul><li><a href="https://lorexxar.cn/2016/07/21/python-tqdm/" target="_blank" rel="noopener">python tqdm模块分析</a></li><li><a href="https://blog.csdn.net/CherDW/article/details/54881167" target="_blank" rel="noopener">Sklearn-train_test_split随机划分训练集和测试集</a></li><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html" target="_blank" rel="noopener">numpy_ndarray.flatten</a></li><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html" target="_blank" rel="noopener">sklearn.LabelBinarizer</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;此日志为参照Udacity课程中《Intro to tensorflow》的jupyter notebook所做的分解源码，目的在于理解代码逻辑，熟悉创建流程和套路。其中参考了不少博文链接，非常感谢，全部放在文末，在原文中不再指出。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="tensorflow" scheme="http://shamy1997.github.io/tags/tensorflow/"/>
    
    <category term="深度学习" scheme="http://shamy1997.github.io/tags/深度学习/"/>
    
  </entry>
  
</feed>
